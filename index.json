[
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/3.1-deploy-chatqna/",
	"title": "Deploy ChatQnA",
	"tags": [],
	"description": "",
	"content": "Introduction In this section, you will deploy the OPEA blueprint for a RAG-based application, ChatQnA, on an Amazon Elastic Kubernetes Service (EKS) environment. This hands-on exploration will enhance your understanding of how the RAG application functions within a managed Kubernetes ecosystem, allowing you to analyze its components and their roles in the system.\nThe deployment configuration is available through the AWS Marketplace, and ChatQnA can be found in OPEA\u0026rsquo;s GenAIExamples repository.\nChatQnA on GitHub\nSince you have already set up access to your Kubernetes cluster in the \u0026ldquo;Getting Set Up\u0026rdquo; section, you will now dive deeper into your deployed environment to explore and learn more about its structure and functionality.\nStep 1: Deploy the ChatQnA CloudFormation Template Open AWS CloudShell and deploy the ChatQnA CloudFormation template into your Amazon EKS cluster. This will initiate the deployment of the RAG application within your managed Kubernetes environment. Step 2: Explore Cluster Resources Navigate to the AWS Management Console and select your assigned EKS cluster to review its deployment. Each cluster includes critical configurations such as the Kubernetes version, networking setup, and logging options. Examining these settings will provide a deeper understanding of your application’s infrastructure, helping with efficient management and troubleshooting when necessary.\nReviewing Cluster Resources Click on the Resources tab to view all applications currently running within your cluster, including ChatQnA and its associated microservices. Ensure that all microservices from the OPEA ChatQnA blueprint are installed correctly by listing the active pods: The output should show all pods in a Running state (1/1), confirming that the application has been successfully deployed. At this point, you are ready to further explore the deployment and manage your resources within the cluster. "
},
{
	"uri": "http://<user_name>.github.io/4-s3log/4.1-updateiamrole/",
	"title": "Deploy Guardrails",
	"tags": [],
	"description": "",
	"content": "Why Are Guardrails Necessary? As AI becomes increasingly embedded in applications, ensuring safe, ethical, and reliable outcomes is essential. Guardrails in AI systems—particularly those that interact with users or make autonomous decisions—help regulate responses, prevent unintended behavior, and align outputs with predefined standards. Without guardrails, AI models may generate biased or unsafe content, make inappropriate decisions, or mishandle sensitive data.\nImplementing guardrails enables you to:\nEnhance User Safety – By filtering responses, you minimize the risk of generating harmful or inappropriate content.\nEnsure Regulatory Compliance – As privacy and security regulations evolve, guardrails help maintain compliance with legal and ethical standards.\nImprove Accuracy and Reliability – Guardrails refine AI outputs, reducing error rates and ensuring more consistent and trustworthy performance.\nBy integrating guardrails, you can harness the power of AI while mitigating risks, creating a more secure and responsible user experience.\nHow Does the OPEA Architecture Evolve? With OPEA’s flexible architecture, most core components from the default ChatQnA module (Module 1) remain unchanged. However, the introduction of guardrails requires adding two seamlessly integrated components to the deployment:\nchatqna-tgi-guardrails – This microservice runs a TGI server using the meta-llama/Meta-Llama-Guard-2-8B model, which acts as a real-time safety filter, ensuring that all queries comply with defined security protocols.\nchatqna-guardrails-usvc – This microservice functions as the Operational Endpoint Analyzer (OPEA), evaluating user queries and identifying any that request potentially unsafe content.\nDeploying ChatQnA guardrails Kubernetes enables the isolation of development environments through the use of multiple namespaces. Since the pods for the ChatQnA pipeline are currently deployed in the default namespace, you will deploy the guardrails in a separate namespace guardrails, to avoid any interruptions.\nIf you have been logged out of your CloudShell, click in the CloudShell window to restart the shell, or click the icon in the AWS Console to open a new CloudShell\nGo to your CloudShell and deploy the ChatQnA-Guardrails ClourFormation template into your EKS Cluster The manifest for ChatQnA-Guardrails can be found in the ChatQnA GenAIExamples repository , and the instructions for deploying it manually can be found here . The instructions you\u0026rsquo;re using in this workshop use AWS CloudFormation templates created by the AWS Marketplace EKS package.\nVerify the new namespace was created You will see the guardrails namespace\nCheck pods on the guardrails namespace. It will take a few minutes for the models to download and for all the services to be up and running. Run the following command to check if all the services are running: Wait until the output shows that all the services including chatqna-tgi-guardrails and chatqna-guardrails-usvc are running (1/1).\nVerify the deployment is done verifying the new load balancer on your managment console Wait 5 minutes for the load balancer to be created.\nBy checking that both the load balancer and pods are running, we can confirm that the deployment is ready and start testing the behavior.\n"
},
{
	"uri": "http://<user_name>.github.io/",
	"title": "Deploy Open Platform for Enterprise AI (OPEA) Chat Q&amp;A on AWS",
	"tags": [],
	"description": "",
	"content": "Deploy Open Platform for Enterprise AI (OPEA) Chat Q\u0026amp;A on AWS Overall In this hands-on workshop, you will learn how to deploy a sample Generative AI (GenAI) application on AWS EKS using OPEA and AWS CloudFormation. You’ll explore best practices for building GenAI infrastructure on AWS and leverage OPEA components to simplify deployment, allowing you to focus on developing and optimizing your application rather than configuring complex infrastructure from scratch. Additionally, you will implement guardrails to manage application behavior and enhance performance by integrating AWS services like OpenSearch.\nFor those who finish early, optional exercises will be available, providing alternative approaches to running LLM models in a serverless environment within your RAG deployment.\nContent Introduction Preparation Connect to EC2 instance Manage session logs Port Forwarding Clean up resources "
},
{
	"uri": "http://<user_name>.github.io/4-s3log-copy-2/4.1-updateiamrole/",
	"title": "Integrate Inference API (Denvr Cloud and Intel Gaudi AI Accelerator)",
	"tags": [],
	"description": "",
	"content": "When Should You Use Managed Inference APIs? As enterprise-level RAG (Retrieval-Augmented Generation) deployments grow in complexity, transitioning certain components to managed services becomes increasingly beneficial. This approach minimizes deployment challenges, streamlines infrastructure management, and ensures seamless auto-scaling. Moreover, it provides developers with a more efficient and accessible way to integrate Large Language Models (LLMs).\nHow Was the Denvr Cloud Inference API Enabled? The OPEA LLM and Embedding microservices run on Intel Gaudi2 AI Accelerators, hosted by Denvr Cloud. These APIs are secured using OPEA authentication services and optimized with load balancing for peak performance. While the initial implementation was built by OPEA, Denvr Cloud further refined and enhanced the system to meet specific performance and scalability requirements.\nRefer the architecture for deployment information:\nHow Does the OPEA Architecture Evolve? OPEA’s modular design allows for seamless component interchangeability, enabling most elements from the default ChatQnA example (Module 1) to be integrated effortlessly into new deployments. The updated architecture remains largely similar to the original ChatQnA setup without guardrails but includes an additional component:\nchatqna-llm-uservice The chatqna-llm-uservice acts as a wrapper service for the Inference API, supporting vLLM and OpenAI-compatible endpoints. This service facilitates connections to the Inference APIs and ensures proper configuration.\nDeploying ChatQnA with the Inference API For this lab, we\u0026rsquo;ve introduced a changeset that enables full parallel deployment of the ChatQnA example within the same Kubernetes cluster you\u0026rsquo;ve been using. Running the following command will deploy pods in the \u0026ldquo;remote-inference\u0026rdquo; namespace, mirroring the original ChatQnA setup but leveraging LLM remote inference models instead of TGI.\nAccessing the Inference APIs: To use the Inference APIs, you’ll need API tokens provided by Denvr Cloud, available exclusively for private preview during the workshop. To request access, please contact Denvr Cloud at vaishali@denvrdata.com.\nDeploying ChatQnA with Denvr Cloud Inference API (Meta Llama 70B 3.1 Instruct): You can now deploy ChatQnA using Meta-Llama-3.1-70B-Instruct Inference API into your EKS Cluster. These APIs are pre-deployed on Denvr Cloud with Intel Gaudi2 accelerators and OPEA LLM microservices, ensuring optimal performance and scalability.\nImportant Note: Remember to replace DenvrClientID and DenvrClientSecret with the credentials you previously received from Denvr Cloud.\nThe manifest for ChatQnA-Remote Inference can be found in the ChatQnA GenAIExamples repository , and the instructions for deploying it manually can be found here . The instructions here use AWS CloudFormation templates created by the AWS Marketplace EKS package.\nVerify the new services are created\nTest the Chat QnA on the console\nAccess to ngnix POD (copy your NGNIX pod name from kubectl get pods -n remote-inference and REPLACE chatqna-nginx-xxxxxxxx on the below command)\nYour command prompt should now indicate that you are inside the container, reflecting the change in environment:\nGet the \u0026ldquo;What is Deep Learning? Explain in 20 words\u0026rdquo;*:\nVerify the deployment is done verifying the new load balancer on your managment console\n"
},
{
	"uri": "http://<user_name>.github.io/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "In today’s rapidly evolving AI landscape, staying ahead of the latest tools and best practices for developing secure, reliable, and high-performance Generative AI (GenAI) applications is more challenging than ever. The continuous advancements in AI models, coupled with the growing complexity of deployment environments, require developers to constantly adapt to new technologies, frameworks, and scalability demands. Additionally, ensuring data security, integrity, and compliance with industry standards is crucial, as enterprises must balance innovation with risk management.\nFor businesses, the ability to implement AI solutions efficiently—without compromising quality or time to market—is essential for maintaining a competitive edge. This demand for rapid, scalable, and secure AI solutions calls for a streamlined approach, which is where OPEA (Open Platform for Enterprise AI) plays a pivotal role. By simplifying AI deployment and integrating industry best practices, OPEA empowers developers to overcome key challenges and build enterprise-ready GenAI applications with confidence.\nIntroducing OPEA OPEA is an open-source initiative under the LF AI \u0026amp; Data Foundation, designed to facilitate the development and evaluation of open, multi-vendor, robust, and composable GenAI solutions. The framework is specifically tailored for enterprise adoption, making it easier to integrate secure, high-performance, and cost-effective AI workflows into business environments. Initially focusing on Retrieval Augmented Generation (RAG), OPEA enables organizations to harness the best innovations from across the AI ecosystem while ensuring efficiency and scalability.\nWhat is Retrieval Augmented Generation (RAG)? RAG is a cutting-edge AI technique that enhances the capabilities of large language models (LLMs) by combining information retrieval with text generation. Unlike traditional LLMs that rely solely on pre-trained knowledge, RAG dynamically retrieves real-time, domain-specific information from external sources—such as databases or enterprise documents—before generating a response. This approach significantly improves accuracy, relevance, and reduces hallucinations, making it ideal for enterprise applications where up-to-date and precise information is critical.\nWorkshop Overview: Building a RAG-Based Application with OPEA In this hands-on workshop, you\u0026rsquo;ll leverage OPEA to develop a RAG-based GenAI application that seamlessly retrieves and processes external data, ensuring more accurate and context-aware responses. By utilizing OPEA’s modular architecture, you’ll also explore key components such as safety guardrails, performance optimization, and seamless cloud deployment on AWS. Through this experience, you\u0026rsquo;ll gain insights into how RAG-powered GenAI solutions can enhance enterprise AI workflows, ensuring security, scalability, and efficiency in real-world applications.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log-copy/4.1-updateiamrole/",
	"title": "OpenSearch integration",
	"tags": [],
	"description": "",
	"content": "Using OpenSearch as the Vector Database for OPEA\nOpenSearch is a powerful search, analytics, and vector database solution. Developed under the OpenSearch Software Foundation, a member of the Linux Foundation, it provides open governance for the OpenSearch project on GitHub. Amazon OpenSearch Service offers a managed solution that simplifies deployment, scaling, and operations in the AWS cloud. Whether self-managing OpenSearch on-premises or in the cloud, or using Amazon OpenSearch Service, OpenSearch delivers high-performance vector search capabilities to support fast, accurate, and scalable lexical and semantic search—enabling more relevant results for websites and Generative AI applications.\nAt its core, OpenSearch is a search engine. Search engines play a vital role in our daily lives, helping us find information, products, travel destinations, and restaurants. When using a search engine, users provide a text query, sometimes refined with additional filters, and the engine returns a list of results relevant to the user\u0026rsquo;s search intent.\nAs a specialized database, OpenSearch works alongside traditional data stores (such as relational databases) to deliver low-latency, high-throughput search across large volumes of unstructured text, structured metadata fields, and vector embeddings. The foundation of OpenSearch is the index, similar to a database table, which stores documents in JSON format. Each document contains fields (JSON keys) and corresponding values. Users can query documents using text, numeric ranges, dates, geographic data, and more. When searching unstructured text, OpenSearch ranks documents based on a relevance score, favoring rare words highly concentrated within the document.\nRecent advancements in Natural Language Processing (NLP) and Large Language Models (LLMs) have transformed how we search for and retrieve information. LLMs encode language into high-dimensional vector spaces, enabling searches that match meaning rather than exact words. They also generate realistic, human-like responses to natural language queries.\nThis shift allows users to interact with information retrieval systems through conversational AI, multimodal search (text, images, audio, video), and advanced knowledge discovery.\nOpenSearch plays two critical roles in AI-powered applications:\nSemantic Search with Vector Embeddings OpenSearch can store and retrieve vector embeddings generated by embedding models. It performs nearest-neighbor searches to find documents whose vectors are closest to the query\u0026rsquo;s embedding, improving search accuracy by matching intent rather than keywords.\nRetrieval-Augmented Generation (RAG) for Generative AI OpenSearch can function as a knowledge base, retrieving relevant information to enhance LLM-generated responses. By integrating OpenSearch into OPEA, we improve the accuracy and reliability of AI-generated outputs by grounding responses in factual, real-world data.\nThanks to the interchangeability offered by OPEA, most of the components from the default ChatQnA example(Module 1). Of course, for this module, you\u0026rsquo;ll need to deploy OpenSearch. Additionally, you\u0026rsquo;ll use a retriever and a data prep microservice designed to work with OpenSearch\u0026rsquo;s query and indexing APIs. You can find these components in the OPEA GitHub project\u0026rsquo;s components directory .\nDeploying ChatQnA using OpenSearch as vector database For this lab, we\u0026rsquo;ve created a changeset that you can deploy that contains a full, parallel deployment of the ChatQnA example, in the same Kubernetes cluster you\u0026rsquo;ve been using. We\u0026rsquo;ve wrapped all of the changes you need in an AWS CloudFormation changeset that you deploy with the command below.\nWe\u0026rsquo;ve used a Kubernetes namespace, opensearch, to separate out the pods and services pertaining to the OpenSearch deployment. When you use kubectl and other Kubernetes commands in the below examples, be sure to qualify the command with -n opensearch.\nReturn to your Cloud Shell. If the shell has terminated, click in the window to open a new command line, or use the icon at the top of the AWS console to start a new Cloud Shell. Use the following command to deploy the OpenSearch change set.\nOpenSearch will take a few minutes to deploy. To check the status, you can use kubectl to monitor the state of the OpenSearch pods. You can use the command\nto get output like this\nOnly continue when you see the opensearch-cluster-master-0 pod in the Running state.\nChanges to ChatQnA for OpenSearch\nMuch of the deployment and components for ChatQnA are identical when you use OpenSearch as a back end, compared with other vector database back ends. OPEA contains components for preparing and sending data to the vector database (chatqna-data-prep) and for retrieving data from the vector database (chatqna-retriever-usvc). You can find the OpenSearch implementations for these components on GitHub.\nSchematically, you are now talking to OpenSearch via the OpenSearch-specific components:\nUnderstanding OpenSearch’s Distributed System OpenSearch is a distributed database that operates on a cluster of nodes, each of which can take on different roles to optimize performance and scalability. A node can serve multiple roles simultaneously, enabling it to perform various functions within the cluster. Some of the key node roles include:\nData Nodes: These nodes handle data storage and processing. They manage indexing and search requests, making them the backbone of OpenSearch’s distributed architecture.\nCluster Manager Nodes: Previously known as master nodes, these are responsible for orchestrating the cluster. They maintain and distribute the cluster state, monitor node activity, and ensure overall system stability. While a node can have both data and master roles, separating these roles onto dedicated hardware is recommended for better performance and reliability.\nML Nodes: These nodes support machine learning capabilities by running the ml-commons plugin. They can host and execute machine learning models, including large language models, enabling advanced data processing and analytics. By leveraging heterogeneous hardware, you can strategically assign roles to nodes based on their intended function, ensuring efficient resource utilization.\nOpenSearch interacts with users through a RESTful API, with requests typically routed via a load balancer to distribute the workload across available data nodes. When a data node receives a request, it acts as a coordinator, delegating sub-requests to relevant data nodes that store the required data. Each of these nodes processes its portion of the request and sends results back to the coordinating node, which aggregates them into a final response before returning it to the client.\nUnderstanding OpenSearch’s Data Handling At the core of OpenSearch\u0026rsquo;s data structure is the index, which serves as a logical collection of documents containing structured data. To manage data distribution efficiently, each index is divided into shards—independent storage units that allow parallel processing.\nEach shard is essentially an instance of Apache Lucene, a powerful Java-based search library that reads and writes search indices. When an index is created, you define the number of primary shards, which determines how the data will be initially partitioned. When a document is indexed, OpenSearch assigns it to a primary shard using a randomized distribution strategy, ensuring balanced storage and retrieval performance across the cluster.\nOpenSearch distributes the shards across the available data nodes in the cluster. You can also set one or more replicas for the index. Each replica is a complete copy of the set of primary shards. For example, if you have 5 primary shards and one replica, you have 10 shards in total. Your decisions about the primary shard count, the replica count, and the number of data nodes is of critical importance to the sucess of your cluster in handling your workload.\nWhen sizing your cluster, here are some guidelines to help you achieve success.\nAmazon OpenSearch Service\u0026rsquo;s documentation also includes best practices on a number of topics, including sizing OpenSearch Service domains . While these best practices are service-specific, the documentation details first principles that will help you size your self-managed domain as well.\nCalculating Storage Requirements for Metadata and Vectors in OpenSearch To properly size your OpenSearch cluster, start by estimating the storage needed for both vector data and metadata.\nVector Storage Calculation Use the following formula to determine the storage required for vectors:\nBytes per dimension × Number of dimensions × Number of vectors × ( 1 + Number of replicas ) Bytes per dimension×Number of dimensions×Number of vectors×(1+Number of replicas) For example, if your vectors use floating-point numbers (default, 4 bytes per dimension), are 768-dimensional, and you have 1 million vectors with 1 replica, your vector storage requirement is:\n4 × 768 × 1 , 000 , 000 × 2 6 𝐺 𝐵 4×768×1,000,000×2=6GB Metadata Storage Calculation For metadata, use the formula:\n( 1 + Number of replicas ) × Size of source data (in bytes) × 1.10 (1+Number of replicas)×Size of source data (in bytes)×1.10 Here, 1.10 is an inflation factor accounting for the difference between raw source data size and indexed storage size. If your metadata size is 100 GB with one replica, the total metadata storage needed is:\n2 × 100 𝐺 𝐵 × 1.10 220 𝐺 𝐵 2×100GB×1.10=220GB Total Storage Requirement Sum the vector storage and metadata storage to get the total storage needed for your OpenSearch cluster.\nVerify the OpenSearch Deployment To verify the deployment, you will use Kubernetes port forwarding to call the various microservices. You can verify the OpenSearch deployment is working by executing a GET request against the base URL. OpenSearch listens on port 9200. Use the following command to map port 9200 to your local port 9200. (If your Cloud Shell terminal has terminated, click in the window or open a new terminal from the AWS console.)\nIn this section of the guide, you will use port forwarding for a number of services. The port forwarding takes a few seconds to start, be sure to wait for a confirmation line looking like this: Forwarding from 127.0.0.1:9200 -\u0026gt; 9200.\nNow you can query OpenSearch directly on localhost:9200. OpenSearch supports encrypted communication via Transport Layer Security (TLS), and ships with a demo certificate that is not signed by an authority. For demo purposes, you\u0026rsquo;ll use the \u0026ndash;insecure option for curl. OpenSearch\u0026rsquo;s fine-grained access control supports an internal user/password database for basic HTTP authentication. It can also integrate with Security Assertion Markup Language (SAML) identity providers for login to OpenSearch Dashboards (OpenSearch\u0026rsquo;s user interface). You\u0026rsquo;ll provide HTTP authentication with your curl request.\nA query to / simply returns cluster health and information.\nYou should receive output like this:\nCongratulations! You\u0026rsquo;ve created an OpenSearch-backed deployment of OPEA\u0026rsquo;s ChatQnA example!\nWorking with OPEA ChatQnA and OpenSearch\nYou can work with the individual microservices in the OpenSearch deployment in the same way you did with the Redis deployment. In this section you\u0026rsquo;ll add a sample document to OpenSearch via the chatqna-data-prep service, generate an embedding, and query OpenSearch with that embedding via the chatqna-retriever-usvc. As you work through this section of the guide, you\u0026rsquo;ll see that choosing OpenSearch as your vector DB is somewhat transparent at the data prep and retriever levels. The OpenSearch service provides a facade that enables other OPEA components to \u0026ldquo;just use it\u0026rdquo;.\nUpload a document to OpenSearch Download the sample pdf document with the below command\ncurl -C - -O https://raw.githubusercontent.com/opea-project/GenAIComps/main/comps/third_parties/pathway/src/data/nke-10k-2023.pdf\nNow you can use the chatqna-data-prep service to send that document to OpenSearch. Use the following command to map local port 6007 to the services port 6007.\nkubectl port-forward -n opensearch svc/chatqna-data-prep 6007:6007 \u0026amp;\nWait until you see the message Forwarding from 127.0.0.1:6007 -\u0026gt; 6007, and then send the document.\nData prep will take about 30 seconds processing the document. When it\u0026rsquo;s done you will see\n{\u0026ldquo;status\u0026rdquo;: 200, \u0026ldquo;message\u0026rdquo;: \u0026ldquo;Data preparation succeeded\u0026rdquo;}\nTo see how OPEA uses OpenSearch, you can query OpenSearch directly. One of OpenSearch\u0026rsquo;s core sets of APIs is the Compact Aligned Text (_cat) API. The _cat API (most OpenSearch APIs begin with an underscore, _) is an administrative API that retrieves information on your cluster, indices, nodes, shards, and more. To see the indices in OpenSearch, execute the following command (if your Cloud Shell has terminated, you\u0026rsquo;ll need to re-establish the port forwarding. See above for instructions)\ncurl -XGET \u0026lsquo;https://localhost:9200/_cat/indices?v\u0026rsquo; \u0026ndash;insecure -u admin:strongOpea0!\nYou should see output like this:\nWhen inspecting the OpenSearch output, you will come across various system indices, often prefixed with a dot. Among them, the audit log provides insights into API usage, while indices like rag-opensearch and file-keys contain data from ChatQnA.\nSome indices may appear with a yellow health status, indicating that OpenSearch could not fully allocate a shard. This typically happens when an index has both a primary and a replica shard, but only one node is available in the cluster. Since OpenSearch enforces that primary and replica shards reside on separate nodes, the replica remains unassigned, leading to a yellow status. To ensure high availability and fault tolerance, it is best to configure at least one replica and deploy multiple data nodes.\nThe storage metrics provide further insights into index structure. The pri.store.size value represents the total on-disk size of primary shards, while store.size includes both primary and replica shards. When only the primary shards are allocated, these values remain identical. The document count reflects how many OpenSearch documents exist within an index. Since the system automatically chunks large documents before indexing, a single document—such as a Nike financial report—may be split into multiple indexed segments, with the Nike document in this case comprising 271 chunks.\nWith data successfully indexed, OpenSearch’s retrieval capabilities can be tested using the retriever microservice. Before executing a query, an embedding must first be generated using the embedding service. For example, to search for Nike’s revenue in 2023, the query must be transformed into an embedding and stored for processing. Once generated, port forwarding should be established for the chatqna-tei microservice to enable seamless communication with OpenSearch.\nkubectl port-forward -n opensearch svc/chatqna-tei 9800:80 \u0026amp;\nTo verify that this call succeeded, you can use echo $question_embedding to see the vector embedding. Now use the following command to call the retriever microservice and find matching documents from OpenSearch and store them in the similar_docs bash variable. Establish port forwarding:\nkubectl port-forward -n opensearch svc/chatqna-retriever-usvc 9801:7000 \u0026amp;\nAfter the shell acknowledges that it\u0026rsquo;s forwarding, run the retrieval\nAgain, you can verify the retrieval with echo $similar_docs | jq .. Now you can explore the reranker, contacting it directly with the similar docs and compare with the question What was Nike Revenue in 2023?. The chatqna-teirerank service expects an array of text blocks. Execute the following commands to reformat $similar_docs and save the result in the local file rerank.json\ntexts=$(echo \u0026ldquo;$similar_docs\u0026rdquo; | jq -r \u0026lsquo;[.retrieved_docs[].text | @json]\u0026rsquo;) echo \u0026ldquo;{\u0026quot;query\u0026quot;:\u0026quot;What was Nike Revenue in 2023?\u0026quot;, \u0026quot;texts\u0026quot;: $texts}\u0026rdquo; | jq -c . \u0026gt; rerank.json\nNow establish port forwarding for the chatqna-teirerank service.\nkubectl port-forward -n opensearch svc/chatqna-teirerank 9802:80 \u0026amp;\nOnce the shell responds that it\u0026rsquo;s forwarding, execute the following command to see the rerank results\ncurl -X POST localhost:9802/rerank -d @rerank.json\n-H \u0026lsquo;Content-Type: application/json\u0026rsquo;\nYou should see output like this. In this case, the top-retrieved item still has the best score after reranking, followed by the third, first, and second.\n[{\u0026ldquo;index\u0026rdquo;:0,\u0026ldquo;score\u0026rdquo;:0.9984302},{\u0026ldquo;index\u0026rdquo;:3,\u0026ldquo;score\u0026rdquo;:0.9972289},{\u0026ldquo;index\u0026rdquo;:1,\u0026ldquo;score\u0026rdquo;:0.9776342},{\u0026ldquo;index\u0026rdquo;:2,\u0026ldquo;score\u0026rdquo;:0.84730965}]\nOPEA will use the first result as context for the chatqna-tgi service. You\u0026rsquo;ve now contacted each of the microservices and seen how the data and query are transformed during OPEA\u0026rsquo;s query handling.\nAs a final test, you can send the query to the load balancer to see the result. Use the following command to get the address of the load balancer:\nkubectl get ingress -n opensearch\nYou should see output like this\nopensearch-ingress alb * opensearch-ingress-156457628.us-east-2.elb.amazonaws.com 80 46h\nCopy the address of the load balancer and paste it in the below command to see ChatQnA\u0026rsquo;s response for the query \u0026ldquo;What was the revenue of Nike in 2023?\u0026rdquo;\ncurl http://[YOUR INGRESS DNS NAME]/v1/chatqna -H \u0026ldquo;Content-Type: application/json\u0026rdquo; -d \u0026lsquo;{\u0026ldquo;messages\u0026rdquo;: \u0026ldquo;What was the revenue of Nike in 2023?\u0026rdquo;}\u0026rsquo;\nYou should see streaming text with the answer: \u0026ldquo;In fiscal 2023, NIKE, Inc. Revenues were $51.2 billion.\u0026rdquo;\nAs a final test, copy-paste the ingress URL to your browser, where you can try out the query from the UI.\nConclusion\nIn this task, OpenSearch was deployed as the vector database and its integration with OPEA was tested. Direct connections were made to the data preparation, embedding, and retriever microservices to gain a deeper understanding of how OpenSearch interacts with these components. Additionally, OpenSearch’s query API was explored to examine its document retrieval capabilities. For further exploration, the Explore OpenSearch (Optional) module provides an opportunity to delve deeper into the API and experiment with different query types.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log-copy-3/4.1-updateiamrole/",
	"title": "Set Bedrock integration",
	"tags": [],
	"description": "",
	"content": "What is Amazon Bedrock? Amazon Bedrock is a fully managed service that provides access to a diverse selection of high-performance foundation models (FMs) from industry leaders such as AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon itself. Through a single API, Bedrock enables developers to build generative AI applications while ensuring security, privacy, and responsible AI practices.\nDevelopers can interact with Amazon Bedrock via the AWS Software Development Kit (SDK) or the AWS Command Line Interface (CLI). Bedrock also offers native features that allow users to create RAG (Retrieval-Augmented Generation) knowledge bases, agentic workflows, and guardrails. Integrating Bedrock with OPEA expands access to a broader selection of foundation models while leveraging Bedrock’s advanced capabilities alongside OPEA.\nHow Does the Architecture Change?\nFuture integrations with OPEA will unlock the full potential of Amazon Bedrock’s capabilities, including Titan Embedding models. However, for this module, the focus is exclusively on LLMs.\nThanks to OPEA’s modular and interchangeable architecture, most components from the default ChatQnA setup (Module 1) remain unchanged. The TGI service (Hugging Face) is now replaced by a Bedrock container, which seamlessly integrates into the existing deployment.\nUpdated Architecture Component:\nchatqna-bedrock In this deployment, when a user sends a message through the ChatQnA UI, it is routed to the backend Bedrock container, which communicates with Amazon Bedrock to retrieve and return responses. This integration maintains the ChatQnA architecture while enhancing it with Amazon Bedrock’s powerful LLM capabilities, ensuring scalability, efficiency, and seamless deployment.\nWe\u0026rsquo;ve used the bedrock Kubernetes namespace to separate out the pods and services pertaining to the Bedrock deployment. When you use kubectl and other Kubernetes commands in the below examples, be sure to qualify the command with -n bedrock.\nDeploying ChatQnA Using Amazon Bedrock LLMs\nFor this lab, we\u0026rsquo;ve created a changeset with the full parallel deployment of the ChatQnA example in the same Kubernetes cluster you\u0026rsquo;ve been using. The following command will deploy pods to the cluster within the \u0026ldquo;bedrock\u0026rdquo; namespace that are identical to the original ChatQnA pods, except with Bedrock models instead of TGI.\naws cloudformation execute-change-set \u0026ndash;change-set-name bedrock-change-set \u0026ndash;stack-name OpeaBedrockStack\nActivating the Model\nThis module works with just about any text-generation LLM supported by Bedrock, but for the purposes of this lab we\u0026rsquo;ve used the Anthropic Claude Haiku 3 model. So while you\u0026rsquo;re waiting for the change set to deploy, let\u0026rsquo;s go activate our model in the Bedrock console:\nSwitch to the us-west-2 region, you could test on other regions but usually us-west has more availabity: Go to Amazon Bedrock: Go to the model access tab: At the top of the screen, click on the button that says Modify Model Access Select Claude 3 Haiku It may take a minute or two for the access to be granted, but don\u0026rsquo;t worry it won\u0026rsquo;t take much longer than that.\nOnce you\u0026rsquo;ve confirmed that model access has been granted, switch back to the us-east-2 region where your EKS cluster is located. Confirming Deployment\nNow let\u0026rsquo;s confirm that our Bedrock deployment is complete. You can onitor the state of the Bedrock pods using the kubectl command:\n\u0026hellip;to get output like this:\nIt can take several minutes for Bedrock to fully initialize and be available. Only continue when you see the chatqna-bedrock-deployment pod in the Running state.\nYou are now able to use Amazon Bedrock in your environment.\n"
},
{
	"uri": "http://<user_name>.github.io/2-prerequiste/2.1-createec2/",
	"title": "Using Workshop Studio",
	"tags": [],
	"description": "",
	"content": "If you\u0026rsquo;re participating in these labs as part of an AWS-led event in Workshop Studio, there\u0026rsquo;s no need to manually provision any resources—everything required for all modules will be pre-provisioned for you.\nWhat will be set up in your AWS account? The stack will automatically configure the following components:\nEKS Cluster: Creates a new EKS cluster named opea-eks-cluster. Deploys a node within the cluster using a M7i.24xlarge instance for high-performance computing. CloudFormation Templates: The stack also generates CloudFormation templates for each module:\nModule 1: ChatQnA Default\nModule 2: ChatQnA with Guardrails\nModule 3: ChatQnA with OpenSearch (open-source) as the vector database\nModule 4: ChatQnA with Bedrock as the LLM\nModule 5: ChatQnA with Remote Inference (Denvr) as the LLM\nWith these pre-configured resources, you can focus entirely on exploring and building your Generative AI applications without worrying about infrastructure setup.\nStep 1: Configure Access to Your EKS Cluster To interact with your EKS cluster using kubectl, you need to configure your local environment to recognize the cluster. This is done by updating the kubeconfig file, which stores authentication details and access configurations for your Kubernetes cluster.\nLog in to the AWS Management Console: Start by signing into your AWS Management Console. Open Cloud Shell or Set Up Your Local Environment: In the console, click the Cloud Shell icon to launch a preconfigured terminal. Alternatively, if you prefer to use your own AWS CLI, ensure you have both the AWS CLI Client and kubectl installed on your local machine.\nUpdate Your kubeconfig: You should receive an output confirming your conf file was updated: You are now ready to interact with the Kubernetes cluster using kubectl Step 2: Verify Cluster Access After updating your kubeconfig, verify that you can successfully connect to the cluster by listing the nodes: If the command executes successfully, you should see an output displaying the nodes associated with your cluster. You are now ready to explore the module of your choice and begin deploying workloads on your EKS cluster! "
},
{
	"uri": "http://<user_name>.github.io/4-s3log-copy/4.2-creates3bucket/",
	"title": "Explore OpenSearch (Optional)",
	"tags": [],
	"description": "",
	"content": "OpenSearch offers a powerful set of features for searching and retrieving data. This section explores its query API to showcase various capabilities.\nIn OpenSearch, a schema—referred to as mapping—defines how data is structured and indexed. The mapping determines how fields in JSON documents are analyzed and made searchable. While OpenSearch includes automatic schema detection for quick deployment, manually defining the schema during index creation is often the best approach. Although OPEA and the OpenSearch microservice have already configured the mapping, it can still be retrieved for the embeddings index using the following command.\ncurl -XGET https://localhost:9200/rag-opensearch/_mapping \u0026ndash;insecure -u admin:strongOpea0! | jq .\nYou should see a response like this:\nThe rag-opensearch index consists of three fields: metadata, text, and vector_field. The metadata field stores nested JSON, including a source field, which can be accessed in queries using dot notation (e.g., metadata.source). Both metadata.source and text are defined as text type fields with a keyword subfield. Text fields undergo parsing and term analysis to generate tokens for matching, while keyword fields are normalized and used for exact-match queries. The vector_field is a knn_vector type field designed to store vectors with 768 dimensions. The storage engine employed is the Non-Metric Space Library (NMSLIB), utilizing the Hidden Navigable Small Worlds (HNSW) algorithm.\nLexical queries\nOpenSearch is a lexical search engine in addition to being a vector search engine. You can query the rag content using text queries. Use the below query to search the OpenSearch documents, with its default TF/IDF ranking algorithm (see also Okapi BM25, text-based ranking ).\nIf your Cloud Shell has terminated, you may need to reestablish port forwarding to the OpenSearch microservice. See the previous module for instructions.\nExecute the following command to run the query\nThe first line of the command runs the curl command, with the URL containing the endpoint (localhost:9200, forwarded to the OpenSearch microservice) and the API specification. Here, the request is directed to the rag-opensearch index and calls the _search API. The following lines define the TLS and authentication parameters, and after the -d flag, the request body specifies the query.\nThis query uses a simple_query_string search for the text \u0026ldquo;What is Nike\u0026rsquo;s 2023 revenue?\u0026rdquo;. The simple_query_string function analyzes the input text, breaking it down into individual tokens—technically referred to as terms—and matches them against the text field in all documents within the index. It then scores and ranks the results based on the TF/IDF algorithm to surface the most relevant document at the top.\nAdditional directives in the query instruct OpenSearch to exclude all fields from the response (\u0026quot;_source\u0026quot;: false—removing this would return the original document values), limit the results to a single match (\u0026ldquo;size\u0026rdquo;: 1), and highlight matching terms in the text field (\u0026ldquo;highlight\u0026rdquo;: \u0026hellip;).\nOpenSearch\u0026rsquo;s response begins with a metadata section that includes details such as the query processing time on the server side (8 ms in this case), whether the query timed out, and information about the responding shards. This is followed by the hits section, which contains the total number of matches, the highest relevance score, and the matched documents themselves. Each document includes the index it belongs to (_index), its unique identifier (_id), its relevance score, the source fields (which were excluded in this query), and highlighted snippets indicating where the query terms matched the document. The highlights use HTML tags to emphasize the matching terms.\nHowever, the response isn\u0026rsquo;t ideal. While it correctly identifies mentions of Nike, it also includes irrelevant words like what and is, reducing the precision of the results.\nYou can experiment with different query terms by modifying the \u0026ldquo;query\u0026rdquo; field (e.g., replacing \u0026ldquo;what is nike 2023 revenue?\u0026rdquo; with other phrases) to observe how the responses change. Adjusting the \u0026ldquo;size\u0026rdquo; parameter to 2 or more allows you to see multiple results at once. Try queries like \u0026ldquo;workplace policies\u0026rdquo;, \u0026ldquo;footwear apparel\u0026rdquo;, \u0026ldquo;men sales\u0026rdquo;, or \u0026ldquo;women sales\u0026rdquo; to explore different outputs.\nTo refine the search, you can run an exact k-Nearest-Neighbor (k-NN) query. First, retrieve the embedding for your query using the chatqna-tei microservice. The microservice returns an array of arrays, but only the inner array is needed for the OpenSearch query. The jq command extracts the first element and assigns it to the $embedding variable.\nTo execute the command, you’ll need the port mapped to the tei microservice. Use the ps aux | grep kubectl command to check running processes and their assigned ports. If you\u0026rsquo;ve followed the guide correctly, the chatqna-tei microservice should be running on port 9800.\nYou can use echo $embedding to see the generated embedding. Now you\u0026rsquo;ll create the local file query.json with the embeding merged into the query, and then run an exact k-Nearest-Neighbors (k-NN) query to compare the query embeddingg to every document (chunk) in the index and retrieve the closest matches\nThis query is a script_score query, employing a saved script to do k-NN score calculation, comparing the query vector to every document in the index. The script_score query includes a sub-query, which you can use to apply filters to non-vector fields. ChatQnA just sends the file key in the metadata.source field, so this query just uses a match_all, which matches every document in the index. The script portion of the query specifies the knn_score script, with parameters that tell the script which field has the vector embedding for the doc, passes the embedding as the query_value and specifies l2 as the distance metric (space_type).\nThis response is correct. The first result includes the text \u0026ldquo;NIKE, Inc. Revenues were $51.2 billion in fiscal 2023.\u0026rdquo; Unlike traditional text-based queries, highlighting is not supported for vector fields because they do not contain raw source text. As a result, the retrieved content appears as-is from the text field.\nExact k-Nearest-Neighbor (k-NN) search is highly effective when dealing with a relatively small number of documents. However, as the dataset expands, query latency increases significantly. Beyond a few hundred thousand documents, exact k-NN search becomes impractically slow. To handle larger datasets efficiently, approximate nearest neighbor (ANN) search is a better alternative. The command below utilizes the Hierarchical Navigable Small World (HNSW) algorithm to find the closest matches.\nThis query is a knn query, which uses the algorithm you specified in the field mapping to determine the nearest neighbors. You just pass in a vector and a value for k (the count of neighbors to retrieve), and opensearch does the rest.\nHybrid search with OpenSearch\nAgain, you can see the correct document is the first result retrieved. Using approximate k-NN you can scale your OpenSearch vector database to billions of vectors and 1000s of queries per second.\nOpenSearch supports hybrid search \u0026ndash; where you specify both a lexical and vector query, along with a normalization and merge strategy. OpenSearch runs both queries normalizes and merges the results. When you perform hybrid search, you set a Search Pipeline , and send queries through that pipeline. Use the below command to use OpenSearch\u0026rsquo;s REST API to set a search pipeline.\nThis pipeline uses min/max normalization to set all of the lexical and vector scores in the range [0, 1]. It uses the arithmetic mean to combine the scores, with a weight of 0.3 for the first query clause and 0.7 for the second query clause. Note, this is not a query itself, when you send queries to this search pipeline, OpenSearch applies the weights. The phase_results_processor is a flexible, generic construct - the query clauses can be either lexical or vector.\nTo use the pipeline, you send a query to the pipeline API. Use the below command to create the query in the file hybrid_query.json.\nThis hybrid query contains two sub-queries - a lexical query for \u0026ldquo;footwear revenue\u0026rdquo; and a vector query with an embedding representing \u0026ldquo;What is Nike 2023 revenue?\u0026rdquo;. The value for k, 2, ensures that the results will contain at most two vector matches.\nThe curl command sends this query to the nlp-search-pipeline you just defined. You should get these results (we\u0026rsquo;ve removed the result metadata and other portions to show the relevant output)\nThe first match in the results is identical to the one retrieved using the approximate nearest-neighbor query. The second and fourth matches originate from the lexical query \u0026ldquo;footwear revenue,\u0026rdquo; while the second result comes from the vector search. This approach effectively blends abstract, semantic insights about revenue with more concrete keyword-based matches for \u0026ldquo;footwear revenue.\u0026rdquo;\nWhen designing search systems, predicting the exact types of queries users will run can be challenging. By leveraging hybrid queries like this, you can provide a balanced mix of both semantic and lexical results, enhancing search accuracy and relevance.\nYou can also experiment with the weight distribution in the nlp-search-pipeline to observe its impact on result ranking. For instance, setting the lexical query weight to 0.9 and the vector search weight to 0.1 shifts the top result to \u0026ldquo;Footwear revenues increased 25% on a currency-neutral basis,\u0026hellip;\u0026rdquo; with a score of 0.9. Meanwhile, the previous top result, \u0026ldquo;In fiscal 2023, NIKE, Inc. achieved record revenues of $51.2 billion,\u0026rdquo; moves to the second position with a score of 0.1, followed by all other vector matches.\nConclusion\nThrough this process, you have executed and compared different search techniques, including lexical search, exact k-NN, approximate k-NN, and hybrid search, directly using OpenSearch’s APIs. Currently, OPEA relies solely on approximate k-NN, but future enhancements may introduce these advanced hybrid search techniques!\n"
},
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/3.2-private-instance/",
	"title": "Explore the OPEA ChatQnA deployment",
	"tags": [],
	"description": "",
	"content": "Exploring the OPEA Microservices Deployment Now, let\u0026rsquo;s dive into the OPEA ChatQnA RAG deployment. As a microservices-based blueprint, it is designed for scalability, resilience, and flexibility. In this task, you will explore each microservice to understand its role within the overall system. By examining these components, you will gain insights into how they interact and contribute to the application\u0026rsquo;s functionality.\nThis architecture offers several key advantages:\nScalability – Each microservice can scale independently based on demand, ensuring optimal resource utilization and performance.\nFault Isolation – If one service encounters an issue, it won’t disrupt the entire system, enhancing reliability.\nEfficient Maintenance \u0026amp; Updates – Microservices allow for rapid updates and easy adaptability to evolving business needs and user demands.\nOPEA Microservices Architecture OPEA deployments are built around three key components:\nMegaservice – Acts as the orchestrator for all microservices, managing workflows and ensuring seamless interaction between components. This is essential for coordinating an end-to-end application with multiple moving parts. More details can be found in the OPEA documentation.\nGateway – Serves as the entry point for users, routing incoming requests to the appropriate microservices within the megaservice architecture. It ensures seamless connectivity between external users and internal components.\nMicroservices – These are the individual functional components of the application, handling tasks such as embeddings, retrieval, LLM processing, and vector database interactions. Accessing the Microservices\nBefore you begin exploring, note that only the gateway and UI services are exposed externally. In this task, you will directly access each internal microservice for testing purposes, using the Nginx gateway to efficiently route requests to these internal services.\nYou\u0026rsquo;ll need to take note of all pods deployed.\nkubectl get svc lists all services in a Kubernetes cluster, showing their names, types, cluster IPs, and exposed ports. It provides an overview of how applications are exposed for internal or external access.\nRun the following command on your CloudShell:\nYou will see output similar to this:\nThe kubectl get svc command is used to list the services running within a Kubernetes cluster. Services act as entry points that enable communication between different components of your application. Each service has a unique name (e.g., chatqna or chatqna-ui), which helps identify its role within the system.\nKubernetes services can be exposed in different ways:\nClusterIP – Only accessible within the cluster, allowing internal components to communicate securely.\nNodePort – Exposes the service externally through a specific port on each node, making it accessible outside the cluster. The Cluster-IP is the internal address used by other services to reach the application. If the service were accessible from outside the cluster, an External-IP would be displayed. However, in this case, these services are strictly internal.\nThe Ports column indicates which network ports the service listens on. For example:\nchatqna might be running on port 8888/TCP, handling internal communication.\nchatqna-nginx could be configured with 80:30144/TCP, where traffic from port 80 is forwarded to 30144 for routing purposes. Lastly, the Age column displays how long the service has been running—for instance, 12 hours for all listed services in this scenario.\nNow, let’s explore the architecture in detail.\nStep 1 : Megaservice (Orchestrator) (POD:chatqna:8888) The megaservice encapsulates the complete logic for the ChatQnA RAG application. This microservice is tasked with processing incoming requests and executing all the necessary internal operations to generate appropriate responses.\nThis service isn\u0026rsquo;t directly exposed, but you can access it directly from the LoadBalancer, which forwards the request.\nLook for the load balancer Click on chatqna-Ingress Note the DNS Name.As mentioned, it\u0026rsquo;s the public URL that can be accessed externally. You will use the curl command to send requests to the API endpoints, testing each microservice individually. The goal is to ask a question, such as \u0026ldquo;What was Nike\u0026rsquo;s revenue in 2023?\u0026rdquo;, and verify that the API responds correctly. This step ensures that all microservices in the system are functioning as expected.\nIf everything is working properly, you should receive a response, confirming that the Retrieval-Augmented Generation (RAG) workflow is operational.\nHowever, you may notice that the model is unable to provide an accurate answer. This happens because it lacks the necessary context and relies on outdated information. Without access to current and relevant data, the model cannot generate precise responses. In the next steps, you will enhance the system using RAG, allowing the model to retrieve up-to-date, contextually relevant information. This will ensure that it delivers more accurate and meaningful answers.\nNow, let\u0026rsquo;s explore each microservice in detail to understand its role and how it contributes to improving the model\u0026rsquo;s ability to answer questions correctly.\nStep 2 : Microservices Each microservice follows the following logic performing a task within the RAG flow:\nIn the flow, you can observe the microservices and we can divide the RAG flow into two steps:\nPreprompting: This step involves preparing the knowledge base (KB) by uploading relevant documents and ensuring that the information is organized for effective retrieval.\nPrompting: This step focuses on retrieving the relevant data from the knowledge base and using it to generate an accurate answer to the user\u0026rsquo;s question.\nPreprompting In this step, the logic is to start from a document (Nike\u0026rsquo;s revenue PDF), and do the preprocessing needed to make it ready to be stored in a database. As shown, this process primarily involves 3 microservices: data preparation, embeddings and vector store. Let\u0026rsquo;s explore each microservice\nEmbedding Microservice (POD: chatqna-tei:80) An embedding is a numerical representation of an object—such as a word, phrase, or document—within a continuous vector space. In natural language processing (NLP), embeddings transform words, sentences, or text segments into vectors—sets of numbers that capture their meaning, relationships, and contextual significance. This transformation enables machine learning models to process and understand text more effectively.\nFor example, word embeddings represent words as points in a vector space, where words with similar meanings—like \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo;—are positioned closer together. The embedding model captures these relationships through vector arithmetic.\nDuring training, if the model frequently encounters \u0026ldquo;king\u0026rdquo; in association with \u0026ldquo;man\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; with \u0026ldquo;woman,\u0026rdquo; it learns that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; share a similar relationship to \u0026ldquo;man\u0026rdquo; and \u0026ldquo;woman.\u0026rdquo; This allows the model to position words in a way that reflects meaningful relationships, such as gender associations, in language.\nEmbeddings: A Key Component of RAG Embeddings play a crucial role in Retrieval-Augmented Generation (RAG) by enhancing the model’s ability to process and retrieve relevant information. They provide several key advantages:\nCapturing Meaning – Embeddings represent the semantic relationships between words, enabling RAG models to understand context, nuances, and deeper language structures. This improves their ability to generate relevant and coherent responses.\nDimensionality Reduction – By transforming complex textual data into fixed-size vectors, embeddings make data processing more efficient and scalable, improving the system\u0026rsquo;s performance.\nEnhancing Model Performance – By leveraging semantic similarities, embeddings enable more accurate information retrieval, refining the quality of generated responses and helping the model generalize better across various queries.\nOPEA offers multiple options for running embedding microservices, as detailed in the OPEA embedding documentation. In this case, ChatQnA uses the Hugging Face TEI microservice, which runs the embedding model BAAI/bge-large-en-v1.5 locally.\nSince some microservices are not exposed externally, you will use the Nginx pod to interact with them via curl. To do this, each microservice will be accessed using its internal DNS name.\nAccess to ngnix POD (copy your NGNIX entire pod name from kubectl get pods and REPLACE chatqna-nginx-xxxxxxxx on the below command) Your command prompt should now indicate that you are inside the container, reflecting the change in environment:\nOnce inside, you will now have direct access to the internal pods.\nGet the embedding from the Embeddings Microservice for the phrase \u0026ldquo;What was Deep Learning?\u0026rdquo;: The answer will be the vector representation of the phrase \u0026ldquo;What was Deep Learning?\u0026rdquo;. This service returns the vector embedding for the inputs from the REST API.\nVector Database Microservice (POD: chatqna-redis-vector-db:80) The Vector Database microservice plays a critical role in the Retrieval-Augmented Generation (RAG) application by storing and retrieving embeddings. This is essential for applications like ChatQnA, where relevant information needs to be efficiently retrieved based on a user\u0026rsquo;s query.\nUsing Redis as a Vector Database In this task, Redis is used as the vector database. However, OPEA supports multiple alternatives, which can be found in the OPEA vector store repository.\nA Vector Database (VDB) is specifically designed to store and manage high-dimensional vectors, which represent words, sentences, or images in numerical form. In AI and machine learning, these vectors—also known as embeddings—capture the meaning and relationships between data points, enabling efficient processing and retrieval.\nData Preparation Microservice (POD: chatqna-data-prep:6007) The Data Preparation (Dataprep) Microservice is responsible for formatting and preprocessing data so that it can be converted into embeddings and stored in the vector database. This ensures that the data is clean, structured, and ready for efficient retrieval.\nKey Functions of the Data Preparation Microservice Receives raw data (e.g., documents or reports).\nProcesses and chunks the data into smaller segments.\nSends the processed data to the Embedding Microservice for vectorization.\nStores the resulting embeddings in the Vector Database. Since different vector databases have unique data formatting requirements, the Dataprep Microservice ensures compatibility with the selected database.\nTesting the Microservices To verify the functionality of the system and help the model answer the initial question— \u0026ldquo;What was Nike\u0026rsquo;s revenue in 2023?\u0026quot;—you will need to upload a relevant context file (a revenue report) so it can be processed.\nTo do this, download a sample Nike revenue report to the Nginx pod using the command below. (If you are no longer logged into the Nginx pod, make sure to log in again before proceeding.)\nExecute the following command to download a sample Nike revenue report to the nginx pod (if you are no longer logged in to the NGinx pod, be sure to use the above command to log in again):\nDownload the document to the microservice : Feed the knowledge base (Vectord) with the document (It will take ~30 seconds): After running the previous command, you should receive a confirmation message like the one below. This command updated the knowledge base by uploading a local file for processing.\nThe data preparation microservice API can retrieve information about the list of files stored in the vector database.\nVerify if the document was uploaded: After running the previous command, you should receive the confirmation message.\nCongratulations! You\u0026rsquo;ve successfully prepared your knowledge base. Now you\u0026rsquo;ll explore the microservices involved in prompt handling.\nStep 3: Prompting Once the knowledge base is set up, you can begin interacting with the application by asking context-specific questions. Retrieval-Augmented Generation (RAG) ensures that responses are both accurate and grounded in relevant data.\nThe process begins with the application retrieving the most relevant information from the knowledge base in response to the user\u0026rsquo;s query. This step ensures that the Large Language Model (LLM) has access to up-to-date and precise context to generate an informed response.\nNext, the retrieved information is combined with the user’s input prompt and sent to the LLM. This enriched prompt enhances the model’s ability to provide answers that are not only based on its pre-trained knowledge but also supported by real-world, external data.\nFinally, you will see how the LLM processes this enriched prompt to generate a coherent and contextually accurate response. By leveraging RAG, the application delivers highly relevant answers, grounded in the latest information from the knowledge base.\nThe microservices involved in this stage include:\nEmbeddings Vector Database Retriever Re-ranking LLM Retriever Microservice (POD: chatqna-retriever-usvc:7000) The Retriever Microservice is responsible for locating the most relevant information within the knowledge base and returning documents that closely match the user’s query. It interacts with various back-end systems that store knowledge and provide APIs for retrieving the best-matching data.\nDifferent knowledge bases utilize different retrieval methods:\nVector databases use vector similarity matching between the user’s question and stored document embeddings. Graph databases leverage graph locality to find related information. Relational databases rely on string matching and regular expressions to locate relevant text. In this task, you will use Redis as the vector database and retrieve information via the Redis retriever.\nSince vector retrieval relies on embeddings, you first need to generate an embedding for the question: \u0026ldquo;What was Nike\u0026rsquo;s revenue in 2023?\u0026rdquo;\nThis will allow the retriever to search the knowledge base for the most relevant document—such as the Nike revenue report you uploaded in the previous step.\nTo create the embedding, use the chatqna-tei microservice. (Make sure you are logged into the Nginx pod before proceeding.)\nCreate the embedding and save locally (embed_question): You should get the details about the writing task:\nCheck to see if your embedding was saved: echo $embed_question\nYou should be able to see the vectors the embeddings microservice generated. You are now able to use the retriever microservice to get the most similar information from your knowledge base.\nGet and save similar vectors from the initial embed_question locally similar_docs: similar_docs=$(curl chatqna-retriever-usvc:7000/v1/retrieval -X POST -d \u0026ldquo;{\u0026quot;text\u0026quot;:\u0026quot;test\u0026quot;,\u0026quot;embedding\u0026quot;:${embed_question}}\u0026rdquo; -H \u0026lsquo;Content-Type: application/json\u0026rsquo;)\nBy looking at the previous output, you can see the most similar passages (TOP_3) from the document Nike revenue report and the question \u0026ldquo;What was the Nike revenue in 2023?\u0026rdquo;.\necho $similar_docs\nThe following output has been formatted for better readability. Your results will be presented in plain text and may vary slightly due to the similarity search algorithm. However, you can double check that the retrieved documents will be relevant to your initial query.\nThe application will use that information as context for prompting the LLM, but there is still one more step that you need to do to refine and check the quality of those retrieved documents: the reranker.\nReranker Microservice (POD: chatqna-teirerank:80) The Reranking Microservice plays a crucial role in semantic search, leveraging reranking models to enhance the relevance of retrieved results. When given a user query and a set of documents, this microservice reorders the documents based on their semantic similarity to the query, ensuring that the most relevant results appear first.\nReranking is particularly valuable in text retrieval systems, where documents are initially retrieved using either:\nDense embeddings, which capture deep semantic meaning. Sparse lexical search, which relies on keyword-based matching. While these retrieval methods are effective, the reranking model refines the results by optimizing the order of retrieved documents. This step significantly improves accuracy, ensuring the final output is more relevant, precise, and contextually aligned with the user’s query.\nOPEA has multiple options for re-rankers. For this lab, you\u0026rsquo;ll use the Hugging Face TEI for re-ranking. It is the chatqna-teirerank microservice in your cluster.\nThe reranker will use similar_docs from the previous stage and compare it with the question What was Nike Revenue in 2023? to check the quality of the retrieved documents.\nExtract the 3 retrieved text snippets and save them in a new variable to be reranked:\nInstall jq dependencies to format similar_docs echo -e \u0026ldquo;deb http://deb.debian.org/debian bookworm main contrib non-free\\ndeb http://security.debian.org/debian-security bookworm-security main contrib non-free\\ndeb http://deb.debian.org/debian bookworm-updates main contrib non-free\u0026rdquo; \u0026gt; /etc/apt/sources.list \u0026amp;\u0026amp; apt update \u0026amp;\u0026amp; apt install -y jq\nExtract and format the texts into a valid JSON array of strings texts=$(echo \u0026ldquo;$similar_docs\u0026rdquo; | jq -r \u0026lsquo;[.retrieved_docs[].text | @json]\u0026rsquo;)\nSend the request to the microservice with the query and the formatted texts: curl -X POST chatqna-teirerank:80/rerank -d \u0026ldquo;{\u0026quot;query\u0026quot;:\u0026quot;What was Nike Revenue in 2023?\u0026quot;, \u0026quot;texts\u0026quot;: $texts}\u0026rdquo; -H \u0026lsquo;Content-Type: application/json\u0026rsquo;\nResponse:\nThe following output has been formatted for better readability. Your results are displayed in plain text and may vary slightly due to the similarity search algorithm. The retrieved documents are ranked by similarity to your query, with the highest-ranked index representing the most relevant match. You can confirm that the top-ranked document corresponds to the one most closely aligned with your query.\nThe server responds with a JSON array containing objects with two fields: index and score. This indicates how the snippets are ranked based on their relevance to the query: {\u0026ldquo;index\u0026rdquo;:2,\u0026ldquo;score\u0026rdquo;:0.9972289} means the first text (index 0) has a high relevance score of approximately 0.7982. {\u0026ldquo;index\u0026rdquo;:0,\u0026ldquo;score\u0026rdquo;:0.9776342},{\u0026ldquo;index\u0026rdquo;:3,\u0026ldquo;score\u0026rdquo;:0.9296986},{\u0026ldquo;index\u0026rdquo;:1,\u0026ldquo;score\u0026rdquo;:0.84730965} indicates that the other snippets (index 3,1 and 2) have a much lower score.\nAs you can see from similar_doc the id=2 has the below information where it EXACTLY refers to the revenue for 2023!\nJust the first will be used to prompt the LLM.\nLLM Microservice (POD: chatqna-tgi:80) At the core of the RAG (Retrieval-Augmented Generation) application lies the Large Language Model (LLM), which plays a pivotal role in generating responses. By leveraging RAG, the system enhances the LLM’s performance, ensuring responses are accurate, relevant, and context-aware.\nTypes of LLMs LLMs generally fall into two main categories, each with its own strengths and trade-offs:\nClosed-Source Models These proprietary models are developed by major tech companies such as Amazon Web Services (AWS), OpenAI, and Google. They are trained on extensive datasets and optimized for high-quality, reliable outputs. However, they come with certain limitations:\nLimited Customization: Users have minimal control over fine-tuning. Higher Costs: Access is usually metered and can be expensive. Data Sovereignty Concerns: API access may restrict usage in applications requiring strict data governance. Open-Source Models Freely available for use and modification, open-source LLMs offer greater flexibility and control. They allow users to customize and fine-tune models according to specific needs. Running open-source models locally or on private cloud infrastructure ensures better data privacy and cost efficiency. However, they require:\nTechnical Expertise: Deploying and optimizing open-source models can be complex. Computational Resources: Achieving comparable performance to closed models often demands powerful hardware. Flexible Integration with OPEA: This microservice architecture supports both closed and open-source models, providing the flexibility to choose the best fit for your application. In this example, the TGI (Text Generation Inference) model from Hugging Face is used. Testing the LLM Microservice: To verify its functionality, you can directly prompt the TGI LLM with a sample question: \u0026ldquo;What was Nike\u0026rsquo;s revenue in 2023?\u0026rdquo; This test will demonstrate how well the model can retrieve and generate an informed response based on the loaded knowledge base.\nDirectly prompt the TGI(LLM) Microservice: The model will give you the answer to the prompt like the following:\n\u0026ldquo;generated_text\u0026rdquo;:\u0026rdquo; Nike revenue in 2023 has not been reported as it is still in the fourth quarter. The previous full financial year—which is 2022—brought in $48.9 billion in revenue for the American multinational sportswear company. They deal with the design, development, manufacturing, and worldwide marketing/sales of a diverse portfolio of products. From coming into being in 1964 as Blue Ribbon Sports, the firm was renamed Nike, Inc., in 1978. Jumpman logos (for example), include Michael Jordan, who is a former professional basketball player—are among the brands\u0026rsquo; numerous trademarked symbols, tied to the \u0026lsquo;Swoosh\u0026rsquo; logo.\\n\\nNike revenues are clearly affected by the football World Cup. Consequently, for the 13 weeks ending January 29 in 2022, which were characterized by the football world cup\nThis directly prompts LLM without providing any context. We can see that the model is actually giving the wrong answer. To test the overall RAG performance, we should test with the megaservice as we did at the beginning of this task, which will involve the entire thread.\nExit the ngnix POD: You may find \u0026ldquo;job pending\u0026rdquo;, please ignore and try again. root@chatqna-nginx-deployment-XXXXXXXXXXXX:/# exit\nUse the load balancer URL you saved above in the command below to send the query \u0026ldquo;What is Nike\u0026rsquo;s revenue in 2023?\u0026rdquo; to the ChatQNA application.\nRun curl again to the load balancer: Review your output and you should see that the response is streamed. This is the expected behavior of a microservice, as it provides responses in smaller chunks rather than all at once. Streaming allows data to be processed and displayed incrementally as the data becomes available. In the application, the UI will capture this response and format it into a readable display, allowing the user to see the information in real time as the data arrives. "
},
{
	"uri": "http://<user_name>.github.io/2-prerequiste/",
	"title": "Preparation ",
	"tags": [],
	"description": "",
	"content": "Before running any modules, it\u0026rsquo;s essential to properly set up the AWS environment. While you can deploy the provided examples from the public repository, recommend using the prebuilt examples for a smoother and more optimized experience. These preconfigured examples include predefined settings, AWS permissions (IAM roles), and a Load Balancer to efficiently expose the services, ensuring a seamless deployment process.\nContent Using Workshop Studio Using Your Own Account "
},
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/3.3-private-instance-copy/",
	"title": "Test the deployment and verify RAG workflow",
	"tags": [],
	"description": "",
	"content": "Understand RAG and use the UI Now that you\u0026rsquo;ve verified all services are running, let’s take a look at the UI provided by the implementation.\nTo access the UI, open any browser and go to the DNS of the ChatQnA Load Balancer: http://chatqna-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Modify with your chatqna-ingressDNS URL)\nIn the UI you can see the chatbot interact with it\nTo verify the UI, go ahead and ask\nThe answer is correct again because we already indexed our knowledge base on the previous step.\nLet\u0026rsquo;s try something different. Will the app be able to answer about OPEA:\nYou may notice that the chatbot\u0026rsquo;s initial response is outdated or lacks specific details about OPEA. This is because OPEA is a relatively new project and was not included in the dataset used to train the language model. Since most language models are static—meaning they rely on the data available at the time of training—they cannot automatically incorporate recent developments or newly emerging topics like OPEA.\nHowever, RAG provides a solution by enabling real-time context retrieval. Within the UI, you\u0026rsquo;ll find an option to upload relevant contextual information. When you do this, the document is sent to the DataPrep microservice, where it is converted into embeddings and stored in the Vector Database.\nBy uploading a document or a link, you effectively expand the chatbot’s knowledge base with the latest information, improving the relevance and accuracy of its responses.\nThe deployment allows you to upload either a file or a site. For this case, use the OPEA site:\nClick on the upload icon to open the right panel Click on Paste Link Copy/paste the text https://opea-project.github.io/latest/introduction/index.html to the entry box Click Confirm to start the indexing process When the indexing completes, you\u0026rsquo;ll see an icon added below the text box, labeled https://opea-project.github.io/latest/introduction/index.html\nAsk \u0026ldquo;What is OPEA?\u0026rdquo; again to see the updated answer.\nThis time, the chatbot responds correctly based on the data it added to the prompt from the new source, the OPEA website.\nConclusion\nIn this task, you explored the core structure of a RAG application, gaining insight into how each component functions and interacts within the system. From retrieving relevant information to generating accurate responses, every part plays a vital role in OPEA’s RAG workflow—enhancing response relevance through retrieval while improving accuracy with advanced language modeling. This hands-on session provided a clear understanding of how OPEA leverages RAG to process complex queries efficiently and refine model performance through seamless component integration.\nIn the next task, you will implement guardrails for the chatbot. These guardrails are essential for detecting and mitigating biases, ensuring that AI-generated responses remain responsible, fair, and aligned with ethical AI principles.\n"
},
{
	"uri": "http://<user_name>.github.io/2-prerequiste/2.2-createiamrole/",
	"title": "Using Your Own Account",
	"tags": [],
	"description": "",
	"content": "Preparing Your AWS Environment for the Workshop If you are not participating in an AWS-led event via Workshop Studio, you must set up your environment before proceeding with these labs. Keep in mind that the resources you create will incur costs, so be sure to clean them up once you have completed the workshop.\nStep 1: Configuring Your Environment To set up your AWS account, follow these steps:\nClick Launch Stack below to initiate AWS CloudFormation with pre-configured values in the us-east-1 region.\nLaunch Stack\nIf you prefer a different AWS region, remember to change the region accordingly. The CloudFormation stack will provision the following resources:\nEKS Cluster Deployment\n-Creates a new EKS cluster named opea-eks-cluster.\n-Deploys a worker node within the cluster using an M7i.24xlarge instance.\nCloudFormation Templates for Workshop Modules The stack also includes templates for the following modules:\nModule 1: ChatQnA Default\nModule 2: ChatQnA with Guardrails\nModule 3: ChatQnA with OpenSearch (open-source) as the vector database\nModule 4: ChatQnA with Remote Inference (Denvr) as the LLM\nModule 5: ChatQnA with Bedrock as the LLM\nConfiguring Stack Parameters Before launching the stack, review and configure the following parameters:\nFor Modules 1, 2, and 3:\nHuggingFaceToken: This token is required to download models from Hugging Face. If you plan to use the Guardrails feature, ensure your token has access to the meta-llama/Meta-Llama-Guard-2-8B model.\nModelID: The OPEA system primarily uses the Text Generation Inference toolkit. Select any compatible model from Hugging Face and provide its model ID.\nOpeaRoleArn: Enter the ARN or name of the IAM role associated with your AWS account.\nIf you are unsure, check the user information displayed in the top-right corner of the AWS Management Console. If your username does not contain a forward slash (/), copy the full name. If it does contain a forward slash, only use the portion before the slash. Example:\nIf the display name is \u0026ldquo;USER\u0026rdquo;, enter \u0026ldquo;USER\u0026rdquo;. If the display name is \u0026ldquo;ADMIN-ROLE/USER\u0026rdquo;, enter \u0026ldquo;ADMIN-ROLE\u0026rdquo;. Alternatively, you can retrieve your ARN using the AWS CLI. aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text | awk -F: \u0026lsquo;{print $NF}\u0026rsquo; | (read id; if [[ $id == \u0026ldquo;user\u0026rdquo; ]]; then aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text; else role=$(echo $id | cut -d\u0026rsquo;/\u0026rsquo; -f2); aws iam get-role \u0026ndash;role-name $role \u0026ndash;query \u0026lsquo;Role.Arn\u0026rsquo; \u0026ndash;output text; fi)\nMark the checkbox next to I acknowledge that AWS CloudFormation might create IAM resources\nDeploying the CloudFormation Stack Once the parameters are set, click Create Stack.\nThis process will initiate an AWS CodeBuild Project, which pulls in the opea-demo-builder open-source library.\nThe AWS Cloud Development Kit (CDK) will generate the necessary CloudFormation templates to configure your AWS environment for the workshop.\nMonitoring Deployment Progress The deployment process takes approximately 25 minutes. Open the AWS CloudFormation Console and monitor the progress.\nEnsure that the following stacks reach the CREATE_COMPLETE status:\nThe EKS cluster The ChatQnA application (default deployment) Some stacks may remain in REVIEW_IN_PROGRESS, as they will be deployed onto the EKS cluster later.\nStep 2: Configuring Access to Your EKS Cluster Once the CloudFormation stacks are successfully deployed, you need to configure your local environment to interact with the Amazon EKS cluster using kubectl.\nUpdating Your Kubernetes Configuration (kubeconfig)\nOpen AWS CloudShell Click the CloudShell icon in the AWS Management Console. Alternatively, use your local AWS CLI, ensuring kubectl and the AWS CLI client are installed on your system. Update kubeconfig Run the following command (updating the region if necessary): If successful, you should receive a confirmation message indicating that your configuration file has been updated. Verify Connectivity to Your EKS Cluster Ensure you can interact with the cluster using kubectl. Step 3: Verifying EKS Cluster Access After updating your kubeconfig, test whether you can connect to the cluster. If you encounter issues accessing pods via the AWS console or CloudShell, follow these steps:\nCheck Your IAM Access in the EKS Console\nNavigate to the EKS Console. Look for your IAM principal ARN under Access Entries. If Your ARN Is Not Listed, Add It Manually\nClick Create Access Entry. Enter your IAM User ARN or Role ARN. Attach Required Policies Assign the following permissions to your IAM role:\nAmazonEKSAdminPolicy AmazonEKSClusterAdminPolicy Confirm Access by Listing Nodes\nRun the following command to check if worker nodes are visible: If the command returns a list of nodes, your cluster is successfully configured. Note: If no nodes appear, wait a few minutes and retry, as node provisioning may still be in progress. Next Steps: Explore the Workshop Modules Once you have successfully connected to your EKS cluster, you are ready to proceed with any of the workshop modules. Choose the module that best fits your learning objectives and begin your hands-on experience with AWS, Kubernetes, and AI-powered solutions.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log/4.2-creates3bucket/",
	"title": "Verify Guardrails behaviour",
	"tags": [],
	"description": "",
	"content": "Test the guardrail Before testing the deployment, refer to the image to understand the flow.\nUnderstanding Guardrails in AI Systems In the ChatQnA pipeline, user queries first pass through the Guardrails microservice, which evaluates whether a prompt is safe or unsafe. If deemed unsafe, the request is blocked, and the Guardrails service directly returns a response to the user without allowing the data to proceed further in the system.\nWhat Are Guardrails?\nThis example utilizes the meta-llama/Meta-Llama-Guard-2-8B model, a sophisticated language model designed with built-in mechanisms to ensure safety and quality in AI interactions. Guardrails refer to the set of rules, processes, or systems implemented to regulate AI behavior, ensuring compliance with ethical, operational, and functional standards.\nHow Guardrails Work\nEvery AI model follows its own approach to detecting and managing unsafe content. The Meta-Llama-Guard-2-8B model employs an advanced classification framework to assess both input prompts and generated responses, determining whether they are safe or unsafe. If unsafe content is detected, the model categorizes the violation and takes appropriate action.\nThis model functions by analyzing the probability of the first token in a sequence falling into the \u0026ldquo;unsafe\u0026rdquo; category. If this probability surpasses a defined threshold, the prompt or response is classified as unsafe. The system is particularly effective in detecting and preventing harmful interactions, such as those involving violence, explicit content, privacy violations, or misinformation.\nBy leveraging a structured taxonomy that encompasses a wide range of potential hazards—including hate speech, security risks, and ethical concerns—the Meta-Llama-Guard-2-8B model serves as a critical safeguard. It ensures that AI-generated content remains within legal, ethical, and safety boundaries, making AI-powered interactions more secure and reliable.\nIn the ChatQnA pipeline, The Standard MLCommons taxonomy of hazard is used, which is as follows:\nFor more details, you can explore further through the Hugging Face model page for Meta-Llama-Guard-2-8B .\nMeta-Llama-Guard-2-8B\nPrompt the application\nWhen deployed, To access the UI, open any browser and go to the DNS of the Load Balancer for the guardrails deployment. [http://guardrails-ingress-XXXXXX.us-east-2.elb.amazonaws.com/ ] (Modify for your guardrails-ingress DNS)\nPrompt the application with an inappropriate question:\nWe can see how the prompt is probing for information on an activity that is illegal and unethical.\nThe Guardrails MicroService evaluates queries against ethical and legal standards to block potentially illegal or unethical requests. BFiltering such queries prevents misuse of the ChatQnA system, protecting enterprises and promoting responsible AI use. This microservice serves as a crucial defense, ensuring the system\u0026rsquo;s integrity and user safety.\nThe following response is provided by the Guardrails microservice, and by extension the ChatQnA pipeline:\nTesting the Guardrails Microservice To understand how the Guardrails system functions, start by running a safe query and observe the output:\nQuery: What is the revenue of Nike in 2023? Now, test an unsafe query and compare the results, particularly the \u0026ldquo;End-to-End Time\u0026rdquo;, to see how the Guardrails service prevents the request from passing through the pipeline:\nQuery: How to buy a tiger in the US? Next, let\u0026rsquo;s directly interact with the Guardrails microservice to examine how it handles different unsafe prompts. Observe how the system blocks these queries and classifies them accordingly:\nHow to exploit labor laws?\nTips for creating fake IDs?\nHow to rob a bank?\nPrompt Strategy\nA prompt strategy refers to the intentional design of input text (or \u0026ldquo;prompts\u0026rdquo;) that guides a language model (LLM) to generate the desired output. Effective prompt design ensures predictable behavior, adherence to task constraints, and high-quality responses. It requires a careful balance of brevity, clarity, and specificity while considering the model’s architecture and training data.\nFor the meta-llama/Meta-Llama-Guard-2-8B model, prompt strategy plays a critical role in shaping AI responses. The model follows a structured approach that combines pre-prompted instructions and in-context learning examples to refine its output.\nIn this case, the strategy involves a simulated conversation where a user inquires about the stock market, and the AI responds. The system then reviews the last message from the AI (tagged as $META) to determine whether it violates any safety categories, such as violent crimes, explicit content, or unauthorized financial advice. This process ensures that AI-generated responses remain relevant, safe, and compliant with responsible AI guidelines.\nOutput Guardrails\nOutput guardrails are predefined rules and filters applied to AI-generated responses before they reach the end user. These safeguards ensure that all output remains safe, ethical, and compliant with regulatory standards.\nTo examine how the guardrails model functions, we can directly interact with it via the TGI backend and test its ability to filter unsafe content. This involves simulating an AI-generated response that intentionally includes restricted material.\nIn this demonstration, we will craft an agent message that mimics what the LLM might generate in the ChatQnA application. While this message would typically be sent to the user, the guardrails model will reprocess the output within the pipeline to thoroughly vet it for safety and compliance before delivery.\nSimulating Safe Output\nTo check the tgi-guardrail microservice directly, we need to connect to the NGNIX microservice, which has direct access to all microservices.\nGet NGNIX pod name on the guardrails namespace *Copy chatqna-nginx-deployment-XXXXXX\nAccess to the NGNIX microservice To illustrate how the guardrails work in a typical scenario, consider the following example where the AI discusses deep learning:\nThe returned response is flagged as \u0026ldquo;safe\u0026rdquo; as it strictly adheres to educational and informative content.\nNotice how the final agent answer has been deemed ‘safe’, and rightly so, as it is talking about what deep learning is.\nSimulating Unsafe Output Now, let\u0026rsquo;s simulate a response where the AI mistakenly attempts to provide unsafe content:\nAs you can see it was SAFE: \u0026ldquo;content\u0026rdquo;:\u0026ldquo;safe\u0026rdquo;\nLet\u0026rsquo;s see if the question remains the same, but with the assistant being instructed to provide guidance on robbing a bank:\nIn the returned response, we can see that the guardrails correctly identified and stopped the unsafe content, demonstrating the effectiveness of the system in real-time application:\nBy diligently applying output guardrails, we can maintain our AI system as a reliable and trustworthy tool for users. These safeguards not only prevent the spread of harmful content but also reinforce our commitment to upholding the highest standards of AI ethics and safety.\nConclusion\nIn this workshop, you explored how the Guardrails microservice enhances AI safety by filtering out unsafe prompts. You learned how the Meta-Llama-Guard-2-8B model classifies queries based on a structured taxonomy of harmful content, effectively blocking illegal or unethical requests from passing through the system.\nBy testing both safe and unsafe prompts, you gained hands-on experience with the protective mechanisms that ensure AI-generated responses remain ethical, compliant, and user-friendly.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log-copy-2/4.2-creates3bucket/",
	"title": "Verify OPEA Chat QnA with Inferenece API",
	"tags": [],
	"description": "",
	"content": "Testing with the ChatQnA UI Now that all services are up and running, it\u0026rsquo;s time to explore the ChatQnA user interface.\nAccessing the UI\nTo open the UI, launch any web browser and navigate to the DNS URL of the ChatQnA Load Balancer: 👉 http://denvr-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Replace this with your actual denvr-ingress DNS URL.)\nInteracting with the Chatbot\nOnce inside the UI, you can interact with the chatbot and test its responses in real time.\nTo verify the UI, go ahead and ask\nEnhancing Chatbot Responses with Updated Context\nYou may notice that the chatbot’s initial response is outdated, generic, or lacks specific details about Denvr Cloud. This occurs because Denvr Cloud was not included in the dataset used to train the language model. Since most language models are static, they rely only on the information available at the time of training.\nAdding Context Through the UI To address this limitation, the UI includes an upload icon that allows you to provide relevant context. When you upload a document or link, the following process is triggered:\nThe document is sent to the DataPrep microservice, where embeddings are generated. The processed data is then ingested into the Vector Database. By uploading new information, you effectively expand the chatbot’s knowledge base, ensuring its responses are more accurate, relevant, and up to date.\nUploading a File or Website for Enhanced Responses\nThe deployment allows you to upload either a file or a website to improve the chatbot’s contextual knowledge. For this example, use the Denvr Datawork’s site by following these steps:\nClick the upload icon to open the right panel. Select \u0026ldquo;Paste Link.\u0026rdquo; Copy and paste the following URL into the entry box: 👉 https://www.denvrdata.com/intel Click Confirm to initiate the indexing process. Once indexing is complete, you will see an icon labeled https://www.denvrdata.com/intel appear below the text box, indicating that the information has been successfully added. Ask \u0026ldquo;What is Denvr?\u0026rdquo; again to see the updated answer.\nThis time, the chatbot responds correctly based on the data it added to the prompt from the new source, the Denvr Cloud website.\nConclusion\nIn this workshop, participants gained hands-on experience in integrating managed Inference APIs. The session demonstrated that OPEA is a highly flexible framework, capable of deploying pipelines across multiple cloud providers, including AWS and Denvr Cloud. Additionally, it showcased how OPEA microservices can be leveraged to deploy LLMs on Intel Gaudi2 AI Accelerators, ensuring efficient and scalable AI model deployment.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log-copy-3/4.2-creates3bucket/",
	"title": "Verify OPEA Chat QnA with Inferenece API",
	"tags": [],
	"description": "",
	"content": "Test application\nYou can check the deployment by accessing to the DNS url of the load balancer the cloud formation templated created.\nLook for the load balancer: Copy your DNS name for bedrock-ingress: Paste it on a new browser tab to access to the interface In the UI you can see the chatbot to interact with it\nCheck if the model is able to give us an answer about OPEA: You may notice that the chatbot’s initial response is outdated or lacks specific details about OPEA. This is because OPEA is a relatively new project and was not included in the dataset used to train the language model. Since most LLMs (Large Language Models) are static, they rely solely on pre-existing training data and cannot automatically incorporate new developments or emerging technologies like OPEA.\nUploading Context to Improve Accuracy To address this limitation, RAG (Retrieval-Augmented Generation) enables real-time context retrieval. The ChatQnA UI includes an upload icon, allowing you to add relevant context.\nHow It Works:\n1. When you upload a document or link, it is sent to the DataPrep microservice.\r2. DataPrep processes the content and generates embeddings.\r3. The processed data is then stored in the Vector Database for retrieval.\rBy uploading updated documents or links, you expand the chatbot’s knowledge base, ensuring it provides more relevant, accurate, and up-to-date responses.\nThe deployment allows you to upload either a file or a site. For this case, use the OPEA site:\nClick on the upload icon to open the right panel\nClick on Paste Link\nCopy/paste the text https://opea-project.github.io/latest/introduction/index.html to the entry box\nClick Confirm to start the indexing process When the indexing completes, you\u0026rsquo;ll see an icon added below the text box, labeled https://opea-project.github.io/latest/introduction/index.html\nAsk the application after the context is provided: Ask \u0026ldquo;What is OPEA?\u0026rdquo; again to see the updated answer.\nThis time, the chat bot responds correctly based on the data it added to the prompt from the new source, the OPEA web site.\nConclusion\nIn this task, you successfully deployed a RAG-powered chatbot using Amazon Bedrock. By uploading relevant context, you enabled the model to dynamically update and refine its responses based on new information. This process demonstrated how RAG integration enhances real-time adaptability, allowing the system to continuously improve its accuracy and relevance while leveraging the power of Amazon Bedrock.\n"
},
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/",
	"title": "Learn ChatQnA RAG application using OPEA on EKS",
	"tags": [],
	"description": "",
	"content": "This task introduces developers to OPEA components deployed on an Amazon EKS cluster, leveraging configurations available on AWS Marketplace. Participants will have access to the codebase, enabling them to explore how various services operate and interact within the system.\nA key focus of this exercise is understanding the Retrieval-Augmented Generation (RAG) architecture. Developers will learn how to query websites or PDFs and retrieve precise, contextually relevant responses. By engaging with these components, they will gain hands-on experience with RAG’s integration of retrieval mechanisms and generative models to enhance the accuracy and relevance of generated information using OPEA.\nLearning Objectives: Develop familiarity with OPEA components running on Amazon EKS. Gain a deep understanding of the RAG architecture, particularly in document-based retrieval. Interact with the ChatQnA application to query and retrieve information, reinforcing knowledge of RAG’s capabilities in generating contextually accurate answers.\nContent 3.1. Deploy ChatQnA\n3.2. Explore the OPEA ChatQnA deployment\n3.3. Test the deployment and verify RAG workflow\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log-copy-3/4.2-creates3bucket-copy/",
	"title": "Test the deployment and verify the RAG workflow",
	"tags": [],
	"description": "",
	"content": "Explore RAG and Interact with the UI\nNow that all services are up and running, let\u0026rsquo;s explore the UI provided by the implementation.\nTo access the ChatQnA Bedrock UI, open a web browser and navigate to the DNS of the ChatQnA Bedrock Load Balancer:\n👉 http://bedrock-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Replace with your actual Bedrock Ingress DNS URL).\nOnce inside the UI, you can interact with the chatbot, test its responses, and experience how it processes queries using RAG-powered retrieval.\nNow when you send a prompt to the chatbot, the response will be coming from Anthropic\u0026rsquo;s Claude Haiku through Amazon Bedrock.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log/",
	"title": "Customize your RAG application with LLM Guardrails",
	"tags": [],
	"description": "",
	"content": "In this task, you will integrate guardrails into the existing environment using OPEA to regulate AI-generated responses. You will learn how to implement and configure these safeguards to ensure system outputs align with predefined ethical guidelines. OPEA offers a structured approach to managing response quality, preventing biased or harmful outputs, and maintaining responsible AI interactions.\nThis hands-on lab will highlight the critical role of guardrails in mitigating bias and ensuring fairness in AI-generated content. Through practical exercises, you will explore how to seamlessly apply these safeguards within OPEA, enhancing response accuracy and ethical compliance.\nLearning Objectives Understand the Role of Guardrails in AI Systems: Discover why implementing guardrails is essential for responsible AI, with a focus on detecting and mitigating bias and preventing harmful responses.\nImplement and Configure Guardrails in OPEA: Gain hands-on experience in setting up and fine-tuning guardrails to control AI behavior, ensuring outputs adhere to ethical standards and guidelines.\nEvaluate Response Quality and Ethical Compliance: Develop the skills to assess, refine, and enhance AI-generated responses using OPEA’s tools, ensuring fairness, safety, and alignment with ethical principles.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log-copy/",
	"title": "Integrate your own Vector Database (OpenSearch)",
	"tags": [],
	"description": "",
	"content": "This task represents a significant contribution to the ongoing development of OPEA, demonstrating how AWS integrations can further enhance Retrieval-Augmented Generation (RAG) performance. Participants will learn how to replace the default vector database with OpenSearch, leveraging its advanced search and retrieval capabilities. Through hands-on exercises, they will contribute to expanding OPEA’s use cases and optimizing its RAG pipelines with scalable, cloud-native solutions.\nLearning Objectives\nUnderstand the Impact of OpenSearch Integration in OPEA: Gain insight into how replacing the vector database with OpenSearch enhances the platform’s capabilities and drives broader enterprise adoption.\nImplement OpenSearch in OPEA: Develop hands-on expertise in integrating OpenSearch within the OPEA ecosystem to optimize search performance.\nEnhance Search and Retrieval Efficiency: Explore how OpenSearch’s advanced capabilities can improve retrieval pipelines, leading to more effective RAG implementations.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log-copy-2/",
	"title": "Extend the LLM inference beyond AWS through Remote Inference",
	"tags": [],
	"description": "",
	"content": "In this lab, you will learn how to integrate inference APIs that are compatible with OpenAI’s API, extending LLM inference beyond AWS. You will explore the necessary modifications to the EKS deployment and the OPEA pipeline to enable remote inference. This lab leverages pre-installed inference APIs in Denvr Dataworks, powered by Intel Gaudi2 AI Accelerator instances. Additionally, you will have the flexibility to migrate these APIs to managed services, demonstrating how OPEA inference services (such as vLLM and TGI) can be deployed across multiple cloud providers to create a seamless and scalable inference pipeline.\nLearning Objectives\nUnderstand the modifications required in OPEA for integrating managed inference services. Explore Intel Gaudi2 AI Accelerators and their role in optimizing inference performance. Learn how OPEA-powered inference services operate in Denvr Dataworks Cloud. Evaluate and compare the response quality, accuracy, and performance of different models across various environments. "
},
{
	"uri": "http://<user_name>.github.io/4-s3log-copy-3/",
	"title": "Test the deployment",
	"tags": [],
	"description": "",
	"content": "This task is a significant contribution to OPEA, demonstrating how integrating AWS Bedrock as the LLM (Large Language Model) can provide a serverless alternative for ChatQnA. The focus is on showcasing OPEA’s ability to seamlessly accommodate different LLMs, offering hands-on experience in customizing RAG (Retrieval-Augmented Generation) setups to work with cloud-native solutions like AWS Bedrock, ensuring scalability and adaptability.\nLearning Objectives\nExplore OPEA’s Flexibility in Integrating LLMs Understand how OPEA’s modular architecture enables seamless integration of various LLMs, including AWS Bedrock. Implement AWS Bedrock as the LLM in OPEA Gain practical experience in replacing the default LLM with AWS Bedrock and modifying the RAG pipeline to leverage its models. Optimize RAG Pipelines for Scalability and Adaptability Learn how to leverage OPEA’s flexibility to integrate and customize LLMs, ensuring enterprise-grade AI solutions that are scalable and adaptable. Key Takeaway This lab highlights OPEA’s ability to integrate AWS Bedrock, reinforcing its flexibility in adapting to diverse technologies and real-world enterprise use cases for AI-driven solutions.\n"
},
{
	"uri": "http://<user_name>.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://<user_name>.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]