[
{
	"uri": "http://<user_name>.github.io/vi/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Trong bối cảnh AI phát triển nhanh chóng hiện nay, việc cập nhật các công cụ và phương pháp tốt nhất để xây dựng ứng dụng AI Tạo Sinh (GenAI) an toàn, đáng tin cậy và hiệu suất cao trở nên thách thức hơn bao giờ hết. Sự tiến bộ liên tục của các mô hình AI, cùng với độ phức tạp ngày càng tăng của môi trường triển khai, đòi hỏi các nhà phát triển phải không ngừng thích ứng với công nghệ, framework và yêu cầu mở rộng mới. Ngoài ra, đảm bảo bảo mật dữ liệu, tính toàn vẹn và tuân thủ các tiêu chuẩn ngành là điều bắt buộc, đặc biệt khi doanh nghiệp cần cân bằng giữa đổi mới và quản lý rủi ro.\nĐối với các doanh nghiệp, khả năng triển khai các giải pháp AI một cách nhanh chóng, mở rộng và an toàn mà không ảnh hưởng đến chất lượng hay thời gian ra thị trường là yếu tố then chốt để duy trì lợi thế cạnh tranh. Điều này đòi hỏi một phương pháp tiếp cận hiệu quả và được tối ưu hóa – đây chính là vai trò của OPEA (Open Platform for Enterprise AI). Bằng cách đơn giản hóa quá trình triển khai AI và tích hợp các phương pháp tốt nhất trong ngành, OPEA giúp các nhà phát triển vượt qua những thách thức cốt lõi và xây dựng các ứng dụng GenAI sẵn sàng cho doanh nghiệp một cách dễ dàng.\nGiới Thiệu OPEA OPEA là một dự án mã nguồn mở thuộc LF AI \u0026amp; Data Foundation, được thiết kế để hỗ trợ phát triển và đánh giá các giải pháp GenAI mở, đa nhà cung cấp, mạnh mẽ và có thể kết hợp. Framework này được tối ưu hóa cho doanh nghiệp, giúp việc tích hợp các quy trình AI an toàn, hiệu suất cao và tiết kiệm chi phí vào hệ thống kinh doanh trở nên dễ dàng hơn. Ban đầu, OPEA tập trung vào Retrieval Augmented Generation (RAG), cho phép doanh nghiệp khai thác những đổi mới tốt nhất trong hệ sinh thái AI đồng thời đảm bảo tính hiệu quả và khả năng mở rộng.\nRetrieval Augmented Generation (RAG) là gì? RAG là một kỹ thuật AI tiên tiến kết hợp giữa khả năng truy xuất thông tin và mô hình ngôn ngữ lớn (LLM). Không giống như các LLM truyền thống chỉ dựa trên kiến thức đã được huấn luyện, RAG có thể truy xuất thông tin theo thời gian thực từ các nguồn dữ liệu bên ngoài (chẳng hạn như cơ sở dữ liệu hoặc tài liệu doanh nghiệp) trước khi tạo ra phản hồi. Cách tiếp cận này giúp nâng cao độ chính xác, tính phù hợp và giảm thiểu sai lệch thông tin (hallucinations), đặc biệt hữu ích cho các ứng dụng doanh nghiệp cần dữ liệu cập nhật và chính xác.\nNội Dung Workshop: Xây Dựng Ứng Dụng RAG với OPEA Trong workshop này, bạn sẽ sử dụng OPEA để xây dựng một ứng dụng GenAI dựa trên RAG có khả năng truy xuất và xử lý dữ liệu bên ngoài, đảm bảo phản hồi chính xác và phù hợp với ngữ cảnh. Nhờ vào kiến trúc module linh hoạt của OPEA, bạn sẽ khám phá cách tích hợp các cơ chế bảo mật, tối ưu hóa hiệu suất và triển khai ứng dụng trên AWS một cách dễ dàng. Trải nghiệm này sẽ giúp bạn hiểu rõ cách mà các giải pháp GenAI dựa trên RAG có thể nâng cao quy trình AI doanh nghiệp, đảm bảo tính bảo mật, khả năng mở rộng và hiệu suất cao trong thực tế.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/2-prerequiste/2.1-createec2/",
	"title": "Sử dụng Workshop Studio",
	"tags": [],
	"description": "",
	"content": "Nếu bạn tham gia các bài lab này trong một sự kiện do AWS tổ chức trên Workshop Studio, bạn không cần tự cấp phát tài nguyên—tất cả các tài nguyên cần thiết cho các mô-đun sẽ được thiết lập sẵn.\nNhững gì sẽ được thiết lập trong tài khoản AWS của bạn? Stack sẽ tự động cấu hình các thành phần sau:\nCụm EKS: Tạo mới cụm EKS có tên opea-eks-cluster. Triển khai một node trong cụm sử dụng M7i.24xlarge instance để đảm bảo hiệu suất cao. CloudFormation Templates: Stack cũng tạo các mẫu CloudFormation cho từng mô-đun:\nMô-đun 1: ChatQnA Default\nMô-đun 2: ChatQnA với Guardrails\nMô-đun 3: ChatQnA với OpenSearch (mã nguồn mở) làm cơ sở dữ liệu vector\nMô-đun 4: ChatQnA với Bedrock làm mô hình LLM\nMô-đun 5: ChatQnA với Remote Inference (Denvr) làm mô hình LLM\nVới các tài nguyên được cấu hình sẵn, bạn có thể tập trung hoàn toàn vào việc khám phá và phát triển ứng dụng GenAI mà không cần lo lắng về việc thiết lập hạ tầng.\nBước 1: Cấu hình quyền truy cập vào cụm EKS Để tương tác với cụm EKS bằng kubectl, bạn cần cấu hình môi trường của mình để nhận diện cụm. Điều này được thực hiện bằng cách cập nhật tệp kubeconfig, tệp này lưu trữ thông tin xác thực và cấu hình truy cập cho Kubernetes.\nĐăng nhập vào AWS Management Console: Bắt đầu bằng cách đăng nhập vào AWS Management Console. Mở Cloud Shell hoặc thiết lập môi trường cục bộ: Trong giao diện console, nhấp vào biểu tượng Cloud Shell để mở terminal được cấu hình sẵn. Nếu sử dụng môi trường cục bộ, hãy đảm bảo bạn đã cài đặt AWS CLI Client và kubectl trên máy cá nhân. Cập nhật kubeconfig: Chạy lệnh sau để cập nhật kubeconfig với thông tin của cụm EKS: Bạn sẽ nhận được một đầu ra xác nhận rằng tệp cấu hình của bạn đã được cập nhật. Giờ đây, bạn có thể sử dụng kubectl để quản lý cụm Kubernetes. Bước 2: Kiểm tra kết nối với cụm EKS Sau khi cập nhật kubeconfig, hãy kiểm tra xem bạn có thể kết nối với cụm hay không bằng cách liệt kê các node: Nếu lệnh thực thi thành công, bạn sẽ thấy đầu ra hiển thị các nút được liên kết với cụm của bạn. Bây giờ bạn đã sẵn sàng khám phá mô-đun bạn chọn và bắt đầu triển khai khối lượng công việc trên cụm EKS của mình!\n"
},
{
	"uri": "http://<user_name>.github.io/vi/4-s3log-copy-3/4.1-updateiamrole/",
	"title": "Thiết lập tích hợp Bedrock",
	"tags": [],
	"description": "",
	"content": "Amazon Bedrock là gì? Amazon Bedrock là dịch vụ được quản lý hoàn toàn, cung cấp quyền truy cập vào nhiều mô hình nền tảng hiệu suất cao (FM) từ các công ty hàng đầu trong ngành như AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI và chính Amazon. Thông qua một API duy nhất, Bedrock cho phép các nhà phát triển xây dựng các ứng dụng AI tạo sinh đồng thời đảm bảo tính bảo mật, quyền riêng tư và các hoạt động AI có trách nhiệm.\nCác nhà phát triển có thể tương tác với Amazon Bedrock thông qua AWS Software Development Kit (SDK) hoặc AWS Command Line Interface (CLI). Bedrock cũng cung cấp các tính năng gốc cho phép người dùng tạo cơ sở kiến ​​thức RAG (Retrieval-Augmented Generation), quy trình làm việc của agentic và lan can. Việc tích hợp Bedrock với OPEA mở rộng quyền truy cập vào nhiều mô hình nền tảng hơn đồng thời tận dụng các khả năng tiên tiến của Bedrock cùng với OPEA.\nKiến trúc thay đổi như thế nào?\nCác tích hợp trong tương lai với OPEA sẽ mở khóa toàn bộ tiềm năng của các khả năng của Amazon Bedrock, bao gồm các mô hình Titan Embedding. Tuy nhiên, đối với mô-đun này, trọng tâm chỉ tập trung vào LLM.\nNhờ kiến ​​trúc mô-đun và có thể hoán đổi của OPEA, hầu hết các thành phần từ thiết lập ChatQnA mặc định (Mô-đun 1) vẫn không thay đổi. Dịch vụ TGI (Hugging Face) hiện được thay thế bằng một vùng chứa Bedrock, tích hợp liền mạch vào triển khai hiện có.\nThành phần kiến ​​trúc được cập nhật:\nchatqna-bedrock Trong triển khai này, khi người dùng gửi tin nhắn qua Giao diện người dùng ChatQnA, tin nhắn đó sẽ được định tuyến đến vùng chứa Bedrock ở phía sau, nơi giao tiếp với Amazon Bedrock để truy xuất và trả về phản hồi. Tích hợp này duy trì kiến ​​trúc ChatQnA đồng thời cải tiến kiến ​​trúc này bằng các khả năng LLM mạnh mẽ của Amazon Bedrock, đảm bảo khả năng mở rộng, hiệu quả và triển khai liền mạch.\nChúng tôi đã sử dụng không gian tên Kubernetes của bedrock để tách các nhóm và dịch vụ liên quan đến triển khai Bedrock. Khi bạn sử dụng kubectl và các lệnh Kubernetes khác trong các ví dụ bên dưới, hãy đảm bảo đủ điều kiện cho lệnh bằng -n bedrock.\nTriển khai ChatQnA bằng Amazon Bedrock LLM\nĐối với phòng thí nghiệm này, chúng tôi đã tạo một tập lệnh thay đổi với triển khai song song đầy đủ của ví dụ ChatQnA trong cùng một cụm Kubernetes mà bạn đã sử dụng. Lệnh sau sẽ triển khai các pod vào cụm trong không gian tên \u0026ldquo;bedrock\u0026rdquo; giống hệt với các pod ChatQnA ban đầu, ngoại trừ các mô hình Bedrock thay vì TGI.\naws cloudformation execute-change-set \u0026ndash;change-set-name bedrock-change-set \u0026ndash;stack-name OpeaBedrockStack\nKích hoạt Mô hình\nMô-đun này hoạt động với hầu hết mọi LLM tạo văn bản được Bedrock hỗ trợ, nhưng vì mục đích của phòng thí nghiệm này, chúng tôi đã sử dụng mô hình Anthropic Claude Haiku 3. Vì vậy, trong khi bạn đang chờ bộ thay đổi triển khai, hãy kích hoạt mô hình của chúng ta trong bảng điều khiển Bedrock:\nChuyển sang vùng us-west-2, bạn có thể thử nghiệm trên các vùng khác nhưng thường thì us-west có nhiều khả năng hơn: Truy cập Amazon Bedrock: Truy cập tab truy cập mô hình: Ở đầu màn hình, nhấp vào nút có nội dung Sửa đổi quyền truy cập mô hình Chọn Claude 3 Haiku Có thể mất một hoặc hai phút để cấp quyền truy cập, nhưng đừng lo lắng sẽ không mất nhiều thời gian hơn thế nữa.\nSau khi bạn xác nhận rằng quyền truy cập mô hình đã được cấp, hãy chuyển trở lại vùng us-east-2 nơi cụm EKS của bạn nằm. Xác nhận triển khai\nBây giờ hãy xác nhận rằng triển khai Bedrock của chúng ta đã hoàn tất. Bạn có thể theo dõi trạng thái của các pod Bedrock bằng lệnh kubectl:\n\u0026hellip;để có đầu ra như thế này:\nCó thể mất vài phút để Bedrock khởi tạo hoàn toàn và khả dụng. Chỉ tiếp tục khi bạn thấy pod chatqna-bedrock-deployment ở trạng thái Đang chạy.\nBây giờ bạn có thể sử dụng Amazon Bedrock trong môi trường của mình.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/4-s3log-copy-2/4.1-updateiamrole/",
	"title": "Tích hợp API suy luận (Denvr Cloud và Intel Gaudi AI Accelerator)",
	"tags": [],
	"description": "",
	"content": "Khi nào bạn nên sử dụng API suy luận được quản lý? Khi các triển khai RAG (Retrieval-Augmented Generation) cấp doanh nghiệp ngày càng phức tạp, việc chuyển đổi một số thành phần nhất định sang các dịch vụ được quản lý ngày càng trở nên có lợi. Cách tiếp cận này giảm thiểu các thách thức trong triển khai, hợp lý hóa việc quản lý cơ sở hạ tầng và đảm bảo khả năng tự động mở rộng liền mạch. Hơn nữa, nó cung cấp cho các nhà phát triển một cách hiệu quả và dễ tiếp cận hơn để tích hợp các Mô hình ngôn ngữ lớn (LLM).\nAPI suy luận của Denvr Cloud được kích hoạt như thế nào? OPEA LLM và các dịch vụ vi mô nhúng chạy trên Intel Gaudi2 AI Accelerators, do Denvr Cloud lưu trữ. Các API này được bảo mật bằng các dịch vụ xác thực OPEA và được tối ưu hóa bằng cân bằng tải để có hiệu suất cao nhất. Trong khi triển khai ban đầu được OPEA xây dựng, Denvr Cloud đã tinh chỉnh và cải tiến hệ thống hơn nữa để đáp ứng các yêu cầu cụ thể về hiệu suất và khả năng mở rộng.\nTham khảo kiến ​​trúc để biết thông tin triển khai:\nKiến trúc OPEA phát triển như thế nào? Thiết kế mô-đun của OPEA cho phép hoán đổi thành phần liền mạch, cho phép hầu hết các thành phần từ ví dụ ChatQnA mặc định (Mô-đun 1) được tích hợp dễ dàng vào các triển khai mới. Kiến trúc được cập nhật vẫn tương tự như thiết lập ChatQnA ban đầu mà không có rào cản nhưng bao gồm một thành phần bổ sung:\nchatqna-llm-uservice chatqna-llm-uservice hoạt động như một dịch vụ bao bọc cho API suy luận, hỗ trợ các điểm cuối tương thích với vLLM và OpenAI. Dịch vụ này tạo điều kiện kết nối với API suy luận và đảm bảo cấu hình phù hợp.\nTriển khai ChatQnA với API suy luận Đối với phòng thí nghiệm này, chúng tôi đã giới thiệu một tập hợp thay đổi cho phép triển khai song song hoàn toàn ví dụ ChatQnA trong cùng một cụm Kubernetes mà bạn đã sử dụng. Chạy lệnh sau sẽ triển khai các pod trong không gian tên \u0026ldquo;remote-inference\u0026rdquo;, phản ánh thiết lập ChatQnA ban đầu nhưng tận dụng các mô hình suy luận từ xa LLM thay vì TGI.\nTruy cập API suy luận: Để sử dụng API suy luận, bạn sẽ cần mã thông báo API do Denvr Cloud cung cấp, chỉ có sẵn để xem trước riêng trong hội thảo. Để yêu cầu quyền truy cập, vui lòng liên hệ với Denvr Cloud theo địa chỉ vaishali@denvrdata.com.\nTriển khai ChatQnA bằng API suy luận Denvr Cloud (Meta Llama 70B 3.1 Instruct): Bây giờ bạn có thể triển khai ChatQnA bằng API suy luận Meta-Llama-3.1-70B-Instruct vào Cụm EKS của mình. Các API này được triển khai trước trên Denvr Cloud với bộ tăng tốc Intel Gaudi2 và dịch vụ vi mô OPEA LLM, đảm bảo hiệu suất và khả năng mở rộng tối ưu.\nLưu ý quan trọng: Nhớ thay thế DenvrClientID và DenvrClientSecret bằng thông tin xác thực mà bạn đã nhận được trước đó từ Denvr Cloud.\nBản kê khai cho ChatQnA-Remote Inference có thể được tìm thấy trong kho lưu trữ ChatQnA GenAIExamples và hướng dẫn triển khai thủ công có thể được tìm thấy tại đây. Hướng dẫn tại đây sử dụng các mẫu AWS CloudFormation do gói AWS Marketplace EKS tạo ra.\nXác minh các dịch vụ mới đã được tạo\nKiểm tra Chat QnA trên bảng điều khiển\nTruy cập vào ngnix POD (sao chép tên pod NGNIX của bạn từ kubectl get pods -n remote-inference và REPLACE chatqna-nginx-xxxxxxxx trên lệnh bên dưới)\nDấu nhắc lệnh của bạn bây giờ sẽ chỉ ra rằng bạn đang ở bên trong vùng chứa, phản ánh sự thay đổi trong môi trường:\nNhận \u0026ldquo;Học sâu là gì? Giải thích trong 20 words\u0026rdquo;*:\nXác minh việc triển khai đã hoàn tất bằng cách xác minh bộ cân bằng tải mới trên bảng điều khiển quản lý của bạn\n"
},
{
	"uri": "http://<user_name>.github.io/vi/4-s3log-copy/4.1-updateiamrole/",
	"title": "Tích hợp OpenSearch",
	"tags": [],
	"description": "",
	"content": "Sử dụng OpenSearch làm Cơ sở dữ liệu Vector cho OPEA\nOpenSearch là giải pháp tìm kiếm, phân tích và cơ sở dữ liệu vector mạnh mẽ. Được phát triển theo OpenSearch Software Foundation, một thành viên của Linux Foundation, giải pháp này cung cấp khả năng quản trị mở cho dự án OpenSearch trên GitHub. Amazon OpenSearch Service cung cấp giải pháp được quản lý giúp đơn giản hóa việc triển khai, mở rộng quy mô và hoạt động trên đám mây AWS. Cho dù là OpenSearch tự quản lý tại chỗ hay trên đám mây, hay sử dụng Amazon OpenSearch Service, OpenSearch đều cung cấp khả năng tìm kiếm vector hiệu suất cao để hỗ trợ tìm kiếm theo từ vựng và ngữ nghĩa nhanh, chính xác và có thể mở rộng quy mô—cho phép kết quả có liên quan hơn cho các trang web và ứng dụng AI tạo sinh.\nVề bản chất, OpenSearch là một công cụ tìm kiếm. Công cụ tìm kiếm đóng vai trò quan trọng trong cuộc sống hàng ngày của chúng ta, giúp chúng ta tìm thông tin, sản phẩm, điểm đến du lịch và nhà hàng. Khi sử dụng công cụ tìm kiếm, người dùng cung cấp truy vấn văn bản, đôi khi được tinh chỉnh bằng các bộ lọc bổ sung và công cụ sẽ trả về danh sách kết quả có liên quan đến mục đích tìm kiếm của người dùng.\nLà một cơ sở dữ liệu chuyên biệt, OpenSearch hoạt động cùng với các kho dữ liệu truyền thống (như cơ sở dữ liệu quan hệ) để cung cấp tìm kiếm thông lượng cao, độ trễ thấp trên khối lượng lớn văn bản phi cấu trúc, trường siêu dữ liệu có cấu trúc và nhúng vectơ. Nền tảng của OpenSearch là chỉ mục, tương tự như bảng cơ sở dữ liệu, lưu trữ tài liệu ở định dạng JSON. Mỗi tài liệu chứa các trường (khóa JSON) và các giá trị tương ứng. Người dùng có thể truy vấn tài liệu bằng văn bản, phạm vi số, ngày, dữ liệu địa lý, v.v. Khi tìm kiếm văn bản phi cấu trúc, OpenSearch xếp hạng tài liệu dựa trên điểm liên quan, ưu tiên các từ hiếm có nhiều trong tài liệu.\nNhững tiến bộ gần đây trong Xử lý ngôn ngữ tự nhiên (NLP) và Mô hình ngôn ngữ lớn (LLM) đã thay đổi cách chúng ta tìm kiếm và truy xuất thông tin. LLM mã hóa ngôn ngữ thành không gian vectơ nhiều chiều, cho phép tìm kiếm khớp với nghĩa thay vì từ chính xác. Chúng cũng tạo ra các phản hồi thực tế, giống con người cho các truy vấn ngôn ngữ tự nhiên.\nSự thay đổi này cho phép người dùng tương tác với các hệ thống truy xuất thông tin thông qua AI đàm thoại, tìm kiếm đa phương thức (văn bản, hình ảnh, âm thanh, video) và khám phá kiến ​​thức nâng cao.\nOpenSearch đóng hai vai trò quan trọng trong các ứng dụng hỗ trợ AI:\nTìm kiếm ngữ nghĩa với nhúng vectơ OpenSearch có thể lưu trữ và truy xuất các nhúng vectơ được tạo bởi các mô hình nhúng. Nó thực hiện tìm kiếm lân cận gần nhất để tìm các tài liệu có vectơ gần nhất với nhúng của truy vấn, cải thiện độ chính xác của tìm kiếm bằng cách khớp ý định thay vì từ khóa.\nTạo tăng cường truy xuất (RAG) cho AI tạo ra OpenSearch có thể hoạt động như một cơ sở kiến ​​thức, truy xuất thông tin có liên quan để nâng cao phản hồi do LLM tạo ra. Bằng cách tích hợp OpenSearch vào OPEA, chúng tôi cải thiện độ chính xác và độ tin cậy của các đầu ra do AI tạo ra bằng cách dựa trên phản hồi trong dữ liệu thực tế, có thật.\nNhờ khả năng hoán đổi được cung cấp bởi OPEA, hầu hết các thành phần từ [ví dụ ChatQnA] mặc định (https://github.com/opea-project/GenAIExamples/tree/main/ChatQnA)(Mô-đun 1). Tất nhiên, đối với mô-đun này, bạn sẽ cần triển khai OpenSearch. Ngoài ra, bạn sẽ sử dụng một trình thu thập và một dịch vụ vi mô chuẩn bị dữ liệu được thiết kế để hoạt động với các API truy vấn và lập chỉ mục của OpenSearch. Bạn có thể tìm thấy các thành phần này trong thư mục thành phần của dự án OPEA GitHub .\nTriển khai ChatQnA bằng OpenSearch làm cơ sở dữ liệu vector Đối với phòng thí nghiệm này, chúng tôi đã tạo một tập hợp thay đổi mà bạn có thể triển khai, bao gồm triển khai song song đầy đủ của ví dụ ChatQnA, trong cùng một cụm Kubernetes mà bạn đã sử dụng. Chúng tôi đã gói tất cả các thay đổi bạn cần trong một tập hợp thay đổi AWS CloudFormation mà bạn triển khai bằng lệnh bên dưới.\nChúng tôi đã sử dụng không gian tên Kubernetes, opensearch, để tách các pod và dịch vụ liên quan đến triển khai OpenSearch. Khi bạn sử dụng kubectl và các lệnh Kubernetes khác trong các ví dụ bên dưới, hãy đảm bảo đủ điều kiện cho lệnh bằng -n opensearch.\nQuay lại Cloud Shell của bạn. Nếu shell đã kết thúc, hãy nhấp vào cửa sổ để mở dòng lệnh mới hoặc sử dụng biểu tượng ở đầu bảng điều khiển AWS để bắt đầu Cloud Shell mới. Sử dụng lệnh sau để triển khai bộ thay đổi OpenSearch.\nOpenSearch sẽ mất vài phút để triển khai. Để kiểm tra trạng thái, bạn có thể sử dụng kubectl để theo dõi trạng thái của các pod OpenSearch. Bạn có thể sử dụng lệnh\nđể có đầu ra như thế này\nChỉ tiếp tục khi bạn thấy pod opensearch-cluster-master-0 ở trạng thái Đang chạy.\nNhững thay đổi đối với ChatQnA cho OpenSearch\nPhần lớn triển khai và thành phần cho ChatQnA giống hệt nhau khi bạn sử dụng OpenSearch làm back-end, so với các back-end cơ sở dữ liệu vector khác. OPEA chứa các thành phần để chuẩn bị và gửi dữ liệu đến cơ sở dữ liệu vector (chatqna-data-prep) và để truy xuất dữ liệu từ cơ sở dữ liệu vector (chatqna-retriever-usvc). Bạn có thể tìm thấy các triển khai OpenSearch cho các thành phần này trên GitHub.\nVề mặt sơ đồ, giờ đây bạn đang nói chuyện với OpenSearch thông qua các thành phần dành riêng cho OpenSearch:\nHiểu về Hệ thống phân tán của OpenSearch OpenSearch là cơ sở dữ liệu phân tán hoạt động trên một cụm các nút, mỗi nút có thể đảm nhiệm các vai trò khác nhau để tối ưu hóa hiệu suất và khả năng mở rộng. Một nút có thể phục vụ nhiều vai trò cùng lúc, cho phép nút thực hiện nhiều chức năng khác nhau trong cụm. Một số vai trò chính của nút bao gồm:\nNút dữ liệu: Các nút này xử lý và lưu trữ dữ liệu. Chúng quản lý các yêu cầu lập chỉ mục và tìm kiếm, biến chúng thành xương sống của kiến ​​trúc phân tán của OpenSearch.\nNút quản lý cụm: Trước đây được gọi là nút chính, các nút này chịu trách nhiệm điều phối cụm. Chúng duy trì và phân phối trạng thái cụm, giám sát hoạt động của nút và đảm bảo tính ổn định của toàn bộ hệ thống. Mặc dù một nút có thể có cả vai trò dữ liệu và vai trò chính, nhưng việc tách các vai trò này thành phần cứng chuyên dụng được khuyến nghị để có hiệu suất và độ tin cậy tốt hơn.\nNút ML: Các nút này hỗ trợ khả năng học máy bằng cách chạy plugin ml-commons. Họ có thể lưu trữ và thực thi các mô hình học máy, bao gồm các mô hình ngôn ngữ lớn, cho phép xử lý dữ liệu và phân tích nâng cao. Bằng cách tận dụng phần cứng không đồng nhất, bạn có thể chỉ định vai trò cho các nút một cách chiến lược dựa trên chức năng dự định của chúng, đảm bảo sử dụng tài nguyên hiệu quả.\nOpenSearch tương tác với người dùng thông qua API RESTful, với các yêu cầu thường được định tuyến qua bộ cân bằng tải để phân phối khối lượng công việc trên các nút dữ liệu khả dụng. Khi một nút dữ liệu nhận được yêu cầu, nó hoạt động như một bộ điều phối, phân bổ các yêu cầu phụ cho các nút dữ liệu có liên quan lưu trữ dữ liệu cần thiết. Mỗi nút trong số các nút này xử lý phần yêu cầu của mình và gửi kết quả trở lại nút điều phối, nút này tổng hợp chúng thành phản hồi cuối cùng trước khi trả về cho máy khách.\nHiểu về cách xử lý dữ liệu của OpenSearch Cốt lõi của cấu trúc dữ liệu của OpenSearch là chỉ mục, đóng vai trò là tập hợp hợp lý các tài liệu chứa dữ liệu có cấu trúc. Để quản lý phân phối dữ liệu hiệu quả, mỗi chỉ mục được chia thành các phân đoạn—các đơn vị lưu trữ độc lập cho phép xử lý song song.\nMỗi phân đoạn về cơ bản là một phiên bản của Apache Lucene, một thư viện tìm kiếm mạnh mẽ dựa trên Java có chức năng đọc và ghi chỉ mục tìm kiếm. Khi tạo chỉ mục, bạn sẽ xác định số lượng phân mảnh chính, xác định cách dữ liệu sẽ được phân vùng ban đầu. Khi lập chỉ mục một tài liệu, OpenSearch sẽ gán tài liệu đó cho một phân mảnh chính bằng chiến lược phân phối ngẫu nhiên, đảm bảo hiệu suất lưu trữ và truy xuất cân bằng trên toàn cụm.\nOpenSearch phân phối các phân mảnh trên các nút dữ liệu khả dụng trong cụm. Bạn cũng có thể đặt một hoặc nhiều bản sao cho chỉ mục. Mỗi bản sao là một bản sao hoàn chỉnh của tập hợp các phân mảnh chính. Ví dụ: nếu bạn có 5 phân mảnh chính và một bản sao, tổng cộng bạn có 10 phân mảnh. Quyết định của bạn về số lượng phân mảnh chính, số lượng bản sao và số lượng nút dữ liệu có tầm quan trọng sống còn đối với sự thành công của cụm trong việc xử lý khối lượng công việc của bạn.\nKhi định cỡ cụm, sau đây là một số hướng dẫn giúp bạn đạt được thành công.\nTài liệu của Amazon OpenSearch Service cũng bao gồm các biện pháp thực hành tốt nhất về một số chủ đề, bao gồm cả việc định cỡ miền OpenSearch Service. Mặc dù các biện pháp thực hành tốt nhất này dành riêng cho từng dịch vụ, nhưng tài liệu nêu chi tiết các nguyên tắc đầu tiên sẽ giúp bạn định cỡ miền tự quản lý của mình.\nTính toán Yêu cầu Lưu trữ cho Siêu dữ liệu và Vectơ trong OpenSearch Để định cỡ cụm OpenSearch của bạn một cách chính xác, hãy bắt đầu bằng cách ước tính dung lượng lưu trữ cần thiết cho cả dữ liệu vectơ và siêu dữ liệu.\nTính toán lưu trữ vectơ Sử dụng công thức sau để xác định dung lượng lưu trữ cần thiết cho vectơ:\nByte trên mỗi chiều × Số chiều × Số vectơ × ( 1 + Số bản sao ) Byte trên mỗi chiều×Số chiều×Số vectơ×(1+Số bản sao) Ví dụ, nếu vectơ của bạn sử dụng số dấu phẩy động (mặc định, 4 byte trên mỗi chiều), có 768 chiều và bạn có 1 triệu vectơ với 1 bản sao, yêu cầu lưu trữ vectơ của bạn là:\n4 × 768 × 1 , 000 , 000 × 2 6 𝐺 𝐵 4×768×1.000.000×2=6GB Tính toán lưu trữ siêu dữ liệu Đối với siêu dữ liệu, hãy sử dụng công thức:\n( 1 + Số lượng bản sao ) × Kích thước dữ liệu nguồn (tính bằng byte) × 1,10 (1+Số lượng bản sao)×Kích thước dữ liệu nguồn (tính bằng byte)×1,10 Ở đây, 1,10 là hệ số lạm phát tính đến sự khác biệt giữa kích thước dữ liệu nguồn thô và kích thước lưu trữ được lập chỉ mục. Nếu kích thước siêu dữ liệu của bạn là 100 GB với một bản sao, tổng dung lượng lưu trữ siêu dữ liệu cần thiết là:\n2 × 100 𝐺 𝐵 × 1.10 220 𝐺 𝐵 2×100GB×1.10=220GB Yêu cầu về tổng dung lượng lưu trữ Cộng dung lượng lưu trữ vector và dung lượng lưu trữ siêu dữ liệu để có được tổng dung lượng lưu trữ cần thiết cho cụm OpenSearch của bạn.\nXác minh triển khai OpenSearch Để xác minh triển khai, bạn sẽ sử dụng chuyển tiếp cổng Kubernetes để gọi các dịch vụ vi mô khác nhau. Bạn có thể xác minh triển khai OpenSearch đang hoạt động bằng cách thực hiện yêu cầu GET đối với URL cơ sở. OpenSearch lắng nghe trên cổng 9200. Sử dụng lệnh sau để ánh xạ cổng 9200 đến cổng cục bộ 9200 của bạn. (Nếu thiết bị đầu cuối Cloud Shell của bạn đã kết thúc, hãy nhấp vào cửa sổ hoặc mở thiết bị đầu cuối mới từ bảng điều khiển AWS.)\nTrong phần hướng dẫn này, bạn sẽ sử dụng chuyển tiếp cổng cho một số dịch vụ. Chuyển tiếp cổng mất vài giây để bắt đầu, hãy đảm bảo đợi dòng xác nhận trông như thế này: Chuyển tiếp từ 127.0.0.1:9200 -\u0026gt; 9200.\nBây giờ bạn có thể truy vấn OpenSearch trực tiếp trên localhost:9200. OpenSearch hỗ trợ giao tiếp được mã hóa thông qua Bảo mật lớp truyền tải (TLS) và được cung cấp kèm theo chứng chỉ demo không được cơ quan có thẩm quyền ký. Đối với mục đích demo, bạn sẽ sử dụng tùy chọn \u0026ndash;insecure cho curl. Kiểm soát truy cập chi tiết của OpenSearch hỗ trợ cơ sở dữ liệu người dùng/mật khẩu nội bộ để xác thực HTTP cơ bản. Nó cũng có thể tích hợp với các nhà cung cấp danh tính Security Assertion Markup Language (SAML) để đăng nhập vào OpenSearch Dashboards (giao diện người dùng của OpenSearch). Bạn sẽ cung cấp xác thực HTTP với yêu cầu curl của mình.\nTruy vấn tới / chỉ trả về tình trạng và thông tin của cụm.\nBạn sẽ nhận được kết quả như sau:\nXin chúc mừng! Bạn đã tạo triển khai được OpenSearch hỗ trợ cho ví dụ ChatQnA của OPEA!\nLàm việc với OPEA ChatQnA và OpenSearch\nBạn có thể làm việc với từng dịch vụ vi mô trong triển khai OpenSearch theo cùng cách bạn đã làm với triển khai Redis. Trong phần này, bạn sẽ thêm một tài liệu mẫu vào OpenSearch thông qua dịch vụ chatqna-data-prep, tạo nhúng và truy vấn OpenSearch bằng nhúng đó thông qua chatqna-retriever-usvc. Khi bạn thực hiện phần này của hướng dẫn, bạn sẽ thấy rằng việc chọn OpenSearch làm DB vectơ của mình có phần minh bạch ở cấp độ chuẩn bị dữ liệu và truy xuất dữ liệu. Dịch vụ OpenSearch cung cấp một mặt tiền cho phép các thành phần OPEA khác \u0026ldquo;chỉ cần sử dụng nó\u0026rdquo;.\nTải tài liệu lên OpenSearch Tải xuống tài liệu pdf mẫu bằng lệnh bên dưới\ncurl -C - -O https://raw.githubusercontent.com/opea-project/GenAIComps/main/comps/third_parties/pathway/src/data/nke-10k-2023.pdf\nBây giờ bạn có thể sử dụng dịch vụ chatqna-data-prep để gửi tài liệu đó đến OpenSearch. Sử dụng lệnh sau để ánh xạ cổng cục bộ 6007 đến cổng dịch vụ 6007.\nkubectl port-forward -n opensearch svc/chatqna-data-prep 6007:6007 \u0026amp;\nChờ cho đến khi bạn thấy thông báo Chuyển tiếp từ 127.0.0.1:6007 -\u0026gt; 6007, sau đó gửi tài liệu.\nViệc chuẩn bị dữ liệu sẽ mất khoảng 30 giây để xử lý tài liệu. Khi hoàn tất, bạn sẽ thấy\n{\u0026ldquo;status\u0026rdquo;: 200, \u0026ldquo;message\u0026rdquo;: \u0026ldquo;Data preparation succeeded\u0026rdquo;}\nĐể xem OPEA sử dụng OpenSearch như thế nào, bạn có thể truy vấn OpenSearch trực tiếp. Một trong những bộ API cốt lõi của OpenSearch là API Compact Aligned Text (_cat). API _cat (hầu hết các API OpenSearch đều bắt đầu bằng dấu gạch dưới, _) là một API quản trị truy xuất thông tin về cụm, chỉ mục, nút, phân đoạn và nhiều thông tin khác của bạn. Để xem các chỉ mục trong OpenSearch, hãy thực hiện lệnh sau (nếu Cloud Shell của bạn đã kết thúc, bạn sẽ cần thiết lập lại chuyển tiếp cổng. Xem hướng dẫn ở trên)\ncurl -XGET \u0026lsquo;https://localhost:9200/_cat/indices?v\u0026rsquo; \u0026ndash;insecure -u admin:strongOpea0!\nBạn sẽ thấy đầu ra như thế này:\nKhi kiểm tra đầu ra của OpenSearch, bạn sẽ thấy nhiều chỉ mục hệ thống khác nhau, thường được thêm dấu chấm. Trong số đó, nhật ký kiểm tra cung cấp thông tin chi tiết về cách sử dụng API, trong khi các chỉ mục như rag-opensearch và file-keys chứa dữ liệu từ ChatQnA.\nMột số chỉ mục có thể xuất hiện với trạng thái sức khỏe màu vàng, cho biết OpenSearch không thể phân bổ đầy đủ một phân đoạn. Điều này thường xảy ra khi một chỉ mục có cả phân đoạn chính và phân đoạn bản sao, nhưng chỉ có một nút khả dụng trong cụm. Vì OpenSearch thực thi rằng các phân đoạn chính và phân đoạn bản sao nằm trên các nút riêng biệt, nên bản sao vẫn chưa được chỉ định, dẫn đến trạng thái màu vàng. Để đảm bảo tính khả dụng cao và khả năng chịu lỗi, tốt nhất là cấu hình ít nhất một bản sao và triển khai nhiều nút dữ liệu.\nCác số liệu lưu trữ cung cấp thêm thông tin chi tiết về cấu trúc chỉ mục. Giá trị pri.store.size biểu thị tổng kích thước trên đĩa của các phân đoạn chính, trong khi store.size bao gồm cả phân đoạn chính và phân đoạn bản sao. Khi chỉ phân bổ các phân đoạn chính, các giá trị này vẫn giống hệt nhau. Số lượng tài liệu phản ánh số lượng tài liệu OpenSearch tồn tại trong một chỉ mục. Vì hệ thống tự động phân đoạn các tài liệu lớn trước khi lập chỉ mục, nên một tài liệu duy nhất—chẳng hạn như báo cáo tài chính của Nike—có thể được chia thành nhiều phân đoạn được lập chỉ mục, trong đó tài liệu Nike trong trường hợp này bao gồm 271 phân đoạn.\nVới dữ liệu được lập chỉ mục thành công, khả năng truy xuất của OpenSearch có thể được kiểm tra bằng dịch vụ vi mô thu thập. Trước khi thực hiện truy vấn, trước tiên phải tạo nhúng bằng dịch vụ nhúng. Ví dụ: để tìm kiếm doanh thu của Nike vào năm 2023, truy vấn phải được chuyển đổi thành nhúng và lưu trữ để xử lý. Sau khi tạo, cần thiết lập chuyển tiếp cổng cho dịch vụ vi mô chatqna-tei để cho phép giao tiếp liền mạch với OpenSearch.\nkubectl port-forward -n opensearch svc/chatqna-tei 9800:80 \u0026amp;\nĐể xác minh rằng cuộc gọi này thành công, bạn có thể sử dụng echo $question_embedding để xem nhúng vector. Bây giờ hãy sử dụng lệnh sau để gọi dịch vụ vi mô của trình thu thập và tìm các tài liệu khớp từ OpenSearch và lưu trữ chúng trong biến bash similar_docs. Thiết lập chuyển tiếp cổng:\nkubectl port-forward -n opensearch svc/chatqna-retriever-usvc 9801:7000 \u0026amp;\nSau khi shell xác nhận rằng nó đang chuyển tiếp, hãy chạy lệnh truy xuất\nMột lần nữa, bạn có thể xác minh việc truy xuất bằng lệnh echo $similar_docs | jq .. Bây giờ bạn có thể khám phá trình xếp hạng lại, liên hệ trực tiếp với tài liệu tương tự và so sánh với câu hỏi Doanh thu của Nike năm 2023 là bao nhiêu?. Dịch vụ chatqna-teirerank mong đợi một mảng các khối văn bản. Thực hiện các lệnh sau để định dạng lại $similar_docs và lưu kết quả vào tệp cục bộ rerank.json\ntexts=$(echo \u0026ldquo;$similar_docs\u0026rdquo; | jq -r \u0026lsquo;[.retrieved_docs[].text | @json]\u0026rsquo;) echo \u0026ldquo;{\u0026quot;query\u0026quot;:\u0026quot;Doanh thu của Nike năm 2023 là bao nhiêu?\u0026quot;, \u0026quot;texts\u0026quot;: $texts}\u0026rdquo; | jq -c . \u0026gt; rerank.json\nBây giờ hãy thiết lập chuyển tiếp cổng cho dịch vụ chatqna-teirerank.\nkubectl port-forward -n opensearch svc/chatqna-teirerank 9802:80 \u0026amp;\nSau khi shell phản hồi rằng nó đang chuyển tiếp, hãy thực hiện lệnh sau để xem kết quả xếp hạng lại\ncurl -X POST localhost:9802/rerank -d @rerank.json\n-H \u0026lsquo;Content-Type: application/json\u0026rsquo;\nBạn sẽ thấy đầu ra như thế này. Trong trường hợp này, mục được truy xuất nhiều nhất vẫn có điểm số tốt nhất sau khi xếp hạng lại, tiếp theo là mục thứ ba, thứ nhất và thứ hai.\n[{\u0026ldquo;index\u0026rdquo;:0,\u0026ldquo;score\u0026rdquo;:0.9984302},{\u0026ldquo;index\u0026rdquo;:3,\u0026ldquo;score\u0026rdquo;:0.9972289},{\u0026ldquo;index\u0026rdquo;:1,\u0026ldquo;score\u0026rdquo;:0.9776342},{\u0026ldquo;index\u0026rdquo;:2,\u0026ldquo;score\u0026rdquo;:0.84730965}]\nOPEA sẽ sử dụng kết quả đầu tiên làm ngữ cảnh cho dịch vụ chatqna-tgi. Bây giờ bạn đã liên hệ với từng dịch vụ vi mô và thấy dữ liệu và truy vấn được chuyển đổi như thế nào trong quá trình xử lý truy vấn của OPEA.\nNhư một bài kiểm tra cuối cùng, bạn có thể gửi truy vấn đến bộ cân bằng tải để xem kết quả. Sử dụng lệnh sau để lấy địa chỉ của bộ cân bằng tải:\nkubectl get ingress -n opensearch\nBạn sẽ thấy kết quả như thế này\nopensearch-ingress alb * opensearch-ingress-156457628.us-east-2.elb.amazonaws.com 80 46h\nSao chép địa chỉ của bộ cân bằng tải và dán vào lệnh bên dưới để xem phản hồi của ChatQnA cho truy vấn \u0026ldquo;Doanh thu của Nike năm 2023 là bao nhiêu?\u0026rdquo;\ncurl http://[TÊN DNS INGRESS CỦA BẠN]/v1/chatqna -H \u0026ldquo;Content-Type: application/json\u0026rdquo; -d \u0026lsquo;{\u0026ldquo;messages\u0026rdquo;: \u0026ldquo;Doanh thu của Nike năm 2023 là bao nhiêu?\u0026rdquo;}\u0026rsquo;\nBạn sẽ thấy văn bản phát trực tuyến có câu trả lời: \u0026ldquo;Trong năm tài chính 2023, NIKE, Inc. Doanh thu là 51,2 tỷ đô la.\u0026rdquo;\nĐể kiểm tra cuối cùng, hãy sao chép-dán URL ingress vào trình duyệt của bạn, tại đó bạn có thể thử truy vấn từ UI.\nKết luận\nTrong nhiệm vụ này, OpenSearch được triển khai dưới dạng cơ sở dữ liệu vector và tích hợp của nó với OPEA đã được thử nghiệm. Các kết nối trực tiếp đã được thực hiện với các dịch vụ vi mô chuẩn bị dữ liệu, nhúng và truy xuất để hiểu sâu hơn về cách OpenSearch tương tác với các thành phần này. Ngoài ra, API truy vấn của OpenSearch đã được khám phá để kiểm tra khả năng truy xuất tài liệu của nó. Để khám phá thêm, mô-đun Explore OpenSearch (Tùy chọn) cung cấp cơ hội để đào sâu hơn vào API và thử nghiệm với các loại truy vấn khác nhau.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/3-accessibilitytoinstances/3.1-deploy-chatqna/",
	"title": "Triển khai ChatQnA",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Trong phần này, bạn sẽ triển khai bản thiết kế OPEA cho ứng dụng dựa trên RAG, ChatQnA, trên môi trường Amazon Elastic Kubernetes Service (EKS). Khám phá thực hành này sẽ giúp bạn hiểu rõ hơn về cách ứng dụng RAG hoạt động trong hệ sinh thái Kubernetes được quản lý, cho phép bạn phân tích các thành phần của ứng dụng và vai trò của chúng trong hệ thống.\nCấu hình triển khai có sẵn thông qua AWS Marketplace và ChatQnA có thể được tìm thấy trong kho lưu trữ GenAIExamples của OPEA.\nChatQnA trên GitHub\nVì bạn đã thiết lập quyền truy cập vào cụm Kubernetes của mình trong phần \u0026ldquo;Thiết lập\u0026rdquo;, giờ đây bạn sẽ tìm hiểu sâu hơn về môi trường đã triển khai để khám phá và tìm hiểu thêm về cấu trúc và chức năng của cụm.\nBước 1: Triển khai mẫu ChatQnA CloudFormation Mở AWS CloudShell và triển khai mẫu ChatQnA CloudFormation vào cụm Amazon EKS của bạn. Thao tác này sẽ khởi tạo quá trình triển khai ứng dụng RAG trong môi trường Kubernetes được quản lý của bạn. Bước 2: Khám phá Tài nguyên cụm Điều hướng đến AWS Management Console và chọn cụm EKS được chỉ định của bạn để xem lại quá trình triển khai. Mỗi cụm bao gồm các cấu hình quan trọng như phiên bản Kubernetes, thiết lập mạng và tùy chọn ghi nhật ký. Kiểm tra các thiết lập này sẽ giúp hiểu sâu hơn về cơ sở hạ tầng của ứng dụng, giúp quản lý hiệu quả và khắc phục sự cố khi cần thiết.\nXem lại Tài nguyên cụm Nhấp vào tab Tài nguyên để xem tất cả các ứng dụng hiện đang chạy trong cụm của bạn, bao gồm ChatQnA và các dịch vụ vi mô liên quan. Đảm bảo rằng tất cả các dịch vụ vi mô từ bản thiết kế OPEA ChatQnA được cài đặt đúng cách bằng cách liệt kê các pod đang hoạt động: Đầu ra sẽ hiển thị tất cả các pod ở trạng thái Đang chạy (1/1), xác nhận rằng ứng dụng đã được triển khai thành công. Tại thời điểm này, bạn đã sẵn sàng để khám phá thêm về việc triển khai và quản lý tài nguyên của mình trong cụm. "
},
{
	"uri": "http://<user_name>.github.io/vi/",
	"title": "Triển khai công cụ hỏi đáp trò chuyện trên Nền tảng mở dành cho AI doanh nghiệp (OPEA) trên AWS",
	"tags": [],
	"description": "",
	"content": "Triển khai công cụ hỏi đáp trò chuyện trên Nền tảng mở dành cho AI doanh nghiệp (OPEA) trên AWS Tổng quan Trong hội thảo thực hành này, bạn sẽ học cách triển khai một ứng dụng Trí tuệ Nhân tạo Tạo sinh (GenAI) mẫu trên AWS EKS bằng cách sử dụng OPEA và AWS CloudFormation. Bạn sẽ khám phá các phương pháp tốt nhất để xây dựng cơ sở hạ tầng GenAI trên AWS và tận dụng các thành phần OPEA để đơn giản hóa quá trình triển khai, giúp bạn tập trung vào việc phát triển và tối ưu hóa ứng dụng mà không cần thiết lập hạ tầng phức tạp từ đầu. Ngoài ra, bạn sẽ áp dụng các cơ chế kiểm soát (guardrails) để quản lý hành vi của ứng dụng và cải thiện hiệu suất bằng cách tích hợp các dịch vụ AWS như OpenSearch.\nĐối với những người hoàn thành sớm, sẽ có các bài tập tùy chọn cung cấp các phương pháp thay thế để chạy mô hình LLM theo hướng serverless trong triển khai RAG của bạn.\nNội dung Giới thiệu Các bước chuẩn bị Tạo kết nối đến máy chủ EC2 Quản lý session logs Port Forwarding Dọn dẹp tài nguyên "
},
{
	"uri": "http://<user_name>.github.io/vi/4-s3log/4.1-updateiamrole/",
	"title": "Triển khai Guardrails",
	"tags": [],
	"description": "",
	"content": "Tại sao cần có lan can bảo vệ? Khi AI ngày càng được nhúng vào các ứng dụng, việc đảm bảo kết quả an toàn, có đạo đức và đáng tin cậy là điều cần thiết. Lan can bảo vệ trong các hệ thống AI—đặc biệt là những hệ thống tương tác với người dùng hoặc đưa ra quyết định tự chủ—giúp điều chỉnh phản hồi, ngăn chặn hành vi không mong muốn và căn chỉnh đầu ra theo các tiêu chuẩn được xác định trước. Nếu không có lan can bảo vệ, các mô hình AI có thể tạo ra nội dung thiên vị hoặc không an toàn, đưa ra quyết định không phù hợp hoặc xử lý sai dữ liệu nhạy cảm.\nViệc triển khai lan can bảo vệ cho phép bạn:\nTăng cường sự an toàn của người dùng – Bằng cách lọc phản hồi, bạn giảm thiểu rủi ro tạo ra nội dung có hại hoặc không phù hợp.\nĐảm bảo tuân thủ quy định – Khi các quy định về quyền riêng tư và bảo mật phát triển, lan can bảo vệ giúp duy trì sự tuân thủ các tiêu chuẩn pháp lý và đạo đức.\nCải thiện độ chính xác và độ tin cậy – Lan can bảo vệ tinh chỉnh đầu ra của AI, giảm tỷ lệ lỗi và đảm bảo hiệu suất nhất quán và đáng tin cậy hơn.\nBằng cách tích hợp lan can bảo vệ, bạn có thể khai thác sức mạnh của AI đồng thời giảm thiểu rủi ro, tạo ra trải nghiệm người dùng an toàn và có trách nhiệm hơn.\nKiến trúc OPEA phát triển như thế nào? Với kiến ​​trúc linh hoạt của OPEA, hầu hết các thành phần cốt lõi từ mô-đun ChatQnA mặc định (Mô-đun 1) vẫn không thay đổi. Tuy nhiên, việc giới thiệu các rào chắn bảo vệ đòi hỏi phải thêm hai thành phần tích hợp liền mạch vào quá trình triển khai:\nchatqna-tgi-guardrails – Dịch vụ siêu nhỏ này chạy máy chủ TGI bằng mô hình meta-llama/Meta-Llama-Guard-2-8B, hoạt động như một bộ lọc an toàn thời gian thực, đảm bảo rằng tất cả các truy vấn đều tuân thủ các giao thức bảo mật đã xác định.\nchatqna-guardrails-usvc – Dịch vụ siêu nhỏ này hoạt động như Trình phân tích điểm cuối hoạt động (OPEA), đánh giá các truy vấn của người dùng và xác định bất kỳ truy vấn nào yêu cầu nội dung có khả năng không an toàn.\nTriển khai rào chắn bảo vệ ChatQnA Kubernetes cho phép cô lập các môi trường phát triển thông qua việc sử dụng nhiều không gian tên. Vì các pod cho đường ống ChatQnA hiện đang được triển khai trong không gian tên mặc định, bạn sẽ triển khai các guardrails trong một không gian tên guardrails riêng biệt để tránh bất kỳ sự gián đoạn nào.\nNếu bạn đã đăng xuất khỏi CloudShell, hãy nhấp vào cửa sổ CloudShell để khởi động lại shell hoặc nhấp vào biểu tượng trong AWS Console để mở CloudShell mới\nTruy cập CloudShell của bạn và triển khai mẫu ChatQnA-Guardrails ClourFormation vào EKS Cluster của bạn Có thể tìm thấy manifest cho ChatQnA-Guardrails trong kho lưu trữ ChatQnA GenAIExamples và hướng dẫn triển khai thủ công có thể tìm thấy tại đây. Các hướng dẫn bạn đang sử dụng trong hội thảo này sử dụng các mẫu AWS CloudFormation do gói AWS Marketplace EKS tạo ra.\nXác minh không gian tên mới đã được tạo Bạn sẽ thấy không gian tên guardrails\nKiểm tra các pod trên không gian tên guardrails. Sẽ mất vài phút để tải xuống các mô hình và để tất cả các dịch vụ hoạt động. Chạy lệnh sau để kiểm tra xem tất cả các dịch vụ có đang chạy không: Chờ cho đến khi đầu ra hiển thị rằng tất cả các dịch vụ bao gồm chatqna-tgi-guardrails và chatqna-guardrails-usvc đang chạy (1/1).\nXác minh việc triển khai đã hoàn tất bằng cách xác minh bộ cân bằng tải mới trên bảng điều khiển quản lý của bạn Chờ 5 phút để bộ cân bằng tải được tạo.\nBằng cách kiểm tra xem cả bộ cân bằng tải và pod đều đang chạy hay không, chúng ta có thể xác nhận rằng việc triển khai đã sẵn sàng và bắt đầu kiểm tra hành vi.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/2-prerequiste/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "Trước khi chạy bất kỳ mô-đun nào, bạn cần thiết lập môi trường AWS đúng cách. Mặc dù bạn có thể triển khai các ví dụ có sẵn từ kho lưu trữ công khai, khuyến nghị sử dụng các ví dụ được dựng sẵn để có trải nghiệm mượt mà và tối ưu hơn. Những ví dụ này được cấu hình sẵn với các thiết lập phù hợp, quyền AWS (IAM roles) và Load Balancer để dễ dàng truy cập dịch vụ, giúp quá trình triển khai diễn ra suôn sẻ.\nNội dung Sử dụng Workshop Studio Sử dụng tài khoản của riêng bạn "
},
{
	"uri": "http://<user_name>.github.io/vi/4-s3log-copy/4.2-creates3bucket/",
	"title": "Khám phá OpenSearch (Tùy chọn)",
	"tags": [],
	"description": "",
	"content": "OpenSearch cung cấp một bộ tính năng mạnh mẽ để tìm kiếm và truy xuất dữ liệu. Phần này khám phá API truy vấn của nó để giới thiệu nhiều khả năng khác nhau.\nTrong OpenSearch, một lược đồ—được gọi là ánh xạ—xác định cách dữ liệu được cấu trúc và lập chỉ mục. Ánh xạ xác định cách các trường trong tài liệu JSON được phân tích và có thể tìm kiếm được. Trong khi OpenSearch bao gồm tính năng phát hiện lược đồ tự động để triển khai nhanh chóng, thì việc xác định thủ công lược đồ trong quá trình tạo chỉ mục thường là cách tiếp cận tốt nhất. Mặc dù OPEA và dịch vụ vi mô OpenSearch đã định cấu hình ánh xạ, nhưng vẫn có thể truy xuất ánh xạ cho chỉ mục nhúng bằng lệnh sau.\ncurl -XGET https://localhost:9200/rag-opensearch/_mapping \u0026ndash;insecure -u admin:strongOpea0! | jq .\nBạn sẽ thấy phản hồi như thế này:\nChỉ mục rag-opensearch bao gồm ba trường: siêu dữ liệu, văn bản và vector_field. Trường siêu dữ liệu lưu trữ JSON lồng nhau, bao gồm trường nguồn, có thể được truy cập trong các truy vấn bằng ký hiệu dấu chấm (ví dụ: metadata.source). Cả metadata.source và text đều được định nghĩa là các trường kiểu văn bản với một trường con từ khóa. Các trường văn bản trải qua quá trình phân tích cú pháp và phân tích thuật ngữ để tạo mã thông báo để khớp, trong khi các trường từ khóa được chuẩn hóa và sử dụng cho các truy vấn khớp chính xác. vector_field là trường kiểu knn_vector được thiết kế để lưu trữ các vectơ có 768 chiều. Công cụ lưu trữ được sử dụng là Thư viện không gian phi số liệu (NMSLIB), sử dụng thuật toán Hidden Navigable Small Worlds (HNSW).\nTruy vấn từ vựng\nOpenSearch là một công cụ tìm kiếm từ vựng ngoài việc là một công cụ tìm kiếm vectơ. Bạn có thể truy vấn nội dung rag bằng các truy vấn văn bản. Sử dụng truy vấn bên dưới để tìm kiếm các tài liệu OpenSearch, với thuật toán xếp hạng TF/IDF mặc định của nó (xem thêm Okapi BM25, xếp hạng dựa trên văn bản).\nNếu Cloud Shell của bạn đã chấm dứt, bạn có thể cần thiết lập lại chuyển tiếp cổng đến dịch vụ vi mô OpenSearch. Xem mô-đun trước để biết hướng dẫn.\nThực hiện lệnh sau để chạy truy vấn\nDòng đầu tiên của lệnh chạy lệnh curl, với URL chứa điểm cuối (localhost:9200, được chuyển tiếp đến dịch vụ vi mô OpenSearch) và thông số kỹ thuật API. Tại đây, yêu cầu được chuyển hướng đến chỉ mục rag-opensearch và gọi API _search. Các dòng sau xác định các tham số TLS và xác thực và sau cờ -d, nội dung yêu cầu chỉ định truy vấn.\nTruy vấn này sử dụng tìm kiếm simple_query_string cho văn bản \u0026ldquo;Doanh thu năm 2023 của Nike là bao nhiêu?\u0026rdquo;. Hàm simple_query_string phân tích văn bản đầu vào, chia nhỏ thành các mã thông báo riêng lẻ—về mặt kỹ thuật được gọi là các thuật ngữ—và khớp chúng với trường văn bản trong tất cả các tài liệu trong chỉ mục. Sau đó, nó sẽ chấm điểm và xếp hạng kết quả dựa trên thuật toán TF/IDF để đưa tài liệu có liên quan nhất lên đầu.\nCác chỉ thị bổ sung trong truy vấn hướng dẫn OpenSearch loại trừ tất cả các trường khỏi phản hồi (\u0026quot;_source\u0026quot;: false—xóa trường này sẽ trả về các giá trị tài liệu gốc), giới hạn kết quả thành một kết quả khớp duy nhất (\u0026ldquo;size\u0026rdquo;: 1) và tô sáng các thuật ngữ khớp trong trường văn bản (\u0026ldquo;highlight\u0026rdquo;: \u0026hellip;).\nPhản hồi của OpenSearch bắt đầu bằng phần siêu dữ liệu bao gồm các chi tiết như thời gian xử lý truy vấn ở phía máy chủ (8 ms trong trường hợp này), liệu truy vấn có hết thời gian chờ hay không và thông tin về các phân đoạn phản hồi. Tiếp theo là phần lượt truy cập, chứa tổng số kết quả khớp, điểm liên quan cao nhất và chính các tài liệu khớp. Mỗi tài liệu bao gồm chỉ mục mà tài liệu đó thuộc về (_index), mã định danh duy nhất (_id), điểm liên quan, các trường nguồn (đã bị loại trừ trong truy vấn này) và các đoạn trích được tô sáng cho biết các thuật ngữ truy vấn khớp với tài liệu ở đâu. Các điểm nổi bật sử dụng thẻ HTML để nhấn mạnh các thuật ngữ khớp.\nTuy nhiên, phản hồi không lý tưởng. Mặc dù xác định đúng các đề cập đến Nike, nhưng nó cũng bao gồm các từ không liên quan như what và is, làm giảm độ chính xác của kết quả.\nBạn có thể thử nghiệm các thuật ngữ truy vấn khác nhau bằng cách sửa đổi trường \u0026ldquo;truy vấn\u0026rdquo; (ví dụ: thay thế \u0026ldquo;doanh thu năm 2023 của nike là gì?\u0026rdquo; bằng các cụm từ khác) để quan sát cách phản hồi thay đổi. Điều chỉnh tham số \u0026ldquo;size\u0026rdquo; thành 2 hoặc nhiều hơn cho phép bạn xem nhiều kết quả cùng một lúc. Hãy thử các truy vấn như \u0026ldquo;workplace policies\u0026rdquo;, \u0026ldquo;footwear apparel\u0026rdquo;, \u0026ldquo;men sales\u0026rdquo; hoặc \u0026ldquo;women sales\u0026rdquo; để khám phá các đầu ra khác nhau.\nĐể tinh chỉnh tìm kiếm, bạn có thể chạy truy vấn k-Nearest-Neighbor (k-NN) chính xác. Trước tiên, hãy truy xuất nhúng cho truy vấn của bạn bằng dịch vụ vi mô chatqna-tei. Dịch vụ vi mô trả về một mảng các mảng, nhưng chỉ cần mảng bên trong cho truy vấn OpenSearch. Lệnh jq trích xuất phần tử đầu tiên và gán nó cho biến $embedding.\nĐể thực thi lệnh, bạn sẽ cần cổng được ánh xạ tới dịch vụ vi mô tei. Sử dụng lệnh ps aux | grep kubectl để kiểm tra các quy trình đang chạy và các cổng được chỉ định của chúng. Nếu bạn đã làm theo hướng dẫn đúng, dịch vụ vi mô chatqna-tei sẽ chạy trên cổng 9800.\nBạn có thể sử dụng echo $embedding để xem nhúng được tạo. Bây giờ, bạn sẽ tạo tệp cục bộ query.json với nhúng được hợp nhất vào truy vấn, sau đó chạy truy vấn k-Nearest-Neighbors (k-NN) chính xác để so sánh nhúng truy vấn với mọi tài liệu (chunk) trong chỉ mục và truy xuất các kết quả khớp gần nhất\nTruy vấn này là truy vấn script_score, sử dụng một tập lệnh đã lưu để thực hiện tính toán điểm k-NN, so sánh vectơ truy vấn với mọi tài liệu trong chỉ mục. Truy vấn script_score bao gồm một truy vấn phụ, mà bạn có thể sử dụng để áp dụng bộ lọc cho các trường không phải vectơ. ChatQnA chỉ gửi khóa tệp trong trường metadata.source, do đó truy vấn này chỉ sử dụng match_all, khớp với mọi tài liệu trong chỉ mục. Phần script của truy vấn chỉ định script knn_score, với các tham số cho script biết trường nào có nhúng vectơ cho tài liệu, truyền nhúng dưới dạng query_value và chỉ định l2 là số liệu khoảng cách (space_type).\nPhản hồi này là chính xác. Kết quả đầu tiên bao gồm văn bản \u0026ldquo;NIKE, Inc. Revenues were $51.2 billion in financial 2023.\u0026rdquo; Không giống như các truy vấn dựa trên văn bản truyền thống, việc tô sáng không được hỗ trợ cho các trường vectơ vì chúng không chứa văn bản nguồn thô. Do đó, nội dung được truy xuất sẽ xuất hiện nguyên trạng từ trường văn bản.\nTìm kiếm k-Nearest-Neighbor (k-NN) chính xác có hiệu quả cao khi xử lý số lượng tài liệu tương đối nhỏ. Tuy nhiên, khi tập dữ liệu mở rộng, độ trễ truy vấn tăng lên đáng kể. Ngoài vài trăm nghìn tài liệu, tìm kiếm k-NN chính xác trở nên chậm một cách không thực tế. Để xử lý hiệu quả các tập dữ liệu lớn hơn, tìm kiếm lân cận gần nhất (ANN) là giải pháp thay thế tốt hơn. Lệnh bên dưới sử dụng thuật toán Hierarchical Navigable Small World (HNSW) để tìm các kết quả khớp gần nhất.\nTruy vấn này là truy vấn knn, sử dụng thuật toán bạn đã chỉ định trong ánh xạ trường để xác định các lân cận gần nhất. Bạn chỉ cần truyền vào một vectơ và một giá trị cho k (số lượng lân cận cần truy xuất) và opensearch sẽ thực hiện phần còn lại.\nTìm kiếm kết hợp với OpenSearch\nMột lần nữa, bạn có thể thấy tài liệu chính xác là kết quả đầu tiên được truy xuất. Sử dụng k-NN gần đúng, bạn có thể mở rộng cơ sở dữ liệu vectơ OpenSearch của mình lên hàng tỷ vectơ và hàng nghìn truy vấn mỗi giây.\nOpenSearch hỗ trợ tìm kiếm kết hợp \u0026ndash; trong đó bạn chỉ định cả truy vấn từ vựng và vectơ, cùng với chiến lược chuẩn hóa và hợp nhất. OpenSearch chạy cả hai truy vấn, chuẩn hóa và hợp nhất các kết quả. Khi bạn thực hiện tìm kiếm kết hợp, bạn thiết lập một Đường ống tìm kiếm và gửi các truy vấn qua đường ống đó. Sử dụng lệnh bên dưới để sử dụng API REST của OpenSearch để thiết lập đường ống tìm kiếm.\nĐường ống này sử dụng chuẩn hóa min/max để thiết lập tất cả các điểm từ vựng và vectơ trong phạm vi [0, 1]. Nó sử dụng trung bình số học để kết hợp các điểm, với trọng số là 0,3 cho mệnh đề truy vấn đầu tiên và 0,7 cho mệnh đề truy vấn thứ hai. Lưu ý, đây không phải là một truy vấn, khi bạn gửi các truy vấn đến đường ống tìm kiếm này, OpenSearch sẽ áp dụng các trọng số. phase_results_processor là một cấu trúc chung linh hoạt - các mệnh đề truy vấn có thể là từ vựng hoặc vectơ.\nĐể sử dụng đường ống, bạn gửi một truy vấn đến API đường ống. Sử dụng lệnh bên dưới để tạo truy vấn trong tệp hybrid_query.json.\nTruy vấn kết hợp này chứa hai truy vấn phụ - truy vấn từ vựng cho \u0026ldquo;doanh thu giày dép\u0026rdquo; và truy vấn vectơ có nhúng biểu diễn \u0026ldquo;Doanh thu Nike 2023 là gì?\u0026rdquo;. Giá trị k, 2, đảm bảo rằng kết quả sẽ chứa tối đa hai kết quả khớp vectơ.\nLệnh curl gửi truy vấn này đến nlp-search-pipeline mà bạn vừa xác định. Bạn sẽ nhận được những kết quả này (chúng tôi đã xóa siêu dữ liệu kết quả và các phần khác để hiển thị đầu ra có liên quan)\nKết quả khớp đầu tiên trong kết quả giống hệt với kết quả được truy xuất bằng truy vấn lân cận gần nhất. Kết quả khớp thứ hai và thứ tư bắt nguồn từ truy vấn từ vựng \u0026ldquo;doanh thu giày dép\u0026rdquo;, trong khi kết quả thứ hai đến từ tìm kiếm vectơ. Cách tiếp cận này kết hợp hiệu quả các hiểu biết trừu tượng, ngữ nghĩa về doanh thu với các kết quả khớp dựa trên từ khóa cụ thể hơn cho \u0026ldquo;doanh thu giày dép\u0026rdquo;.\nKhi thiết kế hệ thống tìm kiếm, việc dự đoán chính xác các loại truy vấn mà người dùng sẽ chạy có thể là một thách thức. Bằng cách tận dụng các truy vấn kết hợp như thế này, bạn có thể cung cấp sự kết hợp cân bằng giữa kết quả ngữ nghĩa và từ vựng, nâng cao độ chính xác và tính liên quan của tìm kiếm.\nBạn cũng có thể thử nghiệm với phân phối trọng số trong nlp-search-pipeline để quan sát tác động của nó lên thứ hạng kết quả. Ví dụ, đặt trọng số truy vấn từ vựng thành 0,9 và trọng số tìm kiếm vectơ thành 0,1 sẽ chuyển kết quả hàng đầu thành \u0026ldquo;Doanh thu giày dép tăng 25% theo cơ sở trung lập tiền tệ,\u0026hellip;\u0026rdquo; với điểm là 0,9. Trong khi đó, kết quả hàng đầu trước đó, \u0026ldquo;Trong năm tài chính 2023, NIKE, Inc. đạt doanh thu kỷ lục là 51,2 tỷ đô la\u0026rdquo;, chuyển lên vị trí thứ hai với điểm là 0,1, tiếp theo là tất cả các kết quả khớp vectơ khác.\nKết luận\nThông qua quy trình này, bạn đã thực hiện và so sánh các kỹ thuật tìm kiếm khác nhau, bao gồm tìm kiếm từ vựng, k-NN chính xác, k-NN gần đúng và tìm kiếm kết hợp, trực tiếp bằng cách sử dụng API của OpenSearch. Hiện tại, OPEA chỉ dựa vào k-NN gần đúng, nhưng các cải tiến trong tương lai có thể giới thiệu các kỹ thuật tìm kiếm kết hợp nâng cao này!\n"
},
{
	"uri": "http://<user_name>.github.io/vi/3-accessibilitytoinstances/3.2-private-instance/",
	"title": "Khám phá triển khai OPEA ChatQnA",
	"tags": [],
	"description": "",
	"content": "Khám phá triển khai dịch vụ vi mô OPEA Bây giờ, chúng ta hãy cùng tìm hiểu sâu hơn về triển khai OPEA ChatQnA RAG. Là bản thiết kế dựa trên dịch vụ vi mô, bản thiết kế này được thiết kế để có khả năng mở rộng, phục hồi và linh hoạt. Trong nhiệm vụ này, bạn sẽ khám phá từng dịch vụ vi mô để hiểu vai trò của chúng trong toàn bộ hệ thống. Bằng cách kiểm tra các thành phần này, bạn sẽ hiểu sâu hơn về cách chúng tương tác và đóng góp vào chức năng của ứng dụng.\nKiến trúc này mang lại một số lợi thế chính:\nKhả năng mở rộng – Mỗi dịch vụ vi mô có thể mở rộng độc lập dựa trên nhu cầu, đảm bảo hiệu suất và sử dụng tài nguyên tối ưu.\nCách ly lỗi – Nếu một dịch vụ gặp sự cố, nó sẽ không làm gián đoạn toàn bộ hệ thống, giúp tăng cường độ tin cậy.\nBảo trì và cập nhật hiệu quả – Các dịch vụ vi mô cho phép cập nhật nhanh chóng và dễ dàng thích ứng với nhu cầu kinh doanh và nhu cầu của người dùng đang thay đổi.\nKiến trúc dịch vụ vi mô OPEA Các triển khai OPEA được xây dựng xung quanh ba thành phần chính:\nMegaservice – Hoạt động như một đơn vị điều phối cho tất cả các dịch vụ vi mô, quản lý quy trình làm việc và đảm bảo tương tác liền mạch giữa các thành phần. Điều này rất cần thiết để phối hợp một ứng dụng đầu cuối với nhiều bộ phận chuyển động. Bạn có thể tìm thêm thông tin chi tiết trong tài liệu OPEA.\nGateway – Hoạt động như điểm vào cho người dùng, định tuyến các yêu cầu đến đến các dịch vụ vi mô phù hợp trong kiến ​​trúc dịch vụ vi mô. Đảm bảo kết nối liền mạch giữa người dùng bên ngoài và các thành phần bên trong.\nMicroservice – Đây là các thành phần chức năng riêng lẻ của ứng dụng, xử lý các tác vụ như nhúng, truy xuất, xử lý LLM và tương tác cơ sở dữ liệu vector. Truy cập các dịch vụ vi mô\nTrước khi bắt đầu khám phá, hãy lưu ý rằng chỉ có dịch vụ cổng và giao diện người dùng được hiển thị bên ngoài. Trong tác vụ này, bạn sẽ trực tiếp truy cập từng dịch vụ vi mô nội bộ cho mục đích thử nghiệm, sử dụng cổng Nginx để định tuyến hiệu quả các yêu cầu đến các dịch vụ nội bộ này.\nBạn sẽ cần ghi chú tất cả các pod đã triển khai.\nkubectl get svc liệt kê tất cả các dịch vụ trong cụm Kubernetes, hiển thị tên, loại, IP cụm và cổng được hiển thị của chúng. Lệnh này cung cấp tổng quan về cách các ứng dụng được hiển thị để truy cập nội bộ hoặc bên ngoài.\nChạy lệnh sau trên CloudShell của bạn:\nBạn sẽ thấy đầu ra tương tự như sau:\nLệnh kubectl get svc được sử dụng để liệt kê các dịch vụ đang chạy trong cụm Kubernetes. Các dịch vụ hoạt động như các điểm vào cho phép giao tiếp giữa các thành phần khác nhau của ứng dụng của bạn. Mỗi dịch vụ có một tên duy nhất (ví dụ: chatqna hoặc chatqna-ui), giúp xác định vai trò của dịch vụ đó trong hệ thống.\nCác dịch vụ Kubernetes có thể được hiển thị theo nhiều cách khác nhau:\nClusterIP – Chỉ có thể truy cập trong cụm, cho phép các thành phần nội bộ giao tiếp an toàn.\nNodePort – Hiển thị dịch vụ bên ngoài thông qua một cổng cụ thể trên mỗi nút, giúp dịch vụ có thể truy cập được bên ngoài cụm. Cluster-IP là địa chỉ nội bộ được các dịch vụ khác sử dụng để tiếp cận ứng dụng. Nếu dịch vụ có thể truy cập được từ bên ngoài cụm, External-IP sẽ được hiển thị. Tuy nhiên, trong trường hợp này, các dịch vụ này hoàn toàn là nội bộ.\nCột Cổng cho biết dịch vụ lắng nghe trên cổng mạng nào. Ví dụ:\nchatqna có thể đang chạy trên cổng 8888/TCP, xử lý giao tiếp nội bộ.\nchatqna-nginx có thể được cấu hình với 80:30144/TCP, trong đó lưu lượng từ cổng 80 được chuyển tiếp đến 30144 cho mục đích định tuyến. Cuối cùng, cột Tuổi hiển thị thời gian dịch vụ đã chạy—ví dụ: 12 giờ cho tất cả các dịch vụ được liệt kê trong kịch bản này.\nBây giờ, chúng ta hãy khám phá kiến ​​trúc chi tiết.\nBước 1: Megaservice (Orchestrator) (POD:chatqna:8888) Megaservice đóng gói toàn bộ logic cho ứng dụng ChatQnA RAG. Dịch vụ siêu nhỏ này có nhiệm vụ xử lý các yêu cầu đến và thực hiện tất cả các hoạt động nội bộ cần thiết để tạo ra các phản hồi phù hợp.\nDịch vụ này không được hiển thị trực tiếp, nhưng bạn có thể truy cập trực tiếp từ LoadBalancer, dịch vụ này sẽ chuyển tiếp yêu cầu.\nTìm bộ cân bằng tải Nhấp vào chatqna-Ingress Lưu ý Tên DNS. Như đã đề cập, đây là URL công khai có thể truy cập bên ngoài. Bạn sẽ sử dụng lệnh curl để gửi yêu cầu đến các điểm cuối API, kiểm tra từng dịch vụ vi mô riêng lẻ. Mục tiêu là đặt một câu hỏi, chẳng hạn như \u0026ldquo;Doanh thu của Nike vào năm 2023 là bao nhiêu?\u0026rdquo; và xác minh rằng API phản hồi chính xác. Bước này đảm bảo rằng tất cả các dịch vụ vi mô trong hệ thống đang hoạt động như mong đợi.\nNếu mọi thứ hoạt động bình thường, bạn sẽ nhận được phản hồi xác nhận rằng quy trình làm việc Retrieval-Augmented Generation (RAG) đang hoạt động.\nTuy nhiên, bạn có thể nhận thấy rằng mô hình không thể cung cấp câu trả lời chính xác. Điều này xảy ra vì nó thiếu ngữ cảnh cần thiết và dựa vào thông tin lỗi thời. Nếu không có quyền truy cập vào dữ liệu hiện tại và có liên quan, mô hình không thể tạo ra phản hồi chính xác. Trong các bước tiếp theo, bạn sẽ cải thiện hệ thống bằng RAG, cho phép mô hình truy xuất thông tin mới nhất, có liên quan theo ngữ cảnh. Điều này sẽ đảm bảo rằng mô hình cung cấp câu trả lời chính xác và có ý nghĩa hơn.\nBây giờ, chúng ta hãy khám phá từng dịch vụ siêu nhỏ một cách chi tiết để hiểu vai trò của nó và cách nó góp phần cải thiện khả năng trả lời câu hỏi chính xác của mô hình.\nBước 2: Dịch vụ siêu nhỏ Mỗi dịch vụ siêu nhỏ tuân theo logic sau để thực hiện một tác vụ trong luồng RAG:\nTrong luồng, bạn có thể quan sát các dịch vụ siêu nhỏ và chúng ta có thể chia luồng RAG thành hai bước:\nNhắc trước: Bước này bao gồm việc chuẩn bị cơ sở kiến ​​thức (KB) bằng cách tải lên các tài liệu có liên quan và đảm bảo rằng thông tin được sắp xếp để truy xuất hiệu quả.\nNhắc nhở: Bước này tập trung vào việc truy xuất dữ liệu có liên quan từ cơ sở kiến ​​thức và sử dụng dữ liệu đó để tạo ra câu trả lời chính xác cho câu hỏi của người dùng.\nNhắc nhở trước Trong bước này, logic là bắt đầu từ một tài liệu (Nike\u0026rsquo;s revenue PDF) và thực hiện tiền xử lý cần thiết để chuẩn bị lưu trữ trong cơ sở dữ liệu. Như đã trình bày, quy trình này chủ yếu liên quan đến 3 dịch vụ siêu nhỏ: chuẩn bị dữ liệu, nhúng và lưu trữ vectơ. Hãy cùng khám phá từng dịch vụ siêu nhỏ\nNhúng dịch vụ siêu nhỏ (POD: chatqna-tei:80) Nhúng là biểu diễn số của một đối tượng—chẳng hạn như từ, cụm từ hoặc tài liệu—trong không gian vectơ liên tục. Trong xử lý ngôn ngữ tự nhiên (NLP), nhúng biến đổi các từ, câu hoặc phân đoạn văn bản thành vectơ—các tập hợp số nắm bắt ý nghĩa, mối quan hệ và ý nghĩa theo ngữ cảnh của chúng. Sự chuyển đổi này cho phép các mô hình máy học xử lý và hiểu văn bản hiệu quả hơn.\nVí dụ, nhúng từ biểu diễn các từ dưới dạng các điểm trong không gian vectơ, trong đó các từ có nghĩa tương tự như \u0026ldquo;vua\u0026rdquo; và \u0026ldquo;nữ hoàng\u0026rdquo; được đặt gần nhau hơn. Mô hình nhúng nắm bắt các mối quan hệ này thông qua phép tính vectơ.\nTrong quá trình đào tạo, nếu mô hình thường xuyên gặp \u0026ldquo;vua\u0026rdquo; khi liên kết với \u0026ldquo;đàn ông\u0026rdquo; và \u0026ldquo;nữ hoàng\u0026rdquo; khi liên kết với \u0026ldquo;phụ nữ\u0026rdquo;, mô hình sẽ học được rằng \u0026ldquo;vua\u0026rdquo; và \u0026ldquo;nữ hoàng\u0026rdquo; có mối quan hệ tương tự với \u0026ldquo;đàn ông\u0026rdquo; và \u0026ldquo;phụ nữ\u0026rdquo;. Điều này cho phép mô hình định vị các từ theo cách phản ánh các mối quan hệ có ý nghĩa, chẳng hạn như liên kết giới tính, trong ngôn ngữ.\nNhúng: Một thành phần chính của RAG Nhúng đóng vai trò quan trọng trong Thế hệ tăng cường truy xuất (RAG) bằng cách tăng cường khả năng xử lý và truy xuất thông tin có liên quan của mô hình. Chúng cung cấp một số lợi thế chính:\nGhi lại ý nghĩa – Các nhúng thể hiện mối quan hệ ngữ nghĩa giữa các từ, cho phép các mô hình RAG hiểu ngữ cảnh, sắc thái và cấu trúc ngôn ngữ sâu hơn. Điều này cải thiện khả năng tạo ra các phản hồi có liên quan và mạch lạc.\nGiảm chiều – Bằng cách chuyển đổi dữ liệu văn bản phức tạp thành các vectơ có kích thước cố định, nhúng giúp xử lý dữ liệu hiệu quả hơn và có khả năng mở rộng, cải thiện hiệu suất của hệ thống.\nNâng cao hiệu suất mô hình – Bằng cách tận dụng các điểm tương đồng về mặt ngữ nghĩa, nhúng cho phép truy xuất thông tin chính xác hơn, tinh chỉnh chất lượng phản hồi được tạo ra và giúp mô hình khái quát hóa tốt hơn trên nhiều truy vấn khác nhau.\nOPEA cung cấp nhiều tùy chọn để chạy các dịch vụ nhúng vi mô, như được nêu chi tiết trong tài liệu nhúng OPEA. Trong trường hợp này, ChatQnA sử dụng dịch vụ nhúng vi mô Hugging Face TEI, chạy mô hình nhúng BAAI/bge-large-en-v1.5 cục bộ.\nVì một số dịch vụ không được hiển thị bên ngoài, bạn sẽ sử dụng pod Nginx để tương tác với chúng thông qua curl. Để thực hiện việc này, mỗi dịch vụ sẽ được truy cập bằng tên DNS nội bộ của dịch vụ đó.\nTruy cập vào ngnix POD (sao chép toàn bộ tên pod NGNIX của bạn từ kubectl get pods và THAY THẾ chatqna-nginx-xxxxxxxx trên lệnh bên dưới) Dấu nhắc lệnh của bạn bây giờ sẽ chỉ ra rằng bạn đang ở bên trong vùng chứa, phản ánh sự thay đổi trong môi trường:\nKhi đã vào bên trong, bây giờ bạn sẽ có quyền truy cập trực tiếp vào các pod bên trong.\nNhận nhúng từ Microservice nhúng cho cụm từ \u0026ldquo;Học sâu là gì?\u0026rdquo;: Câu trả lời sẽ là biểu diễn vectơ của cụm từ \u0026ldquo;Học sâu là gì?\u0026rdquo;. Dịch vụ này trả về nhúng vectơ cho các đầu vào từ REST API.\nDịch vụ cơ sở dữ liệu vectơ (POD: chatqna-redis-vector-db:80) Dịch vụ cơ sở dữ liệu vectơ đóng vai trò quan trọng trong ứng dụng Retrieval-Augmented Generation (RAG) bằng cách lưu trữ và truy xuất các nhúng. Điều này rất cần thiết cho các ứng dụng như ChatQnA, nơi thông tin có liên quan cần được truy xuất hiệu quả dựa trên truy vấn của người dùng.\nSử dụng Redis làm Cơ sở dữ liệu vectơ Trong tác vụ này, Redis được sử dụng làm cơ sở dữ liệu vectơ. Tuy nhiên, OPEA hỗ trợ nhiều phương án thay thế, có thể tìm thấy trong kho lưu trữ vectơ OPEA.\nCơ sở dữ liệu vectơ (VDB) được thiết kế riêng để lưu trữ và quản lý các vectơ có chiều cao, biểu diễn các từ, câu hoặc hình ảnh dưới dạng số. Trong AI và học máy, các vectơ này—còn được gọi là nhúng—ghi lại ý nghĩa và mối quan hệ giữa các điểm dữ liệu, cho phép xử lý và truy xuất hiệu quả.\nDịch vụ vi mô chuẩn bị dữ liệu (POD: chatqna-data-prep:6007) Dịch vụ vi mô chuẩn bị dữ liệu (Dataprep) chịu trách nhiệm định dạng và xử lý trước dữ liệu để có thể chuyển đổi thành nhúng và lưu trữ trong cơ sở dữ liệu vector. Điều này đảm bảo dữ liệu sạch, có cấu trúc và sẵn sàng để truy xuất hiệu quả.\nChức năng chính của dịch vụ vi mô chuẩn bị dữ liệu Nhận dữ liệu thô (ví dụ: tài liệu hoặc báo cáo).\nXử lý và chia nhỏ dữ liệu thành các phân đoạn nhỏ hơn.\nGửi dữ liệu đã xử lý đến Dịch vụ vi mô nhúng để vector hóa.\nLưu trữ các nhúng kết quả trong Cơ sở dữ liệu vector.\nVì các cơ sở dữ liệu vector khác nhau có các yêu cầu định dạng dữ liệu riêng, nên Dịch vụ vi mô chuẩn bị dữ liệu đảm bảo khả năng tương thích với cơ sở dữ liệu đã chọn.\nKiểm tra các dịch vụ vi mô Để xác minh chức năng của hệ thống và giúp mô hình trả lời câu hỏi ban đầu— \u0026ldquo;Doanh thu của Nike năm 2023 là bao nhiêu?\u0026quot;—bạn sẽ cần tải lên tệp ngữ cảnh có liên quan (báo cáo doanh thu) để có thể xử lý.\nĐể thực hiện việc này, hãy tải xuống mẫu báo cáo doanh thu của Nike vào pod Nginx bằng lệnh bên dưới. (Nếu bạn không còn đăng nhập vào Nginx pod, hãy đảm bảo đăng nhập lại trước khi tiếp tục.)\nThực hiện lệnh sau để tải xuống báo cáo doanh thu Nike mẫu vào nginx pod (nếu bạn không còn đăng nhập vào NGinx pod, hãy đảm bảo sử dụng lệnh trên để đăng nhập lại):\nTải xuống tài liệu vào microservice: Cung cấp tài liệu cho cơ sở kiến ​​thức (Vectord) (Sẽ mất khoảng 30 giây): Sau khi chạy lệnh trước đó, bạn sẽ nhận được thông báo xác nhận như bên dưới. Lệnh này đã cập nhật cơ sở kiến ​​thức bằng cách tải lên tệp cục bộ để xử lý.\nAPI microservice chuẩn bị dữ liệu có thể truy xuất thông tin về danh sách các tệp được lưu trữ trong cơ sở dữ liệu vector.\nKiểm tra xem tài liệu đã được tải lên chưa: Sau khi chạy lệnh trước, bạn sẽ nhận được thông báo xác nhận.\nXin chúc mừng! Bạn đã chuẩn bị thành công cơ sở kiến ​​thức của mình. Bây giờ bạn sẽ khám phá các dịch vụ vi mô liên quan đến xử lý nhanh chóng.\nBước 3: Nhắc nhở Sau khi cơ sở kiến ​​thức được thiết lập, bạn có thể bắt đầu tương tác với ứng dụng bằng cách đặt các câu hỏi theo ngữ cảnh cụ thể. Retrieval-Augmented Generation (RAG) đảm bảo rằng các phản hồi vừa chính xác vừa dựa trên dữ liệu có liên quan.\nQuy trình bắt đầu bằng việc ứng dụng truy xuất thông tin có liên quan nhất từ ​​cơ sở kiến ​​thức để trả lời truy vấn của người dùng. Bước này đảm bảo rằng Mô hình ngôn ngữ lớn (LLM) có quyền truy cập vào ngữ cảnh chính xác và cập nhật để tạo ra phản hồi có thông tin.\nTiếp theo, thông tin đã truy xuất được kết hợp với lời nhắc nhập của người dùng và gửi đến LLM. Lời nhắc được làm giàu này nâng cao khả năng của mô hình trong việc cung cấp các câu trả lời không chỉ dựa trên kiến ​​thức đã được đào tạo trước mà còn được hỗ trợ bởi dữ liệu bên ngoài trong thế giới thực.\nCuối cùng, bạn sẽ thấy cách LLM xử lý lời nhắc được làm giàu này để tạo ra phản hồi mạch lạc và chính xác theo ngữ cảnh. Bằng cách tận dụng RAG, ứng dụng cung cấp các câu trả lời có liên quan cao, dựa trên thông tin mới nhất từ ​​cơ sở kiến ​​thức.\nCác dịch vụ vi mô liên quan đến giai đoạn này bao gồm:\nNhúng Cơ sở dữ liệu vectơ Trình thu thập Xếp hạng lại LLM Dịch vụ vi mô thu thập (POD: chatqna-retriever-usvc:7000) Dịch vụ vi mô thu thập chịu trách nhiệm định vị thông tin có liên quan nhất trong cơ sở kiến ​​thức và trả về các tài liệu khớp chặt chẽ với truy vấn của người dùng. Nó tương tác với nhiều hệ thống phụ trợ lưu trữ kiến ​​thức và cung cấp API để truy xuất dữ liệu khớp nhất.\nCác cơ sở kiến ​​thức khác nhau sử dụng các phương pháp truy xuất khác nhau:\nCơ sở dữ liệu vectơ sử dụng phương pháp khớp tương tự vectơ giữa câu hỏi của người dùng và nhúng tài liệu được lưu trữ. Cơ sở dữ liệu đồ thị tận dụng vị trí đồ thị để tìm thông tin liên quan. Cơ sở dữ liệu quan hệ dựa vào phương pháp khớp chuỗi và biểu thức chính quy để định vị văn bản có liên quan. Trong nhiệm vụ này, bạn sẽ sử dụng Redis làm cơ sở dữ liệu vectơ và truy xuất thông tin thông qua trình thu thập Redis.\nVì việc truy xuất vectơ dựa vào nhúng, trước tiên bạn cần tạo nhúng cho câu hỏi: \u0026ldquo;Doanh thu của Nike năm 2023 là bao nhiêu?\u0026rdquo;\nThao tác này sẽ cho phép trình thu thập tìm kiếm trong cơ sở kiến ​​thức để tìm tài liệu có liên quan nhất—chẳng hạn như báo cáo doanh thu của Nike mà bạn đã tải lên ở bước trước.\nĐể tạo nhúng, hãy sử dụng dịch vụ vi mô chatqna-tei. (Đảm bảo bạn đã đăng nhập vào pod Nginx trước khi tiếp tục.)\nTạo nhúng và lưu cục bộ (embed_question): Bạn sẽ nhận được thông tin chi tiết về tác vụ viết:\nKiểm tra xem nhúng của bạn đã được lưu chưa: echo $embed_question\nBạn sẽ có thể thấy các vectơ mà dịch vụ vi mô nhúng tạo ra. Bây giờ bạn có thể sử dụng dịch vụ vi mô trình thu thập để có được thông tin tương tự nhất từ ​​cơ sở kiến ​​thức của mình.\nLấy và lưu các vectơ tương tự từ embed_question ban đầu tại địa phương similar_docs: similar_docs=$(curl chatqna-retriever-usvc:7000/v1/retrieval -X POST -d \u0026ldquo;{\u0026quot;text\u0026quot;:\u0026quot;test\u0026quot;,\u0026quot;embedding\u0026quot;:${embed_question}}\u0026rdquo; -H \u0026lsquo;Content-Type: application/json\u0026rsquo;)\nBằng cách xem kết quả đầu ra trước đó, bạn có thể thấy các đoạn tương tự nhất (TOP_3) từ tài liệu Báo cáo doanh thu của Nike và câu hỏi \u0026ldquo;Doanh thu của Nike năm 2023 là bao nhiêu?\u0026rdquo;.\necho $similar_docs\nKết quả đầu ra sau đây đã được định dạng để dễ đọc hơn. Kết quả của bạn sẽ được trình bày dưới dạng văn bản thuần túy và có thể hơi khác một chút do thuật toán tìm kiếm tương tự. Tuy nhiên, bạn có thể kiểm tra lại xem các tài liệu đã truy xuất có liên quan đến truy vấn ban đầu của bạn không.\nỨng dụng sẽ sử dụng thông tin đó làm ngữ cảnh để nhắc LLM, nhưng vẫn còn một bước nữa mà bạn cần thực hiện để tinh chỉnh và kiểm tra chất lượng của các tài liệu đã truy xuất đó: trình xếp hạng lại.\nVi dịch vụ xếp hạng lại (POD: chatqna-teirerank:80) Vi dịch vụ xếp hạng lại đóng vai trò quan trọng trong tìm kiếm ngữ nghĩa, tận dụng các mô hình xếp hạng lại để tăng cường tính liên quan của các kết quả đã truy xuất. Khi được cung cấp truy vấn của người dùng và một tập hợp các tài liệu, vi dịch vụ này sẽ sắp xếp lại các tài liệu dựa trên mức độ tương đồng về mặt ngữ nghĩa của chúng với truy vấn, đảm bảo rằng các kết quả có liên quan nhất sẽ xuất hiện đầu tiên.\nXếp hạng lại đặc biệt có giá trị trong các hệ thống truy xuất văn bản, trong đó các tài liệu ban đầu được truy xuất bằng cách sử dụng:\nNhúng dày đặc, nắm bắt ý nghĩa ngữ nghĩa sâu sắc. Tìm kiếm từ vựng thưa thớt, dựa trên việc khớp dựa trên từ khóa. Mặc dù các phương pháp truy xuất này có hiệu quả, nhưng mô hình xếp hạng lại sẽ tinh chỉnh các kết quả bằng cách tối ưu hóa thứ tự của các tài liệu đã truy xuất. Bước này cải thiện đáng kể độ chính xác, đảm bảo đầu ra cuối cùng có liên quan hơn, chính xác hơn và phù hợp hơn về mặt ngữ cảnh với truy vấn của người dùng.\nOPEA có nhiều tùy chọn để xếp hạng lại. Đối với phòng thí nghiệm này, bạn sẽ sử dụng Hugging Face TEI để xếp hạng lại. Đây là dịch vụ vi mô chatqna-teirerank trong cụm của bạn.\nTrình xếp hạng lại sẽ sử dụng similar_docs từ giai đoạn trước và so sánh với câu hỏi Doanh thu Nike năm 2023 là bao nhiêu? để kiểm tra chất lượng của các tài liệu đã truy xuất.\nTrích xuất 3 đoạn văn bản đã truy xuất và lưu chúng trong một biến mới để xếp hạng lại:\nCài đặt các phụ thuộc jq để định dạng similar_docs echo -e \u0026ldquo;deb http://deb.debian.org/debian bookworm main contrib non-free\\ndeb http://security.debian.org/debian-security bookworm-security main contrib non-free\\ndeb http://deb.debian.org/debian bookworm-updates main contrib non-free\u0026rdquo; \u0026gt; /etc/apt/sources.list \u0026amp;\u0026amp; apt update \u0026amp;\u0026amp; apt install -y jq\nTrích xuất và định dạng các văn bản thành một mảng chuỗi JSON hợp lệ texts=$(echo \u0026ldquo;$similar_docs\u0026rdquo; | jq -r \u0026lsquo;[.retrieved_docs[].text | @json]\u0026rsquo;)\nGửi yêu cầu đến dịch vụ vi mô với truy vấn và các văn bản đã định dạng: curl -X POST chatqna-teirerank:80/rerank -d \u0026ldquo;{\u0026quot;query\u0026quot;:\u0026quot;Doanh thu của Nike năm 2023 là bao nhiêu?\u0026quot;, \u0026quot;texts\u0026quot;: $texts}\u0026rdquo; -H \u0026lsquo;Content-Type: application/json\u0026rsquo;\nResponse:\nĐầu ra sau đây đã được định dạng để dễ đọc hơn. Kết quả của bạn được hiển thị ở dạng văn bản thuần túy và có thể thay đổi đôi chút do thuật toán tìm kiếm tương đồng. Các tài liệu được truy xuất được xếp hạng theo mức độ tương đồng với truy vấn của bạn, với chỉ mục được xếp hạng cao nhất thể hiện sự khớp có liên quan nhất. Bạn có thể xác nhận rằng tài liệu được xếp hạng cao nhất tương ứng với tài liệu phù hợp nhất với truy vấn của bạn.\nMáy chủ phản hồi bằng một mảng JSON chứa các đối tượng có hai trường: chỉ mục và điểm. Điều này cho biết cách các đoạn trích được xếp hạng dựa trên mức độ liên quan của chúng với truy vấn: {\u0026ldquo;index\u0026rdquo;:2,\u0026ldquo;score\u0026rdquo;:0.9972289} có nghĩa là văn bản đầu tiên (chỉ mục 0) có điểm liên quan cao khoảng 0,7982. {\u0026ldquo;index\u0026rdquo;:0,\u0026ldquo;score\u0026rdquo;:0.9776342},{\u0026ldquo;index\u0026rdquo;:3,\u0026ldquo;score\u0026rdquo;:0.9296986},{\u0026ldquo;index\u0026rdquo;:1,\u0026ldquo;score\u0026rdquo;:0.84730965} cho biết các đoạn trích khác (chỉ mục 3, 1 và 2) có điểm thấp hơn nhiều.\nNhư bạn có thể thấy từ similar_doc, id=2 có thông tin bên dưới, trong đó nó CHÍNH XÁC đề cập đến doanh thu năm 2023!\nChỉ đoạn trích đầu tiên sẽ được sử dụng để nhắc LLM.\nLLM Microservice (POD: chatqna-tgi:80) Cốt lõi của ứng dụng RAG (Retrieval-Augmented Generation) là Mô hình ngôn ngữ lớn (LLM), đóng vai trò quan trọng trong việc tạo ra phản hồi. Bằng cách tận dụng RAG, hệ thống nâng cao hiệu suất của LLM, đảm bảo phản hồi chính xác, có liên quan và nhận thức được ngữ cảnh.\nCác loại LLM LLM thường được chia thành hai loại chính, mỗi loại có điểm mạnh và điểm yếu riêng:\nMô hình nguồn đóng Các mô hình độc quyền này được phát triển bởi các công ty công nghệ lớn như Amazon Web Services (AWS), OpenAI và Google. Chúng được đào tạo trên các tập dữ liệu mở rộng và được tối ưu hóa để có đầu ra chất lượng cao, đáng tin cậy. Tuy nhiên, chúng đi kèm với một số hạn chế nhất định: Tùy chỉnh hạn chế: Người dùng có quyền kiểm soát tối thiểu đối với việc tinh chỉnh. Chi phí cao hơn: Quyền truy cập thường bị giới hạn và có thể tốn kém. Mối quan ngại về chủ quyền dữ liệu: Quyền truy cập API có thể hạn chế việc sử dụng trong các ứng dụng yêu cầu quản trị dữ liệu chặt chẽ. Mô hình nguồn mở Có sẵn miễn phí để sử dụng và sửa đổi, LLM nguồn mở cung cấp tính linh hoạt và khả năng kiểm soát cao hơn. Chúng cho phép người dùng tùy chỉnh và tinh chỉnh các mô hình theo nhu cầu cụ thể. Chạy các mô hình nguồn mở cục bộ hoặc trên cơ sở hạ tầng đám mây riêng đảm bảo quyền riêng tư dữ liệu và hiệu quả chi phí tốt hơn. Tuy nhiên, chúng yêu cầu: Chuyên môn kỹ thuật: Việc triển khai và tối ưu hóa các mô hình nguồn mở có thể phức tạp. Tài nguyên tính toán: Để đạt được hiệu suất tương đương với các mô hình đóng thường đòi hỏi phần cứng mạnh mẽ. Tích hợp linh hoạt với OPEA: Kiến trúc dịch vụ vi mô này hỗ trợ cả mô hình đóng và mô hình nguồn mở, cung cấp tính linh hoạt để lựa chọn mô hình phù hợp nhất cho ứng dụng của bạn. Trong ví dụ này, mô hình TGI (Suy luận tạo văn bản) từ Hugging Face được sử dụng. Kiểm tra dịch vụ vi mô LLM: Để xác minh chức năng của nó, bạn có thể trực tiếp nhắc TGI LLM bằng một câu hỏi mẫu: \u0026ldquo;Doanh thu của Nike vào năm 2023 là bao nhiêu?\u0026rdquo;\nBài kiểm tra này sẽ chứng minh mô hình có thể truy xuất và tạo phản hồi có thông tin tốt như thế nào dựa trên cơ sở kiến ​​thức đã tải.\nTrực tiếp nhắc nhở TGI(LLM) Microservice: Mô hình sẽ cung cấp cho bạn câu trả lời cho lời nhắc như sau:\n**\u0026ldquo;generated_text\u0026rdquo;:\u0026rdquo; Doanh thu của Nike năm 2023 chưa được báo cáo vì vẫn đang trong quý 4. Năm tài chính đầy đủ trước đó—tức là năm 2022—đã mang lại 48,9 tỷ đô la doanh thu cho công ty đồ thể thao đa quốc gia của Mỹ. Họ xử lý việc thiết kế, phát triển, sản xuất và tiếp thị/bán hàng trên toàn thế giới của một danh mục sản phẩm đa dạng. Từ khi thành lập vào năm 1964 với tên gọi Blue Ribbon Sports, công ty đã được đổi tên thành Nike, Inc., vào năm 1978. Logo Jumpman (ví dụ), bao gồm Michael\n"
},
{
	"uri": "http://<user_name>.github.io/vi/3-accessibilitytoinstances/3.3-private-instance-copy/",
	"title": "Kiểm tra việc triển khai và xác minh quy trình làm việc RAG",
	"tags": [],
	"description": "",
	"content": "Hiểu RAG và sử dụng UI Bây giờ bạn đã xác minh tất cả các dịch vụ đang chạy, hãy cùng xem UI do triển khai cung cấp.\nĐể truy cập UI, hãy mở bất kỳ trình duyệt nào và truy cập DNS của ChatQnA Load Balancer: http://chatqna-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Sửa đổi bằng URL chatqna-ingressDNS của bạn)\nTrong UI, bạn có thể thấy chatbot tương tác với nó\nĐể xác minh UI, hãy tiếp tục và hỏi\nCâu trả lời lại đúng vì chúng ta đã lập chỉ mục cơ sở kiến ​​thức của mình ở bước trước.\nHãy thử một cái gì đó khác. Ứng dụng có thể trả lời về OPEA không:\nBạn có thể nhận thấy rằng phản hồi ban đầu của chatbot đã lỗi thời hoặc thiếu thông tin chi tiết cụ thể về OPEA. Nguyên nhân là do OPEA là một dự án tương đối mới và không được đưa vào tập dữ liệu được sử dụng để đào tạo mô hình ngôn ngữ. Vì hầu hết các mô hình ngôn ngữ đều là tĩnh—có nghĩa là chúng dựa vào dữ liệu có sẵn tại thời điểm đào tạo—nên chúng không thể tự động kết hợp các phát triển gần đây hoặc các chủ đề mới nổi như OPEA.\nTuy nhiên, RAG cung cấp giải pháp bằng cách cho phép truy xuất ngữ cảnh theo thời gian thực. Trong UI, bạn sẽ tìm thấy tùy chọn tải lên thông tin ngữ cảnh có liên quan. Khi bạn thực hiện việc này, tài liệu sẽ được gửi đến dịch vụ vi mô DataPrep, tại đó tài liệu sẽ được chuyển đổi thành nhúng và lưu trữ trong Cơ sở dữ liệu Vector.\nBằng cách tải lên tài liệu hoặc liên kết, bạn sẽ mở rộng hiệu quả cơ sở kiến ​​thức của chatbot bằng thông tin mới nhất, cải thiện tính liên quan và độ chính xác của phản hồi.\nViệc triển khai cho phép bạn tải lên tệp hoặc trang web. Đối với trường hợp này, hãy sử dụng trang web OPEA:\nNhấp vào biểu tượng tải lên để mở bảng điều khiển bên phải Nhấp vào Dán liên kết Sao chép/dán văn bản https://opea-project.github.io/latest/introduction/index.html vào hộp nhập Nhấp vào Xác nhận để bắt đầu quá trình lập chỉ mục Khi quá trình lập chỉ mục hoàn tất, bạn sẽ thấy một biểu tượng được thêm vào bên dưới hộp văn bản, có nhãn là https://opea-project.github.io/latest/introduction/index.html\nHỏi \u0026ldquo;OPEA là gì?\u0026rdquo; một lần nữa để xem câu trả lời đã cập nhật.\nLần này, chatbot phản hồi chính xác dựa trên dữ liệu mà nó đã thêm vào lời nhắc từ nguồn mới, trang web OPEA.\nKết luận\nTrong nhiệm vụ này, bạn đã khám phá cấu trúc cốt lõi của ứng dụng RAG, hiểu rõ hơn về cách từng thành phần hoạt động và tương tác trong hệ thống. Từ việc truy xuất thông tin có liên quan đến việc tạo phản hồi chính xác, mọi phần đều đóng vai trò quan trọng trong quy trình làm việc RAG của OPEA—nâng cao tính liên quan của phản hồi thông qua việc truy xuất trong khi cải thiện độ chính xác với mô hình ngôn ngữ nâng cao. Buổi thực hành này cung cấp hiểu biết rõ ràng về cách OPEA tận dụng RAG để xử lý các truy vấn phức tạp một cách hiệu quả và tinh chỉnh hiệu suất mô hình thông qua tích hợp thành phần liền mạch.\nTrong nhiệm vụ tiếp theo, bạn sẽ triển khai các rào cản cho chatbot. Các rào cản này rất cần thiết để phát hiện và giảm thiểu sự thiên vị, đảm bảo rằng các phản hồi do AI tạo ra vẫn có trách nhiệm, công bằng và phù hợp với các nguyên tắc AI có đạo đức.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/2-prerequiste/2.2-createiamrole/",
	"title": "Sử dụng tài khoản của riêng bạn",
	"tags": [],
	"description": "",
	"content": "Hướng dẫn thiết lập môi trường cho phòng thí nghiệm AWS Nếu bạn KHÔNG thực hiện các phòng thí nghiệm này trong một sự kiện do AWS tổ chức trên Workshop Studio, bạn cần chuẩn bị môi trường trước khi bắt đầu. Lưu ý rằng các tài nguyên này sẽ phát sinh chi phí, vì vậy hãy nhớ dọn dẹp sau khi hoàn thành.\nBước 1: Cấu hình môi trường Thiết lập tài khoản Thực hiện các bước sau để cấu hình tài khoản của bạn:\nNhấn vào Launch Stack để khởi chạy CloudFormation với các giá trị được cấu hình sẵn trong khu vực us-east-1.\nLaunch Stack\nNếu muốn chạy workshop ở khu vực khác, hãy thay đổi khu vực phù hợp. Stack sẽ thiết lập các thành phần sau: Cụm EKS (Amazon Elastic Kubernetes Service):\n-Tạo một cụm EKS mới có tên opea-eks-cluster. -Triển khai một node trong cụm sử dụng M7i.24xlarge.\nCác mẫu CloudFormation: Cung cấp các mẫu CloudFormation để triển khai các mô-đun sau: Module 1:ChatQnA Default\nModule 2:ChatQnA with Guardrails\nModule 3:ChatQnA with OpenSearch (một cơ sở dữ liệu vector mã nguồn mở)\nModule 4:ChatQnA with Remote Inference (Denvr) làm mô hình LLM\nModule 5:ChatQnA with Bedrock làm mô hình LLM\nCấu hình trước khi chạy Stack Trước khi chạy stack, trên trang Quick create stack, thực hiện các bước sau:\nHuggingFaceToken:Cung cấp mã token để tải xuống mô hình từ Hugging Face. Nếu sử dụng tính năng Guardrails, đảm bảo token có quyền truy cập vào mô hình meta-llama/Meta-Llama-Guard-2-8B.\nModelID:OPEA sử dụng Text Generation Inference toolkit. Chọn mô hình từ danh sách các mô hình được hỗ trợ trên Hugging Face và nhập Model ID.\nOpeaRoleArn:Nhập ARN hoặc tên của role mà bạn đang sử dụng.\nNếu không chắc, hãy kiểm tra thông tin người dùng ở góc trên bên phải màn hình. Nếu tên không có dấu /, sao chép toàn bộ. Nếu có dấu /, chỉ lấy phần trước dấu /. Ví dụ: Nếu thấy \u0026ldquo;USER\u0026rdquo;, nhập \u0026ldquo;USER\u0026rdquo;. Nếu thấy \u0026ldquo;ADMIN-ROLE/USER\u0026rdquo;, chỉ nhập \u0026ldquo;ADMIN-ROLE\u0026rdquo;. Lấy ARN bằng lệnh AWS CLI (nếu cần). aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text | awk -F: \u0026lsquo;{print $NF}\u0026rsquo; | (read id; if [[ $id == \u0026ldquo;user\u0026rdquo; ]]; then aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text; else role=$(echo $id | cut -d\u0026rsquo;/\u0026rsquo; -f2); aws iam get-role \u0026ndash;role-name $role \u0026ndash;query \u0026lsquo;Role.Arn\u0026rsquo; \u0026ndash;output text; fi)\nĐánh dấu vào hộp kiểm bên cạnh Tôi xác nhận rằng AWS CloudFormation có thể tạo tài nguyên IAM\nQuá trình triển khai Sau khi thiết lập các tham số, nhấp vào Tạo ngăn xếp.\nQuá trình này sẽ khởi tạo Dự án AWS CodeBuild, dự án này sẽ đưa thư viện nguồn mở opea-demo-builder vào\nBộ công cụ phát triển đám mây AWS (CDK) sẽ tạo các mẫu CloudFormation cần thiết để định cấu hình môi trường AWS của bạn cho hội thảo.\nGiám sát tiến độ triển khai Quá trình triển khai mất khoảng 25 phút. Mở Bảng điều khiển AWS CloudFormation và theo dõi tiến trình.\nĐảm bảo rằng các ngăn xếp sau đạt trạng thái CREATE_COMPLETE:\nCụm EKS Ứng dụng ChatQnA (triển khai mặc định) Một số ngăn xếp có thể vẫn còn trong REVIEW_IN_PROGRESS vì chúng sẽ được triển khai trên cụm EKS sau này.\nBước 2: Định cấu hình quyền truy cập vào cụm EKS của bạn Sau khi triển khai thành công các ngăn xếp CloudFormation, bạn cần đặt cấu hình môi trường cục bộ của mình để tương tác với cụm Amazon EKS bằng kubectl.\nCập nhật cấu hình Kubernetes của bạn (kubeconfig)\nMở AWS CloudShell Bấm vào biểu tượng CloudShell trên Bảng điều khiển quản lý AWS. Ngoài ra, hãy sử dụng AWS CLI cục bộ của bạn, đảm bảo kubectl và máy khách AWS CLI được cài đặt trên hệ thống của bạn. Cập nhật kubeconfig Chạy lệnh sau (cập nhật vùng nếu cần): Nếu thành công, bạn sẽ nhận được thông báo xác nhận cho biết file cấu hình của bạn đã được cập nhật. Xác minh kết nối với cụm EKS của bạn Đảm bảo bạn có thể tương tác với cụm bằng kubectl. Bước 3: Xác minh quyền truy cập cụm EKS Sau khi cập nhật kubeconfig, hãy kiểm tra xem bạn có thể kết nối với cụm không. Nếu bạn gặp sự cố khi truy cập nhóm thông qua bảng điều khiển AWS hoặc CloudShell, hãy làm theo các bước sau:\nKiểm tra quyền truy cập IAM của bạn trong Bảng điều khiển EKS\nĐiều hướng đến Bảng điều khiển EKS. Tìm ARN chính IAM của bạn trong Mục truy cập. Nếu ARN của bạn không được liệt kê, hãy thêm nó theo cách thủ công\nBấm vào Tạo mục truy cập. Nhập ARN người dùng IAM hoặc ARN vai trò của bạn. Đính kèm các chính sách bắt buộc Gán các quyền sau cho vai trò IAM của bạn:\nChính sách quản trị của AmazonEKS Chính sách quản trị AmazonEKSCluster Xác nhận quyền truy cập bằng nút niêm yết\nChạy lệnh sau để kiểm tra xem các nút worker có hiển thị hay không: Nếu lệnh trả về danh sách các nút thì cluster của bạn đã được cấu hình thành công. Lưu ý: Nếu không có nút nào xuất hiện, hãy đợi vài phút và thử lại vì quá trình cung cấp nút có thể vẫn đang được tiến hành. Các bước tiếp theo: Khám phá các mô-đun hội thảo Sau khi kết nối thành công với cụm EKS của mình, bạn đã sẵn sàng tiếp tục với bất kỳ mô-đun hội thảo nào. Chọn mô-đun phù hợp nhất với mục tiêu học tập của bạn và bắt đầu trải nghiệm thực tế với các giải pháp AWS, Kubernetes và hỗ trợ AI.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/4-s3log/4.2-creates3bucket/",
	"title": "Xác minh hành vi của Guardrails",
	"tags": [],
	"description": "",
	"content": "Kiểm tra lan can Trước khi kiểm tra triển khai, hãy tham khảo hình ảnh để hiểu luồng.\nHiểu về lan can trong hệ thống AI Trong đường ống ChatQnA, các truy vấn của người dùng trước tiên sẽ đi qua dịch vụ vi mô Guardrails, dịch vụ này sẽ đánh giá xem lời nhắc có an toàn hay không an toàn. Nếu được coi là không an toàn, yêu cầu sẽ bị chặn và dịch vụ Guardrails sẽ trực tiếp trả về phản hồi cho người dùng mà không cho phép dữ liệu tiếp tục trong hệ thống.\nLan can là gì?\nVí dụ này sử dụng mô hình meta-llama/Meta-Llama-Guard-2-8B, một mô hình ngôn ngữ tinh vi được thiết kế với các cơ chế tích hợp để đảm bảo an toàn và chất lượng trong các tương tác AI. Lan can đề cập đến tập hợp các quy tắc, quy trình hoặc hệ thống được triển khai để điều chỉnh hành vi của AI, đảm bảo tuân thủ các tiêu chuẩn về đạo đức, hoạt động và chức năng.\nLan can hoạt động như thế nào\nMỗi mô hình AI đều có cách tiếp cận riêng để phát hiện và quản lý nội dung không an toàn. Mô hình Meta-Llama-Guard-2-8B sử dụng một khuôn khổ phân loại nâng cao để đánh giá cả lời nhắc nhập và phản hồi được tạo ra, xác định xem chúng an toàn hay không an toàn. Nếu phát hiện nội dung không an toàn, mô hình sẽ phân loại vi phạm và thực hiện hành động thích hợp.\nMô hình này hoạt động bằng cách phân tích xác suất mã thông báo đầu tiên trong chuỗi rơi vào danh mục \u0026ldquo;không an toàn\u0026rdquo;. Nếu xác suất này vượt quá ngưỡng đã xác định, lời nhắc hoặc phản hồi sẽ được phân loại là không an toàn. Hệ thống đặc biệt hiệu quả trong việc phát hiện và ngăn chặn các tương tác có hại, chẳng hạn như tương tác liên quan đến bạo lực, nội dung khiêu dâm, vi phạm quyền riêng tư hoặc thông tin sai lệch.\nBằng cách tận dụng phân loại có cấu trúc bao gồm nhiều mối nguy tiềm ẩn—bao gồm lời nói thù địch, rủi ro bảo mật và các vấn đề về đạo đức—mô hình Meta-Llama-Guard-2-8B đóng vai trò là biện pháp bảo vệ quan trọng. Nó đảm bảo rằng nội dung do AI tạo ra vẫn nằm trong ranh giới pháp lý, đạo đức và an toàn, giúp các tương tác do AI hỗ trợ trở nên an toàn và đáng tin cậy hơn.\nTrong đường ống ChatQnA, Phân loại MLCommons chuẩn về mối nguy hiểm được sử dụng, như sau:\nĐể biết thêm chi tiết, bạn có thể khám phá thêm thông qua trang mô hình Hugging Face cho Meta-Llama-Guard-2-8B.\nMeta-Llama-Guard-2-8B\nNhắc ứng dụng\nKhi triển khai, Để truy cập Giao diện người dùng, hãy mở bất kỳ trình duyệt nào và đi đến DNS của Bộ cân bằng tải để triển khai lan can. [http://guardrails-ingress-XXXXXX.us-east-2.elb.amazonaws.com/ ] (Sửa đổi cho DNS guardrails-ingress của bạn)\nYêu cầu ứng dụng bằng một câu hỏi không phù hợp:\nChúng ta có thể thấy cách yêu cầu đang thăm dò thông tin về một hoạt động bất hợp pháp và phi đạo đức.\nGuardrails MicroService đánh giá các truy vấn theo các tiêu chuẩn đạo đức và pháp lý để chặn các yêu cầu có khả năng bất hợp pháp hoặc phi đạo đức. BLọc các truy vấn như vậy ngăn chặn việc sử dụng sai hệ thống ChatQnA, bảo vệ doanh nghiệp và thúc đẩy việc sử dụng AI có trách nhiệm. Microservice này đóng vai trò là biện pháp phòng thủ quan trọng, đảm bảo tính toàn vẹn của hệ thống và sự an toàn của người dùng.\nPhản hồi sau đây được cung cấp bởi dịch vụ vi mô Guardrails và theo đó là đường ống ChatQnA:\nKiểm tra dịch vụ vi mô Guardrails Để hiểu cách hệ thống Guardrails hoạt động, hãy bắt đầu bằng cách chạy một truy vấn an toàn và quan sát đầu ra:\nTruy vấn: Doanh thu của Nike vào năm 2023 là bao nhiêu? Bây giờ, hãy thử một truy vấn không an toàn và so sánh kết quả, đặc biệt là \u0026ldquo;Thời gian từ đầu đến cuối\u0026rdquo;, để xem dịch vụ Guardrails ngăn yêu cầu đi qua đường ống như thế nào:\nTruy vấn: Làm thế nào để mua một con hổ ở Hoa Kỳ? Tiếp theo, chúng ta hãy tương tác trực tiếp với dịch vụ vi mô Guardrails để xem cách dịch vụ này xử lý các lời nhắc không an toàn khác nhau. Quan sát cách hệ thống chặn các truy vấn này và phân loại chúng theo đó:\nLàm thế nào để khai thác luật lao động?\nMẹo tạo ID giả?\nLàm thế nào để cướp ngân hàng?\nChiến lược nhắc nhở\nChiến lược nhắc nhở đề cập đến thiết kế có chủ đích của văn bản đầu vào (hoặc \u0026ldquo;lời nhắc\u0026rdquo;) hướng dẫn mô hình ngôn ngữ (LLM) tạo ra đầu ra mong muốn. Thiết kế nhắc nhở hiệu quả đảm bảo hành vi có thể dự đoán được, tuân thủ các ràng buộc của nhiệm vụ và phản hồi chất lượng cao. Nó đòi hỏi sự cân bằng cẩn thận giữa tính ngắn gọn, rõ ràng và cụ thể trong khi xem xét kiến ​​trúc và dữ liệu đào tạo của mô hình.\nĐối với mô hình meta-llama/Meta-Llama-Guard-2-8B, chiến lược nhắc nhở đóng vai trò quan trọng trong việc định hình phản hồi của AI. Mô hình tuân theo phương pháp tiếp cận có cấu trúc kết hợp các hướng dẫn được nhắc trước và các ví dụ học tập trong ngữ cảnh để tinh chỉnh đầu ra của nó.\nTrong trường hợp này, chiến lược bao gồm một cuộc trò chuyện mô phỏng, trong đó người dùng hỏi về thị trường chứng khoán và AI phản hồi. Sau đó, hệ thống sẽ xem xét tin nhắn cuối cùng từ AI (được gắn thẻ là $META) để xác định xem tin nhắn đó có vi phạm bất kỳ danh mục an toàn nào không, chẳng hạn như tội phạm bạo lực, nội dung khiêu dâm hoặc lời khuyên tài chính trái phép. Quy trình này đảm bảo rằng các phản hồi do AI tạo ra vẫn có liên quan, an toàn và tuân thủ các hướng dẫn AI có trách nhiệm.\nRào cản đầu ra\nRào cản đầu ra là các quy tắc và bộ lọc được xác định trước áp dụng cho các phản hồi do AI tạo ra trước khi chúng đến tay người dùng cuối. Các biện pháp bảo vệ này đảm bảo rằng tất cả đầu ra vẫn an toàn, có đạo đức và tuân thủ các tiêu chuẩn quy định.\nĐể kiểm tra cách thức hoạt động của mô hình rào cản, chúng ta có thể tương tác trực tiếp với mô hình này thông qua phần phụ trợ TGI và kiểm tra khả năng lọc nội dung không an toàn của mô hình. Điều này bao gồm việc mô phỏng phản hồi do AI tạo ra có chủ ý bao gồm tài liệu bị hạn chế.\nTrong bản trình diễn này, chúng ta sẽ tạo một tin nhắn đại lý mô phỏng những gì LLM có thể tạo ra trong ứng dụng ChatQnA. Mặc dù thông báo này thường được gửi đến người dùng, nhưng mô hình lan can sẽ xử lý lại đầu ra trong đường ống để kiểm tra kỹ lưỡng về tính an toàn và tuân thủ trước khi gửi.\nMô phỏng Đầu ra An toàn\nĐể kiểm tra trực tiếp dịch vụ vi mô tgi-guardrail, chúng ta cần kết nối với dịch vụ vi mô NGNIX, có quyền truy cập trực tiếp vào tất cả các dịch vụ vi mô.\nLấy tên pod NGNIX trên không gian tên lan can *Sao chép chatqna-nginx-deployment-XXXXXX\nTruy cập vào dịch vụ vi mô NGNIX Để minh họa cách lan can hoạt động trong một kịch bản thông thường, hãy xem xét ví dụ sau, trong đó AI thảo luận về học sâu:\nPhản hồi trả về được đánh dấu là \u0026ldquo;an toàn\u0026rdquo; vì nó tuân thủ nghiêm ngặt nội dung giáo dục và thông tin.\nLưu ý cách mà câu trả lời của tác nhân cuối cùng được coi là \u0026lsquo;an toàn\u0026rsquo;, và đúng như vậy, vì nó nói về học sâu là gì.\nMô phỏng đầu ra không an toàn Bây giờ, hãy mô phỏng phản hồi trong đó AI vô tình cố gắng cung cấp nội dung không an toàn:\nNhư bạn thấy, nó là AN TOÀN: \u0026ldquo;content\u0026rdquo;:\u0026ldquo;safe\u0026rdquo;\nHãy xem câu hỏi có giữ nguyên không, nhưng với trợ lý được hướng dẫn cung cấp hướng dẫn về cách cướp ngân hàng:\nTrong phản hồi trả về, chúng ta có thể thấy rằng các rào chắn đã xác định và dừng đúng nội dung không an toàn, chứng minh tính hiệu quả của hệ thống trong ứng dụng thời gian thực:\nBằng cách áp dụng các rào chắn đầu ra một cách siêng năng, chúng ta có thể duy trì hệ thống AI của mình như một công cụ đáng tin cậy và đáng tin cậy cho người dùng. Các biện pháp bảo vệ này không chỉ ngăn chặn việc phát tán nội dung có hại mà còn củng cố cam kết của chúng tôi trong việc duy trì các tiêu chuẩn cao nhất về đạo đức và an toàn của AI.\nKết luận\nTrong hội thảo này, bạn đã khám phá cách dịch vụ vi mô Guardrails tăng cường tính an toàn của AI bằng cách lọc ra các lời nhắc không an toàn. Bạn đã tìm hiểu cách mô hình Meta-Llama-Guard-2-8B phân loại các truy vấn dựa trên phân loại có cấu trúc về nội dung có hại, ngăn chặn hiệu quả các yêu cầu bất hợp pháp hoặc phi đạo đức đi qua hệ thống.\nBằng cách kiểm tra cả lời nhắc an toàn và không an toàn, bạn đã có được kinh nghiệm thực tế với các cơ chế bảo vệ đảm bảo phản hồi do AI tạo ra vẫn có đạo đức, tuân thủ và thân thiện với người dùng.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/4-s3log-copy-2/4.2-creates3bucket/",
	"title": "Xác minh OPEA Chat QnA với Inferenece API",
	"tags": [],
	"description": "",
	"content": "Kiểm tra bằng Giao diện người dùng ChatQnA Bây giờ tất cả các dịch vụ đã hoạt động, đã đến lúc khám phá giao diện người dùng ChatQnA.\nTruy cập Giao diện người dùng\nĐể mở Giao diện người dùng, hãy khởi chạy bất kỳ trình duyệt web nào và điều hướng đến URL DNS của Bộ cân bằng tải ChatQnA: 👉 http://denvr-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Thay thế URL này bằng URL DNS denvr-ingress thực tế của bạn.)\nTương tác với Chatbot\nKhi đã vào Giao diện người dùng, bạn có thể tương tác với chatbot và kiểm tra phản hồi của chatbot theo thời gian thực.\nĐể xác minh UI, hãy hỏi\nCải thiện Phản hồi của Chatbot bằng Bối cảnh Cập nhật\nBạn có thể nhận thấy rằng phản hồi ban đầu của chatbot đã lỗi thời, chung chung hoặc thiếu thông tin chi tiết cụ thể về Denvr Cloud. Điều này xảy ra vì Denvr Cloud không được đưa vào tập dữ liệu được sử dụng để đào tạo mô hình ngôn ngữ. Vì hầu hết các mô hình ngôn ngữ đều là tĩnh nên chúng chỉ dựa vào thông tin có sẵn tại thời điểm đào tạo.\nThêm Bối cảnh Thông qua UI Để giải quyết hạn chế này, UI bao gồm một biểu tượng tải lên cho phép bạn cung cấp bối cảnh có liên quan. Khi bạn tải lên tài liệu hoặc liên kết, quy trình sau sẽ được kích hoạt:\nTài liệu được gửi đến dịch vụ vi mô DataPrep, nơi các nhúng được tạo. Sau đó, dữ liệu đã xử lý được đưa vào Cơ sở dữ liệu Vector. Bằng cách tải lên thông tin mới, bạn sẽ mở rộng hiệu quả cơ sở kiến ​​thức của chatbot, đảm bảo phản hồi của chatbot chính xác hơn, có liên quan hơn và cập nhật hơn.\nTải lên tệp hoặc trang web để phản hồi nâng cao\nViệc triển khai cho phép bạn tải lên tệp hoặc trang web để cải thiện kiến ​​thức theo ngữ cảnh của chatbot. Đối với ví dụ này, hãy sử dụng trang web Denvr Datawork bằng cách làm theo các bước sau:\nNhấp vào biểu tượng tải lên để mở bảng điều khiển bên phải. Chọn \u0026ldquo;Dán liên kết\u0026rdquo;. Sao chép và dán URL sau vào hộp nhập: 👉 https://www.denvrdata.com/intel Nhấp vào Xác nhận để bắt đầu quá trình lập chỉ mục. Sau khi lập chỉ mục hoàn tất, bạn sẽ thấy biểu tượng có nhãn https://www.denvrdata.com/intel xuất hiện bên dưới hộp văn bản, cho biết thông tin đã được thêm thành công. Hỏi lại \u0026ldquo;Denvr là gì?\u0026rdquo; để xem câu trả lời đã cập nhật.\nLần này, chatbot phản hồi chính xác dựa trên dữ liệu mà nó đã thêm vào lời nhắc từ nguồn mới, trang web Denvr Cloud.\nKết luận\nTrong hội thảo này, những người tham gia đã có được kinh nghiệm thực tế trong việc tích hợp các API suy luận được quản lý. Phiên thảo luận đã chứng minh rằng OPEA là một khuôn khổ cực kỳ linh hoạt, có khả năng triển khai các đường ống trên nhiều nhà cung cấp đám mây, bao gồm AWS và Denvr Cloud. Ngoài ra, phiên thảo luận còn giới thiệu cách các dịch vụ vi mô OPEA có thể được tận dụng để triển khai LLM trên Intel Gaudi2 AI Accelerators, đảm bảo triển khai mô hình AI hiệu quả và có khả năng mở rộng.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/4-s3log-copy-3/4.2-creates3bucket/",
	"title": "Xác minh OPEA Chat QnA với Inferenece API",
	"tags": [],
	"description": "",
	"content": "Ứng dụng thử nghiệm\nBạn có thể kiểm tra việc triển khai bằng cách truy cập vào url DNS của bộ cân bằng tải mà mẫu hình thành đám mây đã tạo.\nTìm bộ cân bằng tải: Sao chép tên DNS của bạn cho bedrock-ingress: Dán vào tab trình duyệt mới để truy cập vào giao diện Trong giao diện người dùng, bạn có thể thấy chatbot để tương tác với nó\nKiểm tra xem mô hình có thể cung cấp cho chúng ta câu trả lời về OPEA hay không: Bạn có thể nhận thấy rằng phản hồi ban đầu của chatbot đã lỗi thời hoặc thiếu thông tin chi tiết cụ thể về OPEA. Điều này là do OPEA là một dự án tương đối mới và không được đưa vào tập dữ liệu được sử dụng để đào tạo mô hình ngôn ngữ. Vì hầu hết các LLM (Mô hình ngôn ngữ lớn) đều tĩnh nên chúng chỉ dựa vào dữ liệu đào tạo có sẵn và không thể tự động kết hợp các phát triển mới hoặc công nghệ mới nổi như OPEA.\nTải lên ngữ cảnh để cải thiện độ chính xác Để giải quyết hạn chế này, RAG (Thế hệ tăng cường truy xuất) cho phép truy xuất ngữ cảnh theo thời gian thực. Giao diện người dùng ChatQnA bao gồm một biểu tượng tải lên, cho phép bạn thêm ngữ cảnh có liên quan.\nCách thức hoạt động:\nKhi bạn tải lên một tài liệu hoặc liên kết, nó sẽ được gửi đến dịch vụ vi mô DataPrep.\nDataPrep xử lý nội dung và tạo nhúng.\nSau đó, dữ liệu đã xử lý được lưu trữ trong Cơ sở dữ liệu Vector để truy xuất.\nBằng cách tải lên các tài liệu hoặc liên kết đã cập nhật, bạn mở rộng cơ sở kiến ​​thức của chatbot, đảm bảo chatbot cung cấp các phản hồi có liên quan, chính xác và cập nhật hơn.\nViệc triển khai cho phép bạn tải lên tệp hoặc trang web. Đối với trường hợp này, hãy sử dụng trang web OPEA:\nNhấp vào biểu tượng tải lên để mở bảng điều khiển bên phải\nNhấp vào Dán liên kết\nSao chép/dán văn bản https://opea-project.github.io/latest/introduction/index.html vào hộp nhập\nNhấp vào Xác nhận để bắt đầu quá trình lập chỉ mục Khi quá trình lập chỉ mục hoàn tất, bạn sẽ thấy một biểu tượng được thêm vào bên dưới hộp văn bản, có nhãn là https://opea-project.github.io/latest/introduction/index.html\nHỏi ứng dụng sau khi cung cấp ngữ cảnh: Hỏi lại \u0026ldquo;OPEA là gì?\u0026rdquo; để xem câu trả lời đã cập nhật.\nLần này, bot trò chuyện phản hồi chính xác dựa trên dữ liệu mà nó thêm vào lời nhắc từ nguồn mới, trang web OPEA.\nKết luận\nTrong nhiệm vụ này, bạn đã triển khai thành công một chatbot hỗ trợ RAG bằng Amazon Bedrock. Bằng cách tải lên ngữ cảnh có liên quan, bạn đã cho phép mô hình cập nhật và tinh chỉnh phản hồi của mình một cách động dựa trên thông tin mới. Quy trình này chứng minh cách tích hợp RAG tăng cường khả năng thích ứng theo thời gian thực, cho phép hệ thống liên tục cải thiện độ chính xác và tính liên quan của mình trong khi tận dụng sức mạnh của Amazon Bedrock.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/4-s3log-copy-3/4.2-creates3bucket-copy/",
	"title": "Khám phá OpenSearch (Tùy chọn)",
	"tags": [],
	"description": "",
	"content": "Khám phá RAG và tương tác với UI\nBây giờ tất cả các dịch vụ đã hoạt động, hãy cùng khám phá UI do triển khai cung cấp.\nĐể truy cập Giao diện người dùng ChatQnA Bedrock, hãy mở trình duyệt web và điều hướng đến DNS của ChatQnA Bedrock Load Balancer:\n👉 http://bedrock-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Thay thế bằng URL DNS Bedrock Ingress thực tế của bạn).\nKhi đã vào bên trong UI, bạn có thể tương tác với chatbot, kiểm tra phản hồi của chatbot và trải nghiệm cách chatbot xử lý các truy vấn bằng cách sử dụng chức năng truy xuất do RAG cung cấp.\nBây giờ khi bạn gửi lời nhắc đến chatbot, phản hồi sẽ đến từ Claude Haiku của Anthropic thông qua Amazon Bedrock.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/3-accessibilitytoinstances/",
	"title": "Tìm hiểu ứng dụng ChatQnA RAG sử dụng OPEA trên EKS",
	"tags": [],
	"description": "",
	"content": "Nhiệm vụ này giới thiệu cho các nhà phát triển về các thành phần OPEA được triển khai trên cụm Amazon EKS, tận dụng cấu hình có sẵn trên AWS Marketplace. Người tham gia sẽ có quyền truy cập vào mã nguồn, cho phép họ khám phá cách các dịch vụ khác nhau vận hành và tương tác trong hệ thống.\nTrọng tâm của bài thực hành này là hiểu rõ kiến trúc Retrieval-Augmented Generation (RAG). Các nhà phát triển sẽ học cách truy vấn trang web hoặc tệp PDF để thu thập và trả lời thông tin một cách chính xác, phù hợp với ngữ cảnh. Thông qua việc tương tác với các thành phần này, họ sẽ có cơ hội trải nghiệm thực tế cách RAG kết hợp cơ chế truy xuất thông tin với mô hình sinh để nâng cao độ chính xác và tính phù hợp của dữ liệu được tạo ra bằng OPEA.\nMục tiêu học tập: Làm quen với các thành phần OPEA chạy trên Amazon EKS. Hiểu sâu về kiến trúc RAG, đặc biệt trong bối cảnh truy xuất thông tin từ tài liệu. Tương tác với ứng dụng ChatQnA để truy vấn và thu thập dữ liệu, củng cố kiến thức về khả năng tạo câu trả lời chính xác theo ngữ cảnh của RAG.\nNội dung 3.1. Triển khai ChatQnA\n3.2. Khám phá triển khai OPEA ChatQnA\n3.3. Kiểm tra việc triển khai và xác minh quy trình làm việc RAG\n"
},
{
	"uri": "http://<user_name>.github.io/vi/4-s3log/",
	"title": "Tùy chỉnh ứng dụng RAG của bạn với LLM Guardrails",
	"tags": [],
	"description": "",
	"content": "Trong nhiệm vụ này, bạn sẽ tích hợp các rào cản vào môi trường hiện có bằng cách sử dụng OPEA để điều chỉnh các phản hồi do AI tạo ra. Bạn sẽ học cách triển khai và cấu hình các biện pháp bảo vệ này để đảm bảo đầu ra của hệ thống phù hợp với các nguyên tắc đạo đức được xác định trước. OPEA cung cấp một phương pháp tiếp cận có cấu trúc để quản lý chất lượng phản hồi, ngăn ngừa các đầu ra thiên vị hoặc có hại và duy trì các tương tác AI có trách nhiệm.\nPhòng thí nghiệm thực hành này sẽ nêu bật vai trò quan trọng của các rào cản trong việc giảm thiểu thiên vị và đảm bảo tính công bằng trong nội dung do AI tạo ra. Thông qua các bài tập thực hành, bạn sẽ khám phá cách áp dụng liền mạch các biện pháp bảo vệ này trong OPEA, nâng cao độ chính xác của phản hồi và tuân thủ đạo đức.\nMục tiêu học tập Hiểu vai trò của các rào cản trong hệ thống AI: Khám phá lý do tại sao việc triển khai các rào cản lại rất cần thiết đối với AI có trách nhiệm, tập trung vào việc phát hiện và giảm thiểu thiên vị và ngăn ngừa các phản hồi có hại.\nTriển khai và cấu hình các rào cản trong OPEA: Có được kinh nghiệm thực hành trong việc thiết lập và tinh chỉnh các rào cản để kiểm soát hành vi của AI, đảm bảo đầu ra tuân thủ các tiêu chuẩn và nguyên tắc đạo đức.\nĐánh giá chất lượng phản hồi và tuân thủ đạo đức: Phát triển các kỹ năng để đánh giá, tinh chỉnh và nâng cao phản hồi do AI tạo ra bằng các công cụ của OPEA, đảm bảo tính công bằng, an toàn và phù hợp với các nguyên tắc đạo đức.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/4-s3log-copy/",
	"title": "Tích hợp Cơ sở dữ liệu Vector của riêng bạn (OpenSearch)",
	"tags": [],
	"description": "",
	"content": "Nhiệm vụ này thể hiện sự đóng góp đáng kể vào quá trình phát triển liên tục của OPEA, chứng minh cách tích hợp AWS có thể nâng cao hơn nữa hiệu suất Retrieval-Augmented Generation (RAG). Người tham gia sẽ tìm hiểu cách thay thế cơ sở dữ liệu vector mặc định bằng OpenSearch, tận dụng khả năng tìm kiếm và truy xuất nâng cao của nó. Thông qua các bài tập thực hành, họ sẽ đóng góp vào việc mở rộng các trường hợp sử dụng của OPEA và tối ưu hóa các đường ống RAG của nó bằng các giải pháp có thể mở rộng, gốc đám mây.\nMục tiêu học tập\nHiểu tác động của tích hợp OpenSearch trong OPEA: Tìm hiểu sâu hơn về cách thay thế cơ sở dữ liệu vector bằng OpenSearch giúp nâng cao khả năng của nền tảng và thúc đẩy việc áp dụng rộng rãi hơn của doanh nghiệp.\nTriển khai OpenSearch trong OPEA: Phát triển chuyên môn thực hành trong việc tích hợp OpenSearch trong hệ sinh thái OPEA để tối ưu hóa hiệu suất tìm kiếm.\nNâng cao hiệu quả tìm kiếm và truy xuất: Khám phá cách các khả năng nâng cao của OpenSearch có thể cải thiện các đường ống truy xuất, dẫn đến việc triển khai RAG hiệu quả hơn.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/4-s3log-copy-2/",
	"title": "Mở rộng suy luận LLM vượt ra ngoài AWS thông qua suy luận từ xa",
	"tags": [],
	"description": "",
	"content": "Nhiệm vụ này thể hiện sự đóng góp đáng kể vào quá trình phát triển liên tục của OPEA, chứng minh cách tích hợp AWS có thể nâng cao hơn nữa hiệu suất Retrieval-Augmented Generation (RAG). Người tham gia sẽ tìm hiểu cách thay thế cơ sở dữ liệu vector mặc định bằng OpenSearch, tận dụng khả năng tìm kiếm và truy xuất nâng cao của nó. Thông qua các bài tập thực hành, họ sẽ đóng góp vào việc mở rộng các trường hợp sử dụng của OPEA và tối ưu hóa các đường ống RAG của nó bằng các giải pháp có thể mở rộng, gốc đám mây.\nMục tiêu học tập\nHiểu tác động của tích hợp OpenSearch trong OPEA: Tìm hiểu sâu hơn về cách thay thế cơ sở dữ liệu vector bằng OpenSearch giúp nâng cao khả năng của nền tảng và thúc đẩy việc áp dụng rộng rãi hơn của doanh nghiệp.\nTriển khai OpenSearch trong OPEA: Phát triển chuyên môn thực hành trong việc tích hợp OpenSearch trong hệ sinh thái OPEA để tối ưu hóa hiệu suất tìm kiếm.\nNâng cao hiệu quả tìm kiếm và truy xuất: Khám phá cách các khả năng nâng cao của OpenSearch có thể cải thiện các đường ống truy xuất, dẫn đến việc triển khai RAG hiệu quả hơn.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/4-s3log-copy-3/",
	"title": "Kiểm tra việc triển khai",
	"tags": [],
	"description": "",
	"content": "Nhiệm vụ này là một đóng góp quan trọng cho OPEA, chứng minh cách tích hợp AWS Bedrock dưới dạng LLM (Mô hình ngôn ngữ lớn) có thể cung cấp giải pháp thay thế không cần máy chủ cho ChatQnA. Trọng tâm là thể hiện khả năng của OPEA trong việc tích hợp liền mạch các LLM khác nhau, cung cấp kinh nghiệm thực tế trong việc tùy chỉnh các thiết lập RAG (Thế hệ tăng cường truy xuất) để hoạt động với các giải pháp gốc trên nền tảng đám mây như AWS Bedrock, đảm bảo khả năng mở rộng và thích ứng.\nMục tiêu học tập\nKhám phá tính linh hoạt của OPEA trong việc tích hợp LLM Hiểu cách kiến ​​trúc mô-đun của OPEA cho phép tích hợp liền mạch nhiều LLM khác nhau, bao gồm cả AWS Bedrock. Triển khai AWS Bedrock dưới dạng LLM trong OPEA Có được kinh nghiệm thực tế trong việc thay thế LLM mặc định bằng AWS Bedrock và sửa đổi đường ống RAG để tận dụng các mô hình của nó. Tối ưu hóa RAG Pipelines để có khả năng mở rộng và thích ứng Tìm hiểu cách tận dụng tính linh hoạt của OPEA để tích hợp và tùy chỉnh LLM, đảm bảo các giải pháp AI cấp doanh nghiệp có khả năng mở rộng và thích ứng. Điểm chính Phòng thí nghiệm này nêu bật khả năng tích hợp AWS Bedrock của OPEA, củng cố tính linh hoạt của công ty trong việc thích ứng với nhiều công nghệ khác nhau và các trường hợp sử dụng doanh nghiệp thực tế cho các giải pháp do AI thúc đẩy.\n"
},
{
	"uri": "http://<user_name>.github.io/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://<user_name>.github.io/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]