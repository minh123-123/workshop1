<!DOCTYPE html>
<html lang="en" class="js csstransforms3d">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Hugo 0.144.2">
    <meta name="description" content="">
<meta name="author" content="journeyoftheaverageguy@gmail.com">

    <link rel="icon" href="/images/favicon.png" type="image/png">

    <title>Explore the OPEA ChatQnA deployment :: WORK WITH AMAZON SYSTEM MANAGER - SESSION MANAGER</title>

    
    <link href="/css/nucleus.css?1740709594" rel="stylesheet">
    <link href="/css/fontawesome-all.min.css?1740709594" rel="stylesheet">
    <link href="/css/hybrid.css?1740709594" rel="stylesheet">
    <link href="/css/featherlight.min.css?1740709594" rel="stylesheet">
    <link href="/css/perfect-scrollbar.min.css?1740709594" rel="stylesheet">
    <link href="/css/auto-complete.css?1740709594" rel="stylesheet">
    <link href="/css/atom-one-dark-reasonable.css?1740709594" rel="stylesheet">
    <link href="/css/theme.css?1740709594" rel="stylesheet">
    <link href="/css/hugo-theme.css?1740709594" rel="stylesheet">
    
    <link href="/css/theme-workshop.css?1740709594" rel="stylesheet">
    
    

    <script src="/js/jquery-3.3.1.min.js?1740709594"></script>

    <style>
      :root #header + #content > #left > #rlblock_left{
          display:none !important;
      }
      
    </style>
    
  </head>
  <body class="" data-url="/3-accessibilitytoinstances/3.2-private-instance/">
    <nav id="sidebar" class="showVisitedLinks">



  <div id="header-wrapper">
    <div id="header">
      <a id="logo" href="/">

<svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 60 30" width="30%"><defs><style>.cls-1{fill:#fff;}.cls-2{fill:#f90;fill-rule:evenodd;}</style></defs><title>AWS-Logo_White-Color</title><path class="cls-1" d="M14.09,10.85a4.7,4.7,0,0,0,.19,1.48,7.73,7.73,0,0,0,.54,1.19.77.77,0,0,1,.12.38.64.64,0,0,1-.32.49l-1,.7a.83.83,0,0,1-.44.15.69.69,0,0,1-.49-.23,3.8,3.8,0,0,1-.6-.77q-.25-.42-.51-1a6.14,6.14,0,0,1-4.89,2.3,4.54,4.54,0,0,1-3.32-1.19,4.27,4.27,0,0,1-1.22-3.2A4.28,4.28,0,0,1,3.61,7.75,6.06,6.06,0,0,1,7.69,6.46a12.47,12.47,0,0,1,1.76.13q.92.13,1.91.36V5.73a3.65,3.65,0,0,0-.79-2.66A3.81,3.81,0,0,0,7.86,2.3a7.71,7.71,0,0,0-1.79.22,12.78,12.78,0,0,0-1.79.57,4.55,4.55,0,0,1-.58.22l-.26,0q-.35,0-.35-.52V2a1.09,1.09,0,0,1,.12-.58,1.2,1.2,0,0,1,.47-.35A10.88,10.88,0,0,1,5.77.32,10.19,10.19,0,0,1,8.36,0a6,6,0,0,1,4.35,1.35,5.49,5.49,0,0,1,1.38,4.09ZM7.34,13.38a5.36,5.36,0,0,0,1.72-.31A3.63,3.63,0,0,0,10.63,12,2.62,2.62,0,0,0,11.19,11a5.63,5.63,0,0,0,.16-1.44v-.7a14.35,14.35,0,0,0-1.53-.28,12.37,12.37,0,0,0-1.56-.1,3.84,3.84,0,0,0-2.47.67A2.34,2.34,0,0,0,5,11a2.35,2.35,0,0,0,.61,1.76A2.4,2.4,0,0,0,7.34,13.38Zm13.35,1.8a1,1,0,0,1-.64-.16,1.3,1.3,0,0,1-.35-.65L15.81,1.51a3,3,0,0,1-.15-.67.36.36,0,0,1,.41-.41H17.7a1,1,0,0,1,.65.16,1.4,1.4,0,0,1,.33.65l2.79,11,2.59-11A1.17,1.17,0,0,1,24.39.6a1.1,1.1,0,0,1,.67-.16H26.4a1.1,1.1,0,0,1,.67.16,1.17,1.17,0,0,1,.32.65L30,12.39,32.88,1.25A1.39,1.39,0,0,1,33.22.6a1,1,0,0,1,.65-.16h1.54a.36.36,0,0,1,.41.41,1.36,1.36,0,0,1,0,.26,3.64,3.64,0,0,1-.12.41l-4,12.86a1.3,1.3,0,0,1-.35.65,1,1,0,0,1-.64.16H29.25a1,1,0,0,1-.67-.17,1.26,1.26,0,0,1-.32-.67L25.67,3.64,23.11,14.34a1.26,1.26,0,0,1-.32.67,1,1,0,0,1-.67.17Zm21.36.44a11.28,11.28,0,0,1-2.56-.29,7.44,7.44,0,0,1-1.92-.67,1,1,0,0,1-.61-.93v-.84q0-.52.38-.52a.9.9,0,0,1,.31.06l.42.17a8.77,8.77,0,0,0,1.83.58,9.78,9.78,0,0,0,2,.2,4.48,4.48,0,0,0,2.43-.55,1.76,1.76,0,0,0,.86-1.57,1.61,1.61,0,0,0-.45-1.16A4.29,4.29,0,0,0,43,9.22l-2.41-.76A5.15,5.15,0,0,1,38,6.78a3.94,3.94,0,0,1-.83-2.41,3.7,3.7,0,0,1,.45-1.85,4.47,4.47,0,0,1,1.19-1.37A5.27,5.27,0,0,1,40.51.29,7.4,7.4,0,0,1,42.6,0a8.87,8.87,0,0,1,1.12.07q.57.07,1.08.19t.95.26a4.27,4.27,0,0,1,.7.29,1.59,1.59,0,0,1,.49.41.94.94,0,0,1,.15.55v.79q0,.52-.38.52a1.76,1.76,0,0,1-.64-.2,7.74,7.74,0,0,0-3.2-.64,4.37,4.37,0,0,0-2.21.47,1.6,1.6,0,0,0-.79,1.48,1.58,1.58,0,0,0,.49,1.18,4.94,4.94,0,0,0,1.83.92L44.55,7a5.08,5.08,0,0,1,2.57,1.6A3.76,3.76,0,0,1,47.9,11a4.21,4.21,0,0,1-.44,1.93,4.4,4.4,0,0,1-1.21,1.47,5.43,5.43,0,0,1-1.85.93A8.25,8.25,0,0,1,42.05,15.62Z"></path><path class="cls-2" d="M45.19,23.81C39.72,27.85,31.78,30,25,30A36.64,36.64,0,0,1,.22,20.57c-.51-.46-.06-1.09.56-.74A49.78,49.78,0,0,0,25.53,26.4,49.23,49.23,0,0,0,44.4,22.53C45.32,22.14,46.1,23.14,45.19,23.81Z"></path><path class="cls-2" d="M47.47,21.21c-.7-.9-4.63-.42-6.39-.21-.53.06-.62-.4-.14-.74,3.13-2.2,8.27-1.57,8.86-.83s-.16,5.89-3.09,8.35c-.45.38-.88.18-.68-.32C46.69,25.8,48.17,22.11,47.47,21.21Z"></path></svg>

</a>

    </div>
    
        <div class="searchbox">
    <label for="search-by"><i class="fas fa-search"></i></label>
    <input data-search-input id="search-by" type="search" placeholder="Search...">
    <span data-search-clear=""><i class="fas fa-times"></i></span>
</div>

<script type="text/javascript" src="/js/lunr.min.js?1740709594"></script>
<script type="text/javascript" src="/js/auto-complete.js?1740709594"></script>
<script type="text/javascript">
    
        var baseurl = "http:\/\/\u003cuser_name\u003e.github.io\/";
    
</script>
<script type="text/javascript" src="/js/search.js?1740709594"></script>

    
  </div>

    <div class="highlightable">
    <ul class="topics">

        
          
          




 
  
    
    <li data-nav-id="/1-introduce/" title="Introduction" class="dd-item 
        
        
        
        ">
      <a href="/1-introduce/">
           <b> 1. </b> Introduction
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/2-prerequiste/" title="Preparation " class="dd-item 
        
        
        
        ">
      <a href="/2-prerequiste/">
           <b> 2. </b> Preparation 
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
      
        <ul>
          
          
            
          
          
          
        
          
            
            




 
  
    
    <li data-nav-id="/2-prerequiste/2.1-createec2/" title="Using Workshop Studio" class="dd-item 
        
        
        
        ">
      <a href="/2-prerequiste/2.1-createec2/">
           <b> 2.1 </b> Using Workshop Studio
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            




 
  
    
    <li data-nav-id="/2-prerequiste/2.2-createiamrole/" title="Using Your Own Account" class="dd-item 
        
        
        
        ">
      <a href="/2-prerequiste/2.2-createiamrole/">
           <b> 2.2 </b> Using Your Own Account
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/3-accessibilitytoinstances/" title="Learn ChatQnA RAG application using OPEA on EKS" class="dd-item 
        parent
        
        
        ">
      <a href="/3-accessibilitytoinstances/">
           <b> 3. </b> Learn ChatQnA RAG application using OPEA on EKS
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
      
        <ul>
          
          
            
          
          
          
        
          
            
            




 
  
    
    <li data-nav-id="/3-accessibilitytoinstances/3.1-deploy-chatqna/" title="Deploy ChatQnA" class="dd-item 
        
        
        
        ">
      <a href="/3-accessibilitytoinstances/3.1-deploy-chatqna/">
           <b> 3.1. </b> Deploy ChatQnA
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            




 
  
    
    <li data-nav-id="/3-accessibilitytoinstances/3.2-private-instance/" title="Explore the OPEA ChatQnA deployment" class="dd-item 
        
        active
        
        ">
      <a href="/3-accessibilitytoinstances/3.2-private-instance/">
           <b> 3.2. </b> Explore the OPEA ChatQnA deployment
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            




 
  
    
    <li data-nav-id="/3-accessibilitytoinstances/3.3-private-instance-copy/" title="Test the deployment and verify RAG workflow" class="dd-item 
        
        
        
        ">
      <a href="/3-accessibilitytoinstances/3.3-private-instance-copy/">
           <b> 3.3. </b> Test the deployment and verify RAG workflow
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/4-s3log/" title="Customize your RAG application with LLM Guardrails" class="dd-item 
        
        
        
        ">
      <a href="/4-s3log/">
           <b> 4. </b> Customize your RAG application with LLM Guardrails
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
      
        <ul>
          
          
            
          
          
          
        
          
            
            




 
  
    
    <li data-nav-id="/4-s3log/4.1-updateiamrole/" title="Deploy Guardrails" class="dd-item 
        
        
        
        ">
      <a href="/4-s3log/4.1-updateiamrole/">
           <b> 4.1 </b> Deploy Guardrails
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            




 
  
    
    <li data-nav-id="/4-s3log/4.2-creates3bucket/" title="Verify Guardrails behaviour" class="dd-item 
        
        
        
        ">
      <a href="/4-s3log/4.2-creates3bucket/">
           <b> 4.2 </b> Verify Guardrails behaviour
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/4-s3log-copy/" title="Integrate your own Vector Database (OpenSearch)" class="dd-item 
        
        
        
        ">
      <a href="/4-s3log-copy/">
           <b> 5. </b> Integrate your own Vector Database (OpenSearch)
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
      
        <ul>
          
          
            
          
          
          
        
          
            
            




 
  
    
    <li data-nav-id="/4-s3log-copy/4.1-updateiamrole/" title="OpenSearch integration" class="dd-item 
        
        
        
        ">
      <a href="/4-s3log-copy/4.1-updateiamrole/">
           <b> 5.1 </b> OpenSearch integration
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            




 
  
    
    <li data-nav-id="/4-s3log-copy/4.2-creates3bucket/" title="Explore OpenSearch (Optional)" class="dd-item 
        
        
        
        ">
      <a href="/4-s3log-copy/4.2-creates3bucket/">
           <b> 5.2 </b> Explore OpenSearch (Optional)
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/4-s3log-copy-2/" title="Extend the LLM inference beyond AWS through Remote Inference" class="dd-item 
        
        
        
        ">
      <a href="/4-s3log-copy-2/">
           <b> 6. </b> Extend the LLM inference beyond AWS through Remote Inference
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
      
        <ul>
          
          
            
          
          
          
        
          
            
            




 
  
    
    <li data-nav-id="/4-s3log-copy-2/4.1-updateiamrole/" title="Integrate Inference API (Denvr Cloud and Intel Gaudi AI Accelerator)" class="dd-item 
        
        
        
        ">
      <a href="/4-s3log-copy-2/4.1-updateiamrole/">
           <b> 6.1 </b> Integrate Inference API (Denvr Cloud and Intel Gaudi AI Accelerator)
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            




 
  
    
    <li data-nav-id="/4-s3log-copy-2/4.2-creates3bucket/" title="Verify OPEA Chat QnA with Inferenece API" class="dd-item 
        
        
        
        ">
      <a href="/4-s3log-copy-2/4.2-creates3bucket/">
           <b> 6.2 </b> Verify OPEA Chat QnA with Inferenece API
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/4-s3log-copy-3/" title="Test the deployment" class="dd-item 
        
        
        
        ">
      <a href="/4-s3log-copy-3/">
           <b> 7. </b> Test the deployment
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
      
        <ul>
          
          
            
          
          
          
        
          
            
            




 
  
    
    <li data-nav-id="/4-s3log-copy-3/4.1-updateiamrole/" title="Set Bedrock integration" class="dd-item 
        
        
        
        ">
      <a href="/4-s3log-copy-3/4.1-updateiamrole/">
           <b> 7.1 </b> Set Bedrock integration
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            




 
  
    
    <li data-nav-id="/4-s3log-copy-3/4.2-creates3bucket/" title="Verify OPEA Chat QnA with Inferenece API" class="dd-item 
        
        
        
        ">
      <a href="/4-s3log-copy-3/4.2-creates3bucket/">
           <b> 7.2 </b> Verify OPEA Chat QnA with Inferenece API
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            




 
  
    
    <li data-nav-id="/4-s3log-copy-3/4.2-creates3bucket-copy/" title="Test the deployment and verify the RAG workflow" class="dd-item 
        
        
        
        ">
      <a href="/4-s3log-copy-3/4.2-creates3bucket-copy/">
           <b> 7.3 </b> Test the deployment and verify the RAG workflow
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
         
    </ul>

    
    
      <section id="shortcuts">
        <h3>More</h3>
        <ul>
          
              <li> 
                  <a class="padding" href="https://aws.amazon.com/blogs"><i class='fab fa-aws'></i> AWS Study Group - Blog</a>
              </li>
          
              <li> 
                  <a class="padding" href="https://www.facebook.com/groups/awsstudygroupfcj"><i class='fab fa-facebook'></i> AWS Study Group - FB Group</a>
              </li>
          
        </ul>
      </section>
    

    
    <section id="prefooter">
      <hr/>
      <ul>
      
        <li>
          <a class="padding">
            <i class="fas fa-language fa-fw"></i>
          <div class="select-style">
            <select id="select-language" onchange="location = this.value;">
          
          
          
              
              
                  
                    
                    
                      <option id="en" value="http://&lt;user_name&gt;.github.io/3-accessibilitytoinstances/3.2-private-instance/" selected>English</option>
                    
                  
              
                  
              
          
              
              
                  
              
                  
                    
                    
                      <option id="vi" value="http://&lt;user_name&gt;.github.io/vi/3-accessibilitytoinstances/3.2-private-instance/">Tiếng Việt</option>
                    
                  
              
          
        </select>
        <svg version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
          width="255px" height="255px" viewBox="0 0 255 255" style="enable-background:new 0 0 255 255;" xml:space="preserve">
          <g>
            <g id="arrow-drop-down">
              <polygon points="0,63.75 127.5,191.25 255,63.75 		" />
            </g>
          </g>
        </svg>
        </div>
        </a>
        </li>
      
      
      
        <li><a class="padding" href="#" data-clear-history-toggle=""><i class="fas fa-history fa-fw"></i> Clear History</a></li>
      
      </ul>
    </section>
    
    <section id="footer">
      <left>
    
     <b> Workshop</b> <br>
    <img src="https://hitwebcounter.com/counter/counter.php?page=7920860&style=0038&nbdigits=9&type=page&initCount=0" title="Migrate" Alt="web counter"   border="0" /></a>  <br>
     <b> <a href="https://cloudjourney.awsstudygroup.com/">Cloud Journey</a></b> <br>
    <img src="https://hitwebcounter.com/counter/counter.php?page=7830807&style=0038&nbdigits=9&type=page&initCount=0" title="Total CLoud Journey" Alt="web counter"   border="0"   />
     
</left>
<left>
    <br>
    <br>
        <b> Last Updated </b> <br>
        <i><font color=orange>18-04-2024</font></i>
    </left>
    <left>
        <br>
        <br>
            <b> Team </b> <br>
           
            <i> <a href="https://www.linkedin.com/in/sutrinh/"  style="color:orange">Sử Trịnh  </a> <br>
                <a href="https://www.linkedin.com/in/jotaguy"  style="color:orange">Gia Hưng </a> <br>
                <a href="https://www.linkedin.com/in/hiepnguyendt"  style="color:orange">Thanh Hiệp </a>
               
        </i>
        </left>

<script async defer src="https://buttons.github.io/buttons.js"></script>

    </section>
  </div>
</nav>




        <section id="body">
        <div id="overlay"></div>
        <div class="padding highlightable">
              
              <div>
                <div id="top-bar">
                
                
                <div id="breadcrumbs" itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb">
                    <span id="sidebar-toggle-span">
                        <a href="#" id="sidebar-toggle" data-sidebar-toggle="">
                          <i class="fas fa-bars"></i>
                        </a>
                    </span>
                  
                  <span id="toc-menu"><i class="fas fa-list-alt"></i></span>
                  
                  <span class="links">
                 
                 
                    
          
          
            
            
          
          
            
            
          
          
            <a href='/'>Deploy Open Platform for Enterprise AI (OPEA) Chat Q&A on AWS</a> > <a href='/3-accessibilitytoinstances/'>Learn ChatQnA RAG application using OPEA on EKS</a> > Explore the OPEA ChatQnA deployment
          
        
          
        
          
        
                 
                  </span>
                </div>
                
                    <div class="progress">
    <div class="wrapper">
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#exploring-the-opea-microservices-deployment">Exploring the OPEA Microservices Deployment</a></li>
        <li><a href="#vector-database-microservice-pod-chatqna-redis-vector-db80"><strong>Vector Database Microservice (POD: chatqna-redis-vector-db:80)</strong></a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
</div>

                
              </div>
            </div>
            
        <div id="head-tags">
        
        </div>
        
        <div id="body-inner">
          
            <h1>
              
              Explore the OPEA ChatQnA deployment
            </h1>
          

        



	<h3 id="exploring-the-opea-microservices-deployment">Exploring the OPEA Microservices Deployment</h3>
<p>Now, let&rsquo;s dive into the OPEA ChatQnA RAG deployment. As a microservices-based blueprint, it is designed for scalability, resilience, and flexibility. In this task, you will explore each microservice to understand its role within the overall system. By examining these components, you will gain insights into how they interact and contribute to the application&rsquo;s functionality.</p>
<p>This architecture offers several key advantages:</p>
<ul>
<li>
<p><strong>Scalability</strong> – Each microservice can scale independently based on demand, ensuring optimal resource utilization and performance.</p>
</li>
<li>
<p><strong>Fault Isolation</strong> – If one service encounters an issue, it won’t disrupt the entire system, enhancing reliability.</p>
</li>
<li>
<p><strong>Efficient Maintenance &amp; Updates</strong> – Microservices allow for rapid updates and easy adaptability to evolving business needs and user demands.</p>
</li>
</ul>
<h4 id="opea-microservices-architecture">OPEA Microservices Architecture</h4>
<p>OPEA deployments are built around three key components:</p>
<p><img src="/images/3/image025.png" alt="VPC"></p>
<ul>
<li>
<p><strong>Megaservice</strong> – Acts as the orchestrator for all microservices, managing workflows and ensuring seamless interaction between components. This is essential for coordinating an end-to-end application with multiple moving parts. More details can be found in the OPEA documentation.</p>
</li>
<li>
<p><strong>Gateway</strong> – Serves as the entry point for users, routing incoming requests to the appropriate microservices within the megaservice architecture. It ensures seamless connectivity between external users and internal components.</p>
</li>
<li>
<p><strong>Microservices</strong> – These are the individual functional components of the application, handling tasks such as embeddings, retrieval, LLM processing, and vector database interactions.
Accessing the Microservices</p>
</li>
</ul>
<p>Before you begin exploring, note that only the gateway and UI services are exposed externally. In this task, you will directly access each internal microservice for testing purposes, using the Nginx gateway to efficiently route requests to these internal services.</p>

<div class="notices info" ><p>You&rsquo;ll need to take note of all pods deployed.</p>
</div>

<p>kubectl get svc lists all services in a Kubernetes cluster, showing their names, types, cluster IPs, and exposed ports. It provides an overview of how applications are exposed for internal or external access.</p>
<p>Run the following command on your CloudShell:</p>
<p><img src="/images/3/image026.png" alt="VPC"></p>
<p>You will see output similar to this:</p>
<p><img src="/images/3/image027.png" alt="VPC"></p>
<p>The kubectl get svc command is used to list the services running within a Kubernetes cluster. Services act as entry points that enable communication between different components of your application. Each service has a unique name (e.g., chatqna or chatqna-ui), which helps identify its role within the system.</p>
<p>Kubernetes services can be exposed in different ways:</p>
<ul>
<li>
<p><strong>ClusterIP</strong> – Only accessible within the cluster, allowing internal components to communicate securely.</p>
</li>
<li>
<p><strong>NodePort</strong> – Exposes the service externally through a specific port on each node, making it accessible outside the cluster.
The Cluster-IP is the internal address used by other services to reach the application. If the service were accessible from outside the cluster, an External-IP would be displayed. However, in this case, these services are strictly internal.</p>
</li>
</ul>
<p>The Ports column indicates which network ports the service listens on. For example:</p>
<ul>
<li>
<p>chatqna might be running on port 8888/TCP, handling internal communication.</p>
</li>
<li>
<p>chatqna-nginx could be configured with 80:30144/TCP, where traffic from port 80 is forwarded to 30144 for routing purposes.
Lastly, the Age column displays how long the service has been running—for instance, 12 hours for all listed services in this scenario.</p>
</li>
</ul>
<p>Now, let’s explore the architecture in detail.</p>
<h4 id="step-1--megaservice-orchestrator-podchatqna8888">Step 1 : Megaservice (Orchestrator) (POD:chatqna:8888)</h4>
<p>The megaservice encapsulates the complete logic for the ChatQnA RAG application. This microservice is tasked with processing incoming requests and executing all the necessary internal operations to generate appropriate responses.</p>
<p>This service isn&rsquo;t directly exposed, but you can access it directly from the LoadBalancer, which forwards the request.</p>
<ul>
<li>Look for the load balancer</li>
</ul>
<p><img src="/images/3/image028.png" alt="VPC"></p>
<ul>
<li>Click on chatqna-Ingress</li>
</ul>
<p><img src="/images/3/image029.png" alt="VPC"></p>
<ul>
<li>Note the DNS Name.As mentioned, it&rsquo;s the public URL that can be accessed externally.</li>
</ul>
<p><img src="/images/3/image030.png" alt="VPC"></p>
<p>You will use the curl command to send requests to the API endpoints, testing each microservice individually. The goal is to ask a question, such as &ldquo;What was Nike&rsquo;s revenue in 2023?&rdquo;, and verify that the API responds correctly. This step ensures that all microservices in the system are functioning as expected.</p>
<p><img src="/images/3/image031.png" alt="VPC"></p>
<p>If everything is working properly, you should receive a response, confirming that the Retrieval-Augmented Generation (RAG) workflow is operational.</p>
<p>However, you may notice that the model is unable to provide an accurate answer. This happens because it lacks the necessary context and relies on outdated information. Without access to current and relevant data, the model cannot generate precise responses.
In the next steps, you will enhance the system using RAG, allowing the model to retrieve up-to-date, contextually relevant information. This will ensure that it delivers more accurate and meaningful answers.</p>
<p>Now, let&rsquo;s explore each microservice in detail to understand its role and how it contributes to improving the model&rsquo;s ability to answer questions correctly.</p>
<h4 id="step-2--microservices">Step 2 : Microservices</h4>
<p>Each microservice follows the following logic performing a task within the RAG flow:</p>
<p><img src="/images/3/image032.png" alt="VPC"></p>
<p>In the flow, you can observe the microservices and we can divide the RAG flow into two steps:</p>
<ul>
<li>
<p><strong>Preprompting</strong>: This step involves preparing the knowledge base (KB) by uploading relevant documents and ensuring that the information is organized for effective retrieval.</p>
</li>
<li>
<p><strong>Prompting</strong>: This step focuses on retrieving the relevant data from the knowledge base and using it to generate an accurate answer to the user&rsquo;s question.</p>
</li>
</ul>
<h4 id="preprompting"><strong>Preprompting</strong></h4>
<p>In this step, the logic is to start from a document (Nike&rsquo;s revenue PDF), and do the preprocessing needed to make it ready to be stored in a database. As shown, this process primarily involves 3 microservices: data preparation, embeddings and vector store. Let&rsquo;s explore each microservice</p>
<p><img src="/images/3/image033.png" alt="VPC"></p>
<h4 id="embedding-microservice-pod-chatqna-tei80"><strong>Embedding Microservice (POD: chatqna-tei:80)</strong></h4>
<p>An embedding is a numerical representation of an object—such as a word, phrase, or document—within a continuous vector space. In natural language processing (NLP), embeddings transform words, sentences, or text segments into vectors—sets of numbers that capture their meaning, relationships, and contextual significance. This transformation enables machine learning models to process and understand text more effectively.</p>
<p>For example, word embeddings represent words as points in a vector space, where words with similar meanings—like &ldquo;king&rdquo; and &ldquo;queen&rdquo;—are positioned closer together. The embedding model captures these relationships through vector arithmetic.</p>
<p>During training, if the model frequently encounters &ldquo;king&rdquo; in association with &ldquo;man&rdquo; and &ldquo;queen&rdquo; with &ldquo;woman,&rdquo; it learns that &ldquo;king&rdquo; and &ldquo;queen&rdquo; share a similar relationship to &ldquo;man&rdquo; and &ldquo;woman.&rdquo; This allows the model to position words in a way that reflects meaningful relationships, such as gender associations, in language.</p>
<p><img src="/images/3/image034.png" alt="VPC"></p>
<p>Embeddings: A Key Component of RAG
Embeddings play a crucial role in Retrieval-Augmented Generation (RAG) by enhancing the model’s ability to process and retrieve relevant information. They provide several key advantages:</p>
<ul>
<li>
<p><strong>Capturing Meaning</strong> – Embeddings represent the semantic relationships between words, enabling RAG models to understand context, nuances, and deeper language structures. This improves their ability to generate relevant and coherent responses.</p>
</li>
<li>
<p><strong>Dimensionality Reduction</strong> – By transforming complex textual data into fixed-size vectors, embeddings make data processing more efficient and scalable, improving the system&rsquo;s performance.</p>
</li>
<li>
<p><strong>Enhancing Model Performance</strong> – By leveraging semantic similarities, embeddings enable more accurate information retrieval, refining the quality of generated responses and helping the model generalize better across various queries.</p>
</li>
</ul>
<p>OPEA offers multiple options for running embedding microservices, as detailed in the OPEA embedding documentation. In this case, ChatQnA uses the Hugging Face TEI microservice, which runs the embedding model BAAI/bge-large-en-v1.5 locally.</p>
<p>Since some microservices are not exposed externally, you will use the Nginx pod to interact with them via curl. To do this, each microservice will be accessed using its internal DNS name.</p>
<ol>
<li>Access to ngnix POD (copy your NGNIX entire pod name from kubectl get pods and REPLACE chatqna-nginx-xxxxxxxx on the below command)</li>
</ol>
<p><img src="/images/3/image035.png" alt="VPC"></p>
<p>Your command prompt should now indicate that you are inside the container, reflecting the change in environment:</p>
<p><img src="/images/3/image036.png" alt="VPC"></p>
<p>Once inside, you will now have direct access to the internal pods.</p>
<ol start="2">
<li>Get the embedding from the Embeddings Microservice for the phrase &ldquo;What was Deep Learning?&rdquo;:</li>
</ol>
<p><img src="/images/3/image037.png" alt="VPC"></p>
<p>The answer will be the vector representation of the phrase &ldquo;What was Deep Learning?&rdquo;. This service returns the vector embedding for the inputs from the REST API.</p>
<p><img src="/images/3/image038.png" alt="VPC"></p>
<h3 id="vector-database-microservice-pod-chatqna-redis-vector-db80"><strong>Vector Database Microservice (POD: chatqna-redis-vector-db:80)</strong></h3>
<p>The Vector Database microservice plays a critical role in the Retrieval-Augmented Generation (RAG) application by storing and retrieving embeddings. This is essential for applications like ChatQnA, where relevant information needs to be efficiently retrieved based on a user&rsquo;s query.</p>
<h4 id="using-redis-as-a-vector-database"><strong>Using Redis as a Vector Database</strong></h4>
<p>In this task, Redis is used as the vector database. However, OPEA supports multiple alternatives, which can be found in the OPEA vector store repository.</p>
<p>A Vector Database (VDB) is specifically designed to store and manage high-dimensional vectors, which represent words, sentences, or images in numerical form. In AI and machine learning, these vectors—also known as embeddings—capture the meaning and relationships between data points, enabling efficient processing and retrieval.</p>
<h4 id="data-preparation-microservice-pod-chatqna-data-prep6007"><strong>Data Preparation Microservice (POD: chatqna-data-prep:6007)</strong></h4>
<p>The Data Preparation (Dataprep) Microservice is responsible for formatting and preprocessing data so that it can be converted into embeddings and stored in the vector database. This ensures that the data is clean, structured, and ready for efficient retrieval.</p>
<h4 id="key-functions-of-the-data-preparation-microservice"><strong>Key Functions of the Data Preparation Microservice</strong></h4>
<ul>
<li>
<p>Receives raw data (e.g., documents or reports).</p>
</li>
<li>
<p>Processes and chunks the data into smaller segments.</p>
</li>
<li>
<p>Sends the processed data to the Embedding Microservice for vectorization.</p>
</li>
<li>
<p>Stores the resulting embeddings in the Vector Database.
Since different vector databases have unique data formatting requirements, the Dataprep Microservice ensures compatibility with the selected database.</p>
</li>
</ul>
<h4 id="testing-the-microservices"><strong>Testing the Microservices</strong></h4>
<p>To verify the functionality of the system and help the model answer the initial question—
&ldquo;What was Nike&rsquo;s revenue in 2023?&quot;—you will need to upload a relevant context file (a revenue report) so it can be processed.</p>
<p>To do this, download a sample Nike revenue report to the Nginx pod using the command below. (If you are no longer logged into the Nginx pod, make sure to log in again before proceeding.)</p>
<p>Execute the following command to download a sample Nike revenue report  to the nginx pod (if you are no longer logged in to the NGinx pod, be sure to use the above command to log in again):</p>
<ol>
<li>Download the document to the microservice :</li>
</ol>
<p><img src="/images/3/image039.png" alt="VPC"></p>
<ol start="2">
<li>Feed the knowledge base (Vectord) with the document (It will take ~30 seconds):</li>
</ol>
<p><img src="/images/3/image040.png" alt="VPC"></p>
<p>After running the previous command, you should receive a confirmation message like the one below. This command updated the knowledge base by uploading a local file for processing.</p>
<p><img src="/images/3/image041.png" alt="VPC"></p>
<p>The data preparation microservice API can retrieve information about the list of files stored in the vector database.</p>
<ol start="3">
<li>Verify if the document was uploaded:</li>
</ol>
<p><img src="/images/3/image042.png" alt="VPC"></p>
<p>After running the previous command, you should receive the confirmation message.</p>
<p><img src="/images/3/image043.png" alt="VPC"></p>
<p>Congratulations! You&rsquo;ve successfully prepared your knowledge base. Now you&rsquo;ll explore the microservices involved in prompt handling.</p>
<h4 id="step-3-prompting">Step 3: Prompting</h4>
<p>Once the knowledge base is set up, you can begin interacting with the application by asking context-specific questions. Retrieval-Augmented Generation (RAG) ensures that responses are both accurate and grounded in relevant data.</p>
<p>The process begins with the application retrieving the most relevant information from the knowledge base in response to the user&rsquo;s query. This step ensures that the Large Language Model (LLM) has access to up-to-date and precise context to generate an informed response.</p>
<p>Next, the retrieved information is combined with the user’s input prompt and sent to the LLM. This enriched prompt enhances the model’s ability to provide answers that are not only based on its pre-trained knowledge but also supported by real-world, external data.</p>
<p>Finally, you will see how the LLM processes this enriched prompt to generate a coherent and contextually accurate response. By leveraging RAG, the application delivers highly relevant answers, grounded in the latest information from the knowledge base.</p>
<p>The microservices involved in this stage include:</p>
<ul>
<li>Embeddings</li>
<li>Vector Database</li>
<li>Retriever</li>
<li>Re-ranking</li>
<li>LLM</li>
</ul>
<p><img src="/images/3/image044.png" alt="VPC"></p>
<h4 id="retriever-microservice-pod-chatqna-retriever-usvc7000"><strong>Retriever Microservice (POD: chatqna-retriever-usvc:7000)</strong></h4>
<p>The Retriever Microservice is responsible for locating the most relevant information within the knowledge base and returning documents that closely match the user’s query. It interacts with various back-end systems that store knowledge and provide APIs for retrieving the best-matching data.</p>
<p>Different knowledge bases utilize different retrieval methods:</p>
<p>Vector databases use vector similarity matching between the user’s question and stored document embeddings.
Graph databases leverage graph locality to find related information.
Relational databases rely on string matching and regular expressions to locate relevant text.
In this task, you will use Redis as the vector database and retrieve information via the Redis retriever.</p>
<p>Since vector retrieval relies on embeddings, you first need to generate an embedding for the question:
&ldquo;What was Nike&rsquo;s revenue in 2023?&rdquo;</p>
<p>This will allow the retriever to search the knowledge base for the most relevant document—such as the Nike revenue report you uploaded in the previous step.</p>
<p>To create the embedding, use the chatqna-tei microservice. (Make sure you are logged into the Nginx pod before proceeding.)</p>
<ol>
<li>Create the embedding and save locally (embed_question):</li>
</ol>
<p><img src="/images/3/image045.png" alt="VPC"></p>
<p>You should get the details about the writing task:</p>
<p><img src="/images/3/image046.png" alt="VPC"></p>
<ol start="2">
<li>Check to see if your embedding was saved:</li>
</ol>
<p><strong>echo $embed_question</strong></p>
<p>You should be able to see the vectors the embeddings microservice generated. You are now able to use the retriever microservice to get the most similar information from your knowledge base.</p>
<ol start="3">
<li>Get and save similar vectors from the initial embed_question locally similar_docs:</li>
</ol>
<p><strong>similar_docs=$(curl chatqna-retriever-usvc:7000/v1/retrieval -X POST   -d &ldquo;{&quot;text&quot;:&quot;test&quot;,&quot;embedding&quot;:${embed_question}}&rdquo;   -H &lsquo;Content-Type: application/json&rsquo;)</strong></p>
<p>By looking at the previous output, you can see the most similar passages (TOP_3) from the document Nike revenue report  and the question <strong>&ldquo;What was the Nike revenue in 2023?&rdquo;</strong>.</p>
<p><strong>echo $similar_docs</strong></p>

<div class="notices info" ><p>The following output has been formatted for better readability. Your results will be presented in plain text and may vary slightly due to the similarity search algorithm. However, you can double check that the retrieved documents will be relevant to your initial query.</p>
</div>

<p>The application will use that information as context for prompting the LLM, but there is still one more step that you need to do to refine and check the quality of those retrieved documents: the reranker.</p>
<h4 id="reranker-microservice-pod-chatqna-teirerank80"><strong>Reranker Microservice (POD: chatqna-teirerank:80)</strong></h4>
<p>The Reranking Microservice plays a crucial role in semantic search, leveraging reranking models to enhance the relevance of retrieved results. When given a user query and a set of documents, this microservice reorders the documents based on their semantic similarity to the query, ensuring that the most relevant results appear first.</p>
<p>Reranking is particularly valuable in text retrieval systems, where documents are initially retrieved using either:</p>
<ul>
<li>Dense embeddings, which capture deep semantic meaning.</li>
<li>Sparse lexical search, which relies on keyword-based matching.</li>
</ul>
<p>While these retrieval methods are effective, the reranking model refines the results by optimizing the order of retrieved documents. This step significantly improves accuracy, ensuring the final output is more relevant, precise, and contextually aligned with the user’s query.</p>
<p><img src="/images/3/image047.png" alt="VPC"></p>
<p>OPEA has multiple options  for re-rankers. For this lab, you&rsquo;ll use the Hugging Face TEI for re-ranking. It is the chatqna-teirerank microservice in your cluster.</p>
<p>The reranker will use similar_docs from the previous stage and compare it with the question What was Nike Revenue in 2023? to check the quality of the retrieved documents.</p>
<p>Extract the 3 retrieved text snippets and save them in a new variable to be reranked:</p>
<ol>
<li>Install jq dependencies to format similar_docs</li>
</ol>
<p><strong>echo -e &ldquo;deb <a href="http://deb.debian.org/debian">http://deb.debian.org/debian</a> bookworm main contrib non-free\ndeb <a href="http://security.debian.org/debian-security">http://security.debian.org/debian-security</a> bookworm-security main contrib non-free\ndeb <a href="http://deb.debian.org/debian">http://deb.debian.org/debian</a> bookworm-updates main contrib non-free&rdquo; &gt; /etc/apt/sources.list &amp;&amp; apt update &amp;&amp; apt install -y jq</strong></p>
<ol start="2">
<li>Extract and format the texts into a valid JSON array of strings</li>
</ol>
<p><strong>texts=$(echo &ldquo;$similar_docs&rdquo; | jq -r &lsquo;[.retrieved_docs[].text | @json]&rsquo;)</strong></p>
<ol start="3">
<li>Send the request to the microservice with the query and the formatted texts:</li>
</ol>
<p><strong>curl -X POST chatqna-teirerank:80/rerank <br>
-d &ldquo;{&quot;query&quot;:&quot;What was Nike Revenue in 2023?&quot;, &quot;texts&quot;: $texts}&rdquo; <br>
-H &lsquo;Content-Type: application/json&rsquo;</strong></p>
<p><strong>Response:</strong></p>

<div class="notices info" ><p>The following output has been formatted for better readability. Your results are displayed in plain text and may vary slightly due to the similarity search algorithm. The retrieved documents are ranked by similarity to your query, with the highest-ranked index representing the most relevant match. You can confirm that the top-ranked document corresponds to the one most closely aligned with your query.</p>
</div>

<p><img src="/images/3/image048.png" alt="VPC"></p>
<p>The server responds with a JSON array containing objects with two fields: index and score. This indicates how the snippets are ranked based on their relevance to the query: {&ldquo;index&rdquo;:2,&ldquo;score&rdquo;:0.9972289} means the first text (index 0) has a high relevance score of approximately 0.7982. {&ldquo;index&rdquo;:0,&ldquo;score&rdquo;:0.9776342},{&ldquo;index&rdquo;:3,&ldquo;score&rdquo;:0.9296986},{&ldquo;index&rdquo;:1,&ldquo;score&rdquo;:0.84730965} indicates that the other snippets (index 3,1 and 2) have a much lower score.</p>
<p>As you can see from similar_doc the id=2 has the below information where it EXACTLY refers to the revenue for 2023!</p>
<p><img src="/images/3/image049.png" alt="VPC"></p>
<p>Just the first will be used to prompt the LLM.</p>
<h4 id="llm-microservice-pod-chatqna-tgi80"><strong>LLM Microservice (POD: chatqna-tgi:80)</strong></h4>
<p>At the core of the RAG (Retrieval-Augmented Generation) application lies the Large Language Model (LLM), which plays a pivotal role in generating responses. By leveraging RAG, the system enhances the LLM’s performance, ensuring responses are accurate, relevant, and context-aware.</p>
<p>Types of LLMs
LLMs generally fall into two main categories, each with its own strengths and trade-offs:</p>
<ul>
<li>
<p>Closed-Source Models
These proprietary models are developed by major tech companies such as Amazon Web Services (AWS), OpenAI, and Google. They are trained on extensive datasets and optimized for high-quality, reliable outputs. However, they come with certain limitations:</p>
<ul>
<li>Limited Customization: Users have minimal control over fine-tuning.</li>
<li>Higher Costs: Access is usually metered and can be expensive.</li>
<li>Data Sovereignty Concerns: API access may restrict usage in applications requiring strict data governance.</li>
</ul>
</li>
<li>
<p>Open-Source Models
Freely available for use and modification, open-source LLMs offer greater flexibility and control. They allow users to customize and fine-tune models according to specific needs. Running open-source models locally or on private cloud infrastructure ensures better data privacy and cost efficiency. However, they require:</p>
<ul>
<li>Technical Expertise: Deploying and optimizing open-source models can be complex.</li>
<li>Computational Resources: Achieving comparable performance to closed models often demands powerful hardware.
Flexible Integration with OPEA: This microservice architecture supports both closed and open-source models, providing the flexibility to choose the best fit for your application. In this example, the TGI (Text Generation Inference) model from Hugging Face is used.</li>
</ul>
</li>
</ul>
<p>Testing the LLM Microservice: To verify its functionality, you can directly prompt the TGI LLM with a sample question:
&ldquo;What was Nike&rsquo;s revenue in 2023?&rdquo;
This test will demonstrate how well the model can retrieve and generate an informed response based on the loaded knowledge base.</p>
<ol>
<li>Directly prompt the TGI(LLM) Microservice:</li>
</ol>
<p><img src="/images/2/image050.png" alt="VPC"></p>
<p>The model will give you the answer to the prompt like the following:</p>
<p><strong>&ldquo;generated_text&rdquo;:&rdquo; Nike revenue in 2023 has not been reported as it is still in the fourth quarter. The previous full financial year—which is 2022—brought in $48.9 billion in revenue for the American multinational sportswear company. They deal with the design, development, manufacturing, and worldwide marketing/sales of a diverse portfolio of products. From coming into being in 1964 as Blue Ribbon Sports, the firm was renamed Nike, Inc., in 1978. Jumpman logos (for example), include Michael Jordan, who is a former professional basketball player—are among the brands&rsquo; numerous trademarked symbols, tied to the &lsquo;Swoosh&rsquo; logo.\n\nNike revenues are clearly affected by the football World Cup. Consequently, for the 13 weeks ending January 29 in 2022, which were characterized by the football world cup</strong></p>
<p>This directly prompts LLM without providing any context. We can see that the model is actually giving the wrong answer. To test the overall RAG performance, we should test with the megaservice as we did at the beginning of this task, which will involve the entire thread.</p>
<ol start="2">
<li>Exit the ngnix POD: You may find &ldquo;job pending&rdquo;, please ignore and try again.</li>
</ol>
<p><strong>root@chatqna-nginx-deployment-XXXXXXXXXXXX:/# exit</strong></p>
<p>Use the load balancer URL you saved above in the command below to send the query &ldquo;What is Nike&rsquo;s revenue in 2023?&rdquo; to the ChatQNA application.</p>
<ol start="3">
<li>Run curl again to the load balancer:</li>
</ol>
<p><img src="/images/2/image051.png" alt="VPC"></p>
<ol start="4">
<li>Review your output and you should see that the response is streamed. This is the expected behavior of a microservice, as it provides responses in smaller chunks rather than all at once. Streaming allows data to be processed and displayed incrementally as the data becomes available. In the application, the UI will capture this response and format it into a readable display, allowing the user to see the information in real time as the data arrives.</li>
</ol>





<footer class=" footline" >
	
</footer>

        
        </div> 
        

      </div>

    <div id="navigation">
        
        
        
        
            
            
                
                    
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
            
        
        
        


	 
	 
		
			<a class="nav nav-prev" href="/3-accessibilitytoinstances/3.1-deploy-chatqna/" title="Deploy ChatQnA"> <i class="fa fa-chevron-left"></i></a>
		
		
			<a class="nav nav-next" href="/3-accessibilitytoinstances/3.3-private-instance-copy/" title="Test the deployment and verify RAG workflow" style="margin-right: 0px;"><i class="fa fa-chevron-right"></i></a>
		
	
    </div>

    </section>
    
    <div style="left: -1000px; overflow: scroll; position: absolute; top: -1000px; border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;">
      <div style="border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;"></div>
    </div>
    <script src="/js/clipboard.min.js?1740709594"></script>
    <script src="/js/perfect-scrollbar.min.js?1740709594"></script>
    <script src="/js/perfect-scrollbar.jquery.min.js?1740709594"></script>
    <script src="/js/jquery.sticky.js?1740709594"></script>
    <script src="/js/featherlight.min.js?1740709594"></script>
    <script src="/js/highlight.pack.js?1740709594"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="/js/modernizr.custom-3.6.0.js?1740709594"></script>
    <script src="/js/learn.js?1740709594"></script>
    <script src="/js/hugo-learn.js?1740709594"></script>

    <link href="/mermaid/mermaid.css?1740709594" rel="stylesheet" />
    <script src="/mermaid/mermaid.js?1740709594"></script>
    <script>
        mermaid.initialize({ startOnLoad: true });
    </script>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-158079754-2', 'auto');
  ga('send', 'pageview');

</script>
  </body>
</html>
